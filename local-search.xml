<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>多端部署 Hexo</title>
    <link href="/2023/06/02/%E5%A4%9A%E7%AB%AF%E9%83%A8%E7%BD%B2hexo/"/>
    <url>/2023/06/02/%E5%A4%9A%E7%AB%AF%E9%83%A8%E7%BD%B2hexo/</url>
    
    <content type="html"><![CDATA[<p>文章来源：<a href="https://www.jianshu.com/p/a0824fe2e066?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation">https://www.jianshu.com/p/a0824fe2e066?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation</a></p><blockquote><p>害怕原文丢了，所以贴在这里</p></blockquote><p>如果我想要在公司写博客怎么办，或者说如果我换电脑了怎么办，因为在 github 中的我们 github.io 项目是只有编译后的文件的，没有源文件的，也就是说，如果我们的电脑坏了，打不开了，我们的博客就不能进行更新了，所以需要把源文件也上传到 github（或者 Coding）上，然后对我们的源文件进行版本管理，这样我们就可以在另一台电脑上 pull 我们的源码，然后编译完之后 push 上去。</p><p>思路<br>同步思路与 Github 推拉源码思路相同，使用 git 指令，保持本地的博客文件与 Github 上的博客文件相同即可<br>仓库分两个分支：hexo 和 master. hexo 作为默认分支，存放博客源代码，master 分支存放博客生成页面。</p><p>创建 hexo 分支<br>默认已经有 master 分支了，创建 hexo 就可以了（名字不一定要 hexo），并将hexo设置为默认分支（Default branch）</p><p>这个 hexo 分支就是存放我们源文件的分支，我们只需要更新hexo分支上的内容据就好，master 上的分支 hexo 编译的时候会更新的。<br>配置 gitignore 文件<br>为了筛选出配置文件、主题目录、博文等重要信息，作为需要 GitHub 管理的文件<br>public 内文件是根据 source 文件夹内容自动生成，不需要备份，不然每次改动内容太多<br>即使是私有仓库，除去在线服务商员工可以看到的风险外，还有云服务商被攻击造成泄漏等可能，所以不建议将配置文件传上去</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs markdown">.DS<span class="hljs-emphasis">_Store</span><br><span class="hljs-emphasis">Thumbs.db</span><br><span class="hljs-emphasis">db.json</span><br><span class="hljs-emphasis">*.log</span><br><span class="hljs-emphasis">node_</span>modules/<br>public/<br>.deploy<span class="hljs-emphasis">*/</span><br><span class="hljs-emphasis">_config.yml</span><br></code></pre></td></tr></table></figure><p>初始化仓库及提交<br>然后我们再初始化仓库，重新对我们的代码进行版本控制</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">git init  //初始化本地仓库<br>git add -A //添加本地所有文件到仓库        <br>git commit -m &quot;blog源文件&quot; //添加commit<br>git branch hexo //添加本地仓库分支hexo<br>git remote add origin &lt;server&gt;  //添加远程仓库 &lt;server&gt; 是指在线仓库的地址 origin是本地分支,remote add操作会将本地仓库映射到云端<br>git push origin hexo //将本地仓库的源文件推送到远程仓库hexo分支<br></code></pre></td></tr></table></figure><p>这时候博客的源文件就同步到github的hexo分支上了。</p><p>注意这里有个坑！！！如果你用的是第三方的主题 theme，是使用 git clone下来的话，要把主题文件夹下面把 .git 文件夹删除掉，不然主题无法 push 到远程仓库，导致你发布的博客是一片空白。</p><p>另一台电脑的操作<br>首先需要搭建环境（Node，Git）</p><p>直接clone下来，修改发布后提交本地修改到远程master分支就ok.<br>将hexo分支clone到本地，再进入根目录安装npm</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">git clone &lt;server&gt; hexo //&lt;server&gt; 是指在线仓库的地址<br>cd hexo <br>npm install<br></code></pre></td></tr></table></figure><p>npm install 的时候会根据 package.json 中的插件列表自动加载相应插件。<br>本机的同步完成。</p><p>因为在上传博客源文件的时候忽略了配置文件（_config.yml 这是站点的配置文件）的上传，也就是没有上传配置文件的，在克隆下来的时候记得把配置文件拿过来，不然会报错。主题里面的配置文件也要（themes&#x2F;next&#x2F;_config.yml 这是主题配置文件）</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hexo 博客使用记录</title>
    <link href="/2023/06/02/Hexo%20%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/"/>
    <url>/2023/06/02/Hexo%20%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<p>终于搬新博客了</p><h3 id="Record-1：文章内部图片死活加载不出来"><a href="#Record-1：文章内部图片死活加载不出来" class="headerlink" title="Record 1：文章内部图片死活加载不出来"></a>Record 1：文章内部图片死活加载不出来</h3><p>刚使用 fluid 主题时，文章封面图片可以设置，但文章内的图片无法显示，markdown 语法和 HTML 语法都无济于事。最后死马当活马医，将hexo-asset-image卸载了，重试，居然可以了。</p><h3 id="Record-2：图片存放问题"><a href="#Record-2：图片存放问题" class="headerlink" title="Record 2：图片存放问题"></a>Record 2：图片存放问题</h3><p>有两个文件夹可以存放：&#x2F;source&#x2F;img 和 &#x2F;public&#x2F;img，但图片不能放在后者中，因为 &#x2F;public 目录下的东西都是执行 hexo g 生成的（从 &#x2F;source 目录获取相关内容），若执行 hexo clean 就全部被删除了。</p><h3 id="Record-3-引用本地文章"><a href="#Record-3-引用本地文章" class="headerlink" title="Record 3: 引用本地文章"></a>Record 3: 引用本地文章</h3><p>使用这样的格式 (明白为什么要用图片吧 -_- )：<br><img src="/img/usage-records/cite-local-paper.png"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>EnglishLeaningByTalkingToChatGPT</title>
    <link href="/2023/02/14/EnglishLeaningFromTalkingToChatGPT/"/>
    <url>/2023/02/14/EnglishLeaningFromTalkingToChatGPT/</url>
    
    <content type="html"><![CDATA[<p>最近发现与 ChatGPT 聊天是一个学习英语的好方法，既有输入又有输出，还能无限的追问下去。所以专门开一篇记录一些重要的聊天记录，能够从中学习到新的单词、新的表达。</p><h4 id="2023-02-14"><a href="#2023-02-14" class="headerlink" title="2023-02-14"></a>2023-02-14</h4><blockquote><p>$\mathbb{Q}: $</p><p>$\mathbb{A}: $</p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>关系型数据库拓展性解决方案</title>
    <link href="/2022/09/08/%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8B%93%E5%B1%95%E6%80%A7%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <url>/2022/09/08/%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8B%93%E5%B1%95%E6%80%A7%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
    
    <content type="html"><![CDATA[<p>实现一个关系型数据库的可拓展性有很多方法：</p><ol><li>在应用层划分数据，将不同的数据分片划分到不同的关系数据库上，如 MySQL sharding；</li><li>或者在关系数据库内部支持数据自动分片，如 Microsoft SQL Azure；</li><li>或者干脆从存储引擎开始重新写一个全新的分布式数据库，如 Spanner、OceanBase 和 TiDB.</li></ol><h3 id="方式一：引入中间层，应用层数据分片"><a href="#方式一：引入中间层，应用层数据分片" class="headerlink" title="方式一：引入中间层，应用层数据分片"></a>方式一：引入中间层，应用层数据分片</h3><p>最常见最简单的方式就是应用层按照规则将数据拆分成多个分片，分布到多个数据库节点，并引入一个中间层来对应用屏蔽后端的数据库拆分细节。以 MySQL Sharding 架构为例，如下图：<br><img src="/img/TiDB-Paper/1.png"><br>整个架构可以分为：</p><ul><li><strong>中间层 dbproxy 集群</strong>：解析 MySQL 协议、执行 SQL 路由、SQL 过滤、读写分离、结果归并&#x2F;排序&#x2F;分组等。由多个无状态的 dpproxy 进程组成。另外可以在客户端与中间层之间加一个 LVS（Linux Virtual Server）对客户端请求进行负载均衡。</li><li><strong>数据库组</strong>：每一个数据库组由 N 台数据库组成，单主多副本。主节点负责所有的写事务及强一致读事务，副本可以负责一些一致性要求不高的读事务。主备复制采用 binlog 方式.</li><li><strong>元数据服务器</strong>：主要负责维护数据分片规则和组内选主。元数据服务器也需要多副本实现高可用，一般使用 Zookeeper 实现。</li><li><strong>常驻进程</strong>：安装在每一台数据库服务器上，与 Zookeeper 交互，负责监控、单点切换等。</li></ul><br><p>MySQL Sharding 集群一般按照用户 id 进行哈希分区，随着数据量的增长，会出现以下情况可能需要扩容：</p><ul><li>集群的容量不够</li><li>单个用户的数据量太大。</li></ul><p><strong>对于第一种情况</strong>，MySQL Sharding 集群往往采用双倍扩容方案，从 2 台服务器扩到 4 台，接着再扩到 8 台…假设现在有两个数据库组，db1 &#x3D; {A0, A1}, db2 &#x3D; {B0, B1}. hash(id) % 2 为奇数的数据放在 db1，为偶数的数据放在 db2. 常见的一种扩容方式如下：</p><ol><li>停止写服务，等待主备完全同步后解除 A0 与 A1、B0 与 B1 之间的主备关系。</li><li>修改中间层的映射规则，将 hash(id) % 4 &#x3D; 1 的用户数据映射到 A1，hash(id) % 4 &#x3D; 3 的用户数据映射到 B1.</li><li>开启写服务，hash(id) % 4 &#x3D; 0, 1, 2, 3 的数据分别写入到 A0、A1、B0、B1，就相当于有一半的数据分别从 A0、B0 迁移到 A1 和 B1.</li><li>分别给 A0、A1、B0、B1 增加一个副本。</li></ol><blockquote><p>可以看到，扩容过程需要停服一段时间，需要人工参与，难以完全自动化，而且扩容过程若发生了服务器故障会使得扩容变得非常复杂。</p></blockquote><p><strong>对于第二种情况</strong>，可以使用其他的分片方式，或者在应用层定期统计大用户，并且将这些用户的数据按照数据量拆分到多个数据库组，然而维护这些拆分元信息也需要付出代价。</p><h3 id="方式二：Microsoft-SQL-Azure"><a href="#方式二：Microsoft-SQL-Azure" class="headerlink" title="方式二：Microsoft SQL Azure"></a>方式二：Microsoft SQL Azure</h3>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>TiDB: A Raft-based HTAP Database</title>
    <link href="/2022/09/06/TiDB%20A%20Raft%20based%20HTAP%20Database/"/>
    <url>/2022/09/06/TiDB%20A%20Raft%20based%20HTAP%20Database/</url>
    
    <content type="html"><![CDATA[<h2 id="TiDB-要解决什么问题？"><a href="#TiDB-要解决什么问题？" class="headerlink" title="TiDB 要解决什么问题？"></a>TiDB 要解决什么问题？</h2><p>关系型数据库管理系统因提供了关系模型、强事务支持和良好的 SQL 接口而广受欢迎，在如银行业等传统应用场景用得很多。然而以前的关系型数据库不支持拓展性（伸缩性）和高可用性，这时 NoSQL 就应运而生，如 BigTable、DynamoDB 和 Redis. NoSQL 放松了对一致性的要求（事务）而提供了高拓展性和可选的数据模型（如 key-val、图和文本）。那么有没有可能把关系型数据库和 NoSQL 的优点结合在一起呢，同时提供强事务保证、高拓展性和可用性？NewSQL 就在干这件事，如 CockroachDB 和 Spanner.</p><blockquote><p>NoSQL (not only sql) 一般由以下几种驱动因素：</p><ul><li>比关系型数据库更好的伸缩性，包括非常大的数据集或非常高的写吞吐量。</li><li>关系模型不能很好的支持一些特殊的查询操作。</li></ul></blockquote><blockquote><p><strong>对于第一点</strong>，可以参考文章：<a href="/2022/09/08/%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8B%93%E5%B1%95%E6%80%A7%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/" title="关系型数据库拓展性解决方案">关系型数据库拓展性解决方案</a>。<strong>对于第二点</strong>，考虑存储一份简历。一个人可能有多段求学经历、多段工作经历和一些其他信息。在关系型数据库中，可能需要建立多张表：求学经历表，存储每一个用户的求学经历；工作经历表，存储每一个用户的工作信息；用户表，存储用户的基本信息等。那么当查询某个用户的简历时就需要查询多个表，做一些 join 操作，并发度和性能就有所损失，而像 MongoDB 这类文档数据库可以直接存储 Json、XML 等数据，当要访问简历信息时，将文档存储在一处的局部性就会带来性能优势。另外，关系数据模型将数据存储在关系表中，那么就需要一个转换层将表、行、列等转换成应用程序中的对象模型。（当然了，现阶段的 SQL 标准也增加了对结构化数据和 XML 数据的支持，可以将多值数据存储在单行内，并支持对它们的查询和索引。）</p></blockquote><br><p>无论是单机关系型数据库、NoSQL 还是 NewSQL 它们都属于 OLTP（online transaction processing）。用户通过 OLTP 创造了数据，这些数据可能蕴含者巨大的价值，需要进行挖掘。OLAP（online analytical processing）系统就是用来发掘数据价值的。OLTP 所产生的业务数据分散在不同的业务系统中，而 OLAP 往往需要将不同的业务数据集中到一起进行统一综合的分析，这时候就需要根据业务分析需求做对应的数据清洗后存储在数据仓库中，然后由数据仓库来统一提供 OLAP 分析。所以我们常说 OLTP 是数据库的应用，OLAP 是数据仓库的应用。可以将 OLAP 看作是 OLTP 的一种延展，一个让 OLTP 产生的数据发现价值的过程。</p><p>OLTP 和 OLAP 这样搭配起来各干各的或看起来不错，但也存在一些问题，比如数据转换需要一些其他的组件（如ETL），需要花精力维护，转换过程持续时间较长，而且数据都是定期成批次地进行转换（量大），OLTP 性能是否会受到影响也需要考虑，此外在<strong>某些</strong>应用中，数据还需要实时分析。所以 HTAP（OLTP &amp; OLAP）系统就产生了，当然了不能仅仅是缝合在一起，还有新的要求：</p><ul><li>数据新鲜度保证（freshness）。及时分析新数据蕴含着巨大商业价值（如推荐）。</li><li>性能隔离性保证（isolation）。隔离性表示两个系统同时工作时，需要保证两类系统的性能不会相互影响（或影响很小）。</li></ul><br><p>总计一下，TiDB 做了啥：</p><ul><li>保证 OLTP 和 OLAP 负载不会相互干扰。</li><li>OLAP 请求能够读到足够新的，和 OLTP 具有一致性的数据。</li><li>由于数据是一致的，就能够在 SQL 层（优化器）将不同的语句交由 OLTP 或 OLAP 处理。</li></ul><br><p>虽然 TiDB 融合了 OLTP 和 OLAP，但采用了两套存储引擎（TiKV 和 TiFlash）来分别支持事务和分析请求。TiKV 是行式存储（row-format），用于存储事务处理中的数据，TiFlash 是列式存储（column-format），用于给分析型请求提供支持。TiDB 整个架构如下：<br><img src="/img/TiDB-Paper/2.jpg"><br>其中 PD 相当于 Master 集群，主要存储数据分片元数据、负载均衡、分配时间戳等。TiSpark 是用于连接 Hadoop 生态的组件。</p><p><strong>这里先说一下 TiDB 是如何做到 HTAP 功能的</strong>：</p><blockquote><p>在 TiKV 中数据是分片存储的，每一个分片是一段连续的 key 空间，称之为 region. 每一个 region 又包含多个副本，副本之间的数据同步采用 raft 协议，因此整个系统包含多个 region，也就是多个 raft 组，这又称之为 multi-raft.</p></blockquote><blockquote><p>为了保证 OLTP 和 OLAP 不会相互干扰，一般这两种类型的查询执行在不同的机器上，最主要的困难还是如何从 OLTP 中拿到足够新的数据副本给 OLAP，而且还得保证这些副本数据是一致的（当然这样也保证了高可用性）。因此啊，TiDB 在 raft 中引入了一个新的角色：learner. 它只从 leader 那里复制日志，不参与选举、不参与投票（也就是不算做大多数），因此这个日志复制是异步的。在 TiFlash 中会为 TiKV 中的每一个 region 设置一个 learner. learner 从 leader 复制过来的日志会在 TiFlash 中重放处理后，再转换成列式存储在 TiFlash 中供分析型查询使用。</p></blockquote><blockquote><p>由于复制的时间很短，能够保证新鲜度，raft 协议也能保证数据的一致性。在不同的存储引擎上执行不同类型的查询，TP 和 AP 的性能也不会相互干扰。TiKV 与 TiFlash 关系如下图，可以看到 TiKV 中的多个 region 中的数据可能存储在 TiFlash 中的一个 partition 中。<br><img src="/img/TiDB-Paper/3.jpg"></p></blockquote><p>上面的描述看起来好像 TiDB 的创新也不是很多嘛，这里总结下（论文）：</p><blockquote><ul><li>构建了一个基于 raft 的开源 HTAP，为 HTAP 负载提供了高可用性、一致性<br>伸缩性、AP 数据实时性和 AP&#x2F;TP 性能的隔离性。</li><li>为 raft 协议引入了一个 learner 角色，为 AP 查询提供实时的列式存储数据。</li><li>实现 multi-raft，并优化了其读写性能。</li><li>实现了一个 SQL 引擎层，能够依据不同的 HTAP 查询选择合适的存储模式。</li><li>大量的对比实验。</li></ul></blockquote><br><p>其实更多的还是工程实现上的困难：</p><blockquote><ol><li>首先是 raft 和 multi-raft 相关问题：① 如何优化 raft 流程提升读写请求性能；② 分片数据的迁移、分裂、合并等。</li><li>如何及时地（低延迟）将日志同步到 learner 以保证新鲜度？正在进行的事务可能会生成一些非常大的日志，这些日志需要在 learner 中快速重放和实例化（materialization），以便分析型查询读取新数据。将日志数据转换为列式可能会由于模式（schema）不匹配而遇到错误，这同样可能会延迟日志同步。</li><li>同时进行 TP 和 AP 时如何保证各种的性能？大型 TP 需要读取写入多个服务器上的大量数据。AP 同样也消耗大量资源。为了减少执行开销，SQL 层还需要为这两类工作制定合适的执行计划，选择合适的存储引擎。</li></ol></blockquote><br><p>就像论文中说的那样，TiDB 提供了一种思路将 NewSQL 进化为 HTAP. 整篇论文都在将怎么进化的，效果怎么样。 </p><h2 id="TiDB-中的-Raft-算法"><a href="#TiDB-中的-Raft-算法" class="headerlink" title="TiDB 中的 Raft 算法"></a>TiDB 中的 Raft 算法</h2><p>基本的 raft 算法流程大概是下面这样的：</p><blockquote><ol><li>一个 region leader 收到来自 SQL 层的请求。</li><li>leader 将请求追加到自己的日志中（包括写磁盘）。</li><li>leader 将这些新的日志条目发送到 follower（并行），它们也将这些条目追加到自己的日志中。</li><li>leader 等待 follower 回复，一旦超过大多数（包括自己）回复成功，leader 就会提交这些日志，并将它们应用到复制状态机中。</li><li>leader 将应用请求的结果返回客户端（SQL 层）并继续提供服务。</li></ol></blockquote><p>上述过程最大的问题是性能不高，所有步骤都是串行执行的，因此在 TiDB 中有如下的优化：</p><ul><li>步骤 2 和 3 可以并行。这两个过程没有依赖关系，因此 leader 可以在追加日志的同时将日志发送给 follower. 如果 leader 持久化日志失败了，但大多数 follower 成功复制了日志，依然满足大多数原则，日志依旧可以被提交。</li><li>另外，步骤 3 中 leader 给 follower 发送日志时可以按批次发送。而且也不用等待某个 follower 回复成功后再发送后面的，而是假设上次成功了，修改 nextIdx，继续发送后面的日志。如果这个 follower 失败了，调整 nextIdx 即可。</li><li>步骤 4 中的 apply 也可以异步来做。当日志提交后，就可以用另外一个线程来 apply 这些提交了的日志。</li></ul><p>优化后，TiDB 中 raft 算法流程大致如下：</p><blockquote><ol><li>leader 收到来自 SQL 层的请求。</li><li>leader 将请求追加到自己的日志中，同时（并行）将这些日志发送给 follower.</li><li>leader 继续接收请求，重复步骤 1，2.</li><li>leader 提交日志，并将提交的日志发送给另外一个线程 apply.</li><li>apply 后，leader 返回结果给客户端（SQL 层）。</li></ol></blockquote><br><p>另外还有读优化。TiDB 实现了 readIndex、lease Read 和 follower read. </p><h2 id="TiDB-的事务处理"><a href="#TiDB-的事务处理" class="headerlink" title="TiDB 的事务处理"></a>TiDB 的事务处理</h2><p>TiDB 在事务中实现了悲观锁和乐观锁。<strong>乐观事务的流程大致如下</strong>：</p><blockquote><ol><li>从客户端接收到 “begin” 命令后，SQL 层向 PD 申请一个时间戳作为本次事务的开始时间戳 start_ts。</li><li>对于每一条来自客户端的 DML，SQL 层从 TiKV 中读取 [0, start_ts] 区间内最新的数据，然后在事务本地内存中操作数据。</li><li>当 SQL 层收到客户端的 “commit” 命令后开启 2PC 流程。它先随机地选择一个 key 作为 primary key，然后锁住所有的 key（锁住 keys 是为了保证事务的原子性），最后发送 prewrites 给 TiKV. 因为会牵扯到很多的 key，它们可能存在不同的节点上，所有发送 prewrites 之前，需要向 PD 查询路由信息，然后分组发送.</li><li>当所有的 prewrite 都成功后，SQL 层向 PD 申请一个时间戳作为本次事务的提交时间戳 commit_ts. 然后 SQL 层发送命令给 TiKV 提交 primary key.</li><li>primary key 提交成功后，SQL 层即可向客户端回复。</li><li>SQL 层异步提交其他的 key，并清理锁。</li></ol></blockquote><blockquote><p>DML &#x3D; Data Manipulation Language，包括 insert、update、delete、select.</p></blockquote><p>关于乐观事务更加详细的描述可以参考文章: <a href="https://docs.pingcap.com/zh/tidb/dev/optimistic-transaction">TiDB 乐观事务模型</a>。这个过程和 percolator 大致一样，不过 TiDB 是异步提交 secondary keys. 乐观事务实现了快照隔离。</p><p><strong>悲观事务模型</strong>和乐观事务模型相差不大，最大的区别在于加锁的时机。一般来说，乐观模型是一种验证模型，待所有的更新操作完成后，再验证这些更新能否写入数据库；而悲观则是先取得要更新数据的锁，再更新数据。TiDB 的悲观模型有点不一样，可以参考文章：<a href="https://docs.pingcap.com/zh/tidb/dev/pessimistic-transaction">TiDB 悲观事务模式</a>。在这个模型中实现了可重复读（RR）和读已提交（RC）隔离级别。</p><p>关于 PD（placement driver）的运作机理，可以参考这篇文章：<a href="https://cn.pingcap.com/blog/tidb-6.0-make-tso-more-efficient">TiDB 6.0：让 TSO 更高效</a></p><h2 id="TiDB-中的-TiFlash"><a href="#TiDB-中的-TiFlash" class="headerlink" title="TiDB 中的 TiFlash"></a>TiDB 中的 TiFlash</h2><p>待续。。。</p><h2 id="TiDB-的性能"><a href="#TiDB-的性能" class="headerlink" title="TiDB 的性能"></a>TiDB 的性能</h2><p><img src="/img/TiDB-Paper/4.jpg"><br><img src="/img/TiDB-Paper/5.jpg"><br><img src="/img/TiDB-Paper/6.jpg"></p><blockquote><p>TC &#x3D; transaction client, AC &#x3D; analysis client.<br>TPS &#x3D; Transactions Per Second，指一台服务器每秒能够处理的事务数量.<br>QPS &#x3D; Queries Per Second，指一台服务器每秒能够响应的查询次数.</p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>查询处理篇：查询执行模型</title>
    <link href="/2022/08/24/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%9E%8B/"/>
    <url>/2022/08/24/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p>在<a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E6%80%BB%E8%AE%BA/" title="总论篇">总论篇</a>中提到了用户编写的 SQL 语句最终会被转换成具体的执行计划。一个执行计划会包含很多操作，比如选择、投影、join、sort、aggregation 等等，在其他篇章中也介绍了完成这些操作的可以采用的算法。本文主要是讨论以何种方式执行一条执行计划，如何组织这些操作。</p><p>为了避免陷入细节，先以代数表达式介绍一般的执行模型，后面再讨论具体的执行计划是怎么做的。</p><blockquote><p>执行计划 &#x3D; 代数表达式 + 完成各个操作的具体算法</p></blockquote><br><p>以该表达式为例：$\Pi_{name} (\sigma_{building&#x3D;Watson}department)\bowtie instructor)$.</p><p>不考虑查询优化，上面的表达式可以用一个算子树（operator tree）表示：<br><img src="/img/evaluate_expression/1.jpg"></p><blockquote><p>很明显，树中共有三个算子：选择、连接和投影。</p></blockquote><p>执行这个表达式有一个很直观的思路：先在 department 表上执行 $\sigma_{building&#x3D;Watson}$ 操作（具体算法可以是顺序扫描、索引或排序），将满足条件的元组写入一个临时表中。然后以上一步生成的临时表和 instrutor 为输入执行 $\bowtie$ 操作（具体算法可以是 hash join、merge join 或其他），将满足条件的元组写入一个临时表中。最后再在临时表上执行 $\Pi_{name}$ 操作，将生成的元组返回给用户。<strong>这种方法被称为物化执行（materialized evaluation）</strong>，如下图：</p><blockquote><p><img src="/img/evaluate_expression/3.jpg"><br>如果采用物化的方式计算一条表达式，在执行过程中会产生临时表（图中的 out[]），如果该表很大（buffer pool 装不下），那就需要将该表写入磁盘，执行其他算子时再读出，所以其代价需要加上磁盘读写开销。可以使用<strong>双缓冲策略</strong>，一个用于缓冲读入、一个用于缓冲写出，这样一来就可以减少算子等待 I&#x2F;O 的时间。</p></blockquote><br><p>可以换一个思路：没必要为所有的中间过程都创建一个临时表，而是产生一部分结果后就返回它们。以 $\Pi_{a_1, a_2}(r \bowtie s)$ 为例，如果采用物化方式，那么需要一个临时表来存储表 r 和 s join 的结果，然后再在临时表上执行投影操作；但该临时表不是必须的，可以这样，每当 join 操作生成了一些满足条件的元组后，就立即将其发送给 $\Pi$ 算子，因为元组数量少可以保留在内存中，就避免了创建临时表额外读写磁盘的开销。<strong>这种方式被称为流水线执行（pipelined evaluation）</strong>. 使用流水线方式可以提供以下优点：</p><blockquote><ul><li>避免了磁盘读写临时表，减少了查询执行的开销。</li><li>能够减少查询的响应时间。采用 pipeline 后，一部分结果生成后就会立马返回，用户不需要长时间的等待完整的结果生成。</li></ul></blockquote><p>流水线执行方式一般有两种模式：</p><ul><li><strong>Demand-Driven Pipeline</strong>. 又称为迭代器模型或火山模式。在该模式中，可以通过迭代器（iterator）实现算子。迭代器包含 open()、next() 和 close() 等方法. open() 一般用来初始化算子，比如在 merge join 算子中，open 就可以对表排序；调用 open() 之后，每一次调用 next() 都会返回若干个（一个或一批）该算子生成的元组，如 join 的结果；当不需要更多的元组时，就可以调用 close()，该函数可以完成一些内存回收等工作，如下图（左边是 batch，右边是单个元组）：<blockquote><p><img src="/img/evaluate_expression/2.jpg"></p></blockquote></li></ul><br><ul><li><strong>Producer-Consumer Pipeline</strong>. 在该模式中，算子树中相邻的两个算子（父子）之间有一个缓冲. 当一个算子（线程或进程）需要元组时就去其输入缓冲中读取（最底层算子会直接读表），若输入缓冲为空，该算子等待；当算子产生结果时就将其写入输出缓冲中，若输出缓冲满了，该算子等待。各算子同时表现为消费者和生成者。</li></ul><p>demand-driven pipeline 的工作模式是由上层算子驱动的，可以理解为 pull data，是一种懒汉模式. 而 producer-consumer pipeline 的工作模式则更加主动，每个算子都就可能的计算，可以理解为 push data，是一种饿汉模式。demand-driven pipeline 用得比较多，因为简单容易实现，但 producer-consumer pipeline 在多处理器（并行处理）系统中有更高的性能，因为各算子可以并行计算。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Join 的种类</title>
    <link href="/2022/08/22/Join%20%E7%9A%84%E7%A7%8D%E7%B1%BB/"/>
    <url>/2022/08/22/Join%20%E7%9A%84%E7%A7%8D%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<p>join 实际上是连接操作（concatenation），表 r 和 s 之间的 join 可以表示为 $r \bowtie_{\theta} s$，$\theta$ 表示连接的条件，有等于、大于、小于和自然连接几种. 这篇文章不会陷入如何写一条 join SQL 语句的细节中，而是介绍几种不同类型的 join：</p><blockquote><p>(natural) inner join<br>(natural) left outer join<br>(natural) right outer join<br>(natural) full outer join</p></blockquote><br><p>假设有两张表，属性如下：</p><blockquote><p>student &#x3D; { ID, name, dept_name }<br>takes   &#x3D; { ID, course_id, semester, year, grade }，</p></blockquote><blockquote><p>student 是学生信息表；takes 是学生选课信息表，注意不是每一个学生都有选课信息。</p></blockquote><h2 id="natural-join-amp-join"><a href="#natural-join-amp-join" class="headerlink" title="natural join &amp; join"></a>natural join &amp; join</h2><p>实际上 natural 是一个条件，它对这几种 join 的效果是一样的，所以不妨以 inner join 为例搞清楚有无 natural 的区别。</p><p>现在有个需求：查找出学生的选课信息，用 inner join 可以写出如下的 SQL 语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">from</span> student <span class="hljs-keyword">join</span> takes <span class="hljs-keyword">on</span> student.ID <span class="hljs-operator">=</span> takes.ID;<br><br>其结果是一张表，包含的属性是这样的：<br>&#123; student.ID, takes.ID, name, dept_name, course_id, semester, <span class="hljs-keyword">year</span>, grad&#125;<br><br>等价于执行下面的 <span class="hljs-keyword">SQL</span> 语句：<br><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">from</span> student, takes<br><span class="hljs-keyword">where</span> student.ID <span class="hljs-operator">=</span> takes.ID;<br></code></pre></td></tr></table></figure><p>natural join 也可以完成这样一个查询，SQL 语句如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">from</span> student <span class="hljs-keyword">natural</span> <span class="hljs-keyword">join</span> takes;<br><br>其结果是一张表，包含的属性如下：<br>&#123; ID, name, dept_name, course_id, semester, <span class="hljs-keyword">year</span>, grad &#125;<br></code></pre></td></tr></table></figure><br><p>这里可以很明显的看出 natural join 与 inner join 的<strong>区别</strong>：</p><blockquote><ul><li>natural join 只会考虑包含相同属性值（属性同时存在于两张表中）的元组。student 和 takes 表都包含 ID 属性，所以 student <em>natural join</em> takes 只会连接那些 ID 属性值相同的元组。若有两张表，r 和 s，包含多个相同的属性，假设为 A、B、C，那么 natural join 相当于 where r.A &#x3D; s.A &amp;&amp; r.B &#x3D; s.B &amp;&amp; r.C &#x3D; s.C，而 inner join 的条件需要指定，没有默认值。</li><li>natural join 的结果中，相同属性列只会出现一次，而 inner join 会出现两次（如果条件中包含了相同的属性）。</li></ul></blockquote><br><p>当然也有一些<strong>相同点</strong>，除了上面的不同，基本都是一样的。如果 select 不指定输出的列，那么在结果表中，先出现条件中的列（若是 natural join，先出现两张表中共有的列），然后是左表中独有的列，最后再是右表独有的列。</p><h2 id="outer-join-与-inner-join"><a href="#outer-join-与-inner-join" class="headerlink" title="outer join 与 inner join"></a>outer join 与 inner join</h2><p>假设现在我们要查找所有学生的选课情况，那么下面的 SQL 能办到吗：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">from</span> student <span class="hljs-keyword">natural</span> <span class="hljs-keyword">join</span> takes;<br><br>或者<br><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">from</span> student <span class="hljs-keyword">join</span> takes <span class="hljs-keyword">on</span> student.ID <span class="hljs-operator">=</span> takes.ID;<br></code></pre></td></tr></table></figure><p><strong>很遗憾，不能</strong>，为什么呢？因为有些学生没有选课，所以在 takes 表中没有它们的信息，上述语句查询出来的结果只包含那些选过课的学生（ID 值同时存在与 student 和 takes 表中），而没有选过课的学生则会被清除。那么有没有方法列出 student 表中所有学生的选课信息呢，就算他们没有选课，那也应该显示出来，让我知道不是。再比如，如果还有一张 course 表，我现在要知道所有课被选的情况，如果采用上述语句连接 takes 表和 course 表也会出现遗漏某些课程的情况（这些课程没有人选）。</p><p>上述的需求可以通过 outer join 办到。outer join 的工作方式和 inner join 差不多，只不过保留那些会被 inner join 丢弃的元组。当然了，保留了那些元组后，一些属性列的值缺失，这些值会使用 null 值填充。</p><p>举个例子，假如有一个叫做小红的同学没有选课，那么使用 outer join 后，她的选课信息就如下：</p><blockquote><table><thead><tr><th>ID</th><th>name</th><th>dept_name</th><th>course_id</th><th>semester</th><th>year</th><th>grad</th></tr></thead><tbody><tr><td>xx</td><td>小红</td><td>xxxxxxx</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></blockquote><br><p>outer join 有以下三种类型：</p><blockquote><p>left outer join. 只会保留左表会被丢弃的元组</p></blockquote><blockquote><p>right outer join. 只会保留右表会被丢弃的元组</p></blockquote><blockquote><p>full outer join. 保留两张表中会被丢弃的元组，其结果是上述两种的并集。</p></blockquote><br><p>举个例子，如果要找出哪些没有选课的学生，则可以采用如下的 SQL 语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> ID, name<br><span class="hljs-keyword">from</span> student <span class="hljs-keyword">natural</span> <span class="hljs-keyword">left</span> <span class="hljs-keyword">outer</span> <span class="hljs-keyword">join</span> takes<br><span class="hljs-keyword">where</span> course_id <span class="hljs-keyword">is</span> <span class="hljs-keyword">null</span>;<br><br>和下面的结果等价：<br><span class="hljs-keyword">select</span> ID, name<br><span class="hljs-keyword">from</span> takes <span class="hljs-keyword">natural</span> <span class="hljs-keyword">right</span> <span class="hljs-keyword">outer</span> <span class="hljs-keyword">join</span> student<br><span class="hljs-keyword">where</span> course_id <span class="hljs-keyword">is</span> <span class="hljs-keyword">null</span>;<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>MySQL 的复制</title>
    <link href="/2022/08/18/MySQL%20%E7%9A%84%E5%A4%8D%E5%88%B6/"/>
    <url>/2022/08/18/MySQL%20%E7%9A%84%E5%A4%8D%E5%88%B6/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数据的复制</title>
    <link href="/2022/08/12/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%A4%8D%E5%88%B6/"/>
    <url>/2022/08/12/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%A4%8D%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<p>所谓的复制其实就算数据在多个节点上的冗余存储，这些节点一般通过网络连接（内部网络-单个数据中心内；公共网络-跨数据中心）。一般来说，数据冗余存储有如下优点：</p><ul><li>增强数据的可用性和安全性。如果某个节点发生故障，依然能向外提供数据服务。</li><li>减少请求响应时间。不同的请求路由到邻近的副本处理。</li><li>增加吞吐量。请求可以分配到不同的副本处理，减轻单个节点的负载。</li></ul><br><blockquote><p>数据复制的单位通常是数据分片，如 tablet、node、virtual node 等。</p></blockquote><br><p>下面介绍三种常用的复制类型：单主复制（single-master）、多主复制（multi-master）和无主复制（leaderless）。</p><h2 id="单主复制"><a href="#单主复制" class="headerlink" title="单主复制"></a>单主复制</h2><p>单主复制又叫主从复制，在所有副本中选择一个作为主节点，其余的副本称为从节点或从库。客户端的<strong>写请求必须发送到主节点</strong>（若从节点收到写请求，可以路由给主节点），从节点可以处理读请求，从客户端的角度来看从节点都是只读的。这样的配置称之为单活。</p><p>主节点接收客户端的写请求后，除了将数据写入本地存储外，还需要将新数据复制到从节点以保证副本一致性。复制方式可以分三类：</p><ul><li><strong>同步复制（synchronous replication）</strong>。该模式下，主节点执行完一个写请求（或一个事务）后，必须等待所有同步从节点都复制了更新后，才可以回复客户端写入成功。同步复制的<u>优点是</u>，从节点保证有与主节点一致的最新数据副本。如果主节点突然失效，这些数据仍然能在从节点上上找到。<u>缺点是</u>，如果同步从节点没有响应（比如它已经崩溃，或者出现网络故障，或其它任何原因），主节点就无法处理写入操作。主节点必须阻止所有写入，并等待同步副本再次可用。</li><li><strong>异步复制（asynchronous replication）</strong>。在模式下，主节点执行完写请求后，会将新数据复制给从节点，但无需等待其他副本写入成功就返回客户端。很明显，异步复制的<u>优点是</u>，主节点不需要等待从节点，所以写请求响应快。<u>但是</u>，异步复制会潜在地影响副本数据的一致性和持久性。例如，如果客户端收到写请求完成响应后立即查询刚刚写入的数据则有可能读到旧值（或不存在），这可以通过实现读己之写一致性避免。再如，如果主节点还未复制新数据到从节点时就发生不可恢复故障，那么数据就有可能永久丢失。</li><li><strong>半同步复制（semisynchronous replication）</strong>。在该模式下，主节点只需要等待至少一个从节点复制更新后才返回，不需要等待所有从节点。</li></ul><br><blockquote><p>将所有从节点都设置为同步的是不切实际的：任何一个节点的中断都会导致整个系统停滞不前。实际上，如果在数据库上启用同步复制，通常意味着其中一个跟随者是同步的，而其他的则是异步的，也就是半同步。如果同步从节点变得不可用或缓慢，则可以将某一个异步从节点提升为同步从节点，可以保证至少在两个节点上拥有最新的数据副本。</p></blockquote><blockquote><p>异步复制能提供比较好的性能，但存在数据丢失的风险，所以学界工业界一直在寻找不丢数据但仍能提供良好性能和可用性的复制方法，比较成功的有<a href="https://pdos.csail.mit.edu/6.824/papers/cr-osdi04.pdf">链式复制</a>和<a href="/2022/06/14/raft-%E6%9D%82%E8%AE%B0/" title="共识协议">共识协议</a>。</p></blockquote><blockquote><p>引入数据多副本后，客户端的请求会涉及到一致性问题，具体可以参见文章：<a href="/2022/08/02/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B/" title="分布式一致性模型">分布式一致性模型</a>。另外，主节点采用什么方式将数据复制给从节点呢？这个还是得以一个具体得数据库为例弄清楚比较好，参见文章：<a href="/2022/08/18/MySQL%20%E7%9A%84%E5%A4%8D%E5%88%B6/" title="MySQL 的复制">MySQL 的复制</a>。</p></blockquote><p>总的来说，<strong>单主复制的主要优点有</strong>：① 简单易懂、易于实现；② 仅在主节点执行（并发）写请求，能够保证操作的顺序性，避免了副本数据冲突的麻烦，当然了单主场景也会发生冲突，比如切换主节点后原主节点的数据与当前主节点冲突，解决方法当然是以当前主节点数据为准。这个特性使得单主复制更容易支持事务类操作，因为简单，性能高；③ 对于大量读请求工作负载的系统，单主复制是可拓展的，通过增加从节点就能提高读事务吞吐量。</p><p><strong>缺点也很明显</strong>：① 面对大量写请求工作负载时系统很难进行拓展，因为写请求的瓶颈在主节点；② 主节点宕机时，从节点切换是一个难题。节点切换可以有手动切换和自动切换两种。对于手动切换能保证新主节点具有完整的数据，但耗时长；对于主动切换则可能发生脑裂，比如在网络分区中，多个从节点检测到主节点失效同时成为新的主节点，当然了共识协议可以解决这个问题。</p><h2 id="多主复制"><a href="#多主复制" class="headerlink" title="多主复制"></a>多主复制</h2><p>所谓多主复制就是指在副本中存在多个主节点，复制仍然以同样的方式发生：处理写入的每个主节点都必须将该数据更改转发给所有其他节点。 称之为多领导者配置（也称多主、多活复制）。在这种情况下，每个主节点同时扮演其他主节点的从节点。</p><blockquote><p>这里需要注意，数据分片后，每个分片都会有一个主节点，这种场景应该不是多主复制，因为这些主节点负责的数据没有交集，不存在冲突一说。</p></blockquote><p><img src="/img/data_replication/2.png"></p><p>因为好处很少超过复杂性的代价，在单个数据中心内部基本不使用多主配置，在一些情况下，多活配置也是合理的：<strong>① 多数据中心</strong>。假如有一个数据库，副本分散在好几个不同的数据中心（也许这样可以容忍单个数据中心的故障，或地理上更接近用户）。 使用常规的基于领导者的复制设置，主节点必须位于其中一个数据中心，且所有写入都必须经过该数据中心。多领导者配置中可以在每个数据中心都有主节点。</p><blockquote><p>如果是单活配置，每个写入都必须穿过互联网，延迟会很高，违背了多数据中心的一部分初衷（就近原则）；若是多活配置，各写入可以路由到最近的数据中心，各数据中心采用异步复制，提升写性能。</p></blockquote><blockquote><p>如果是单活配置，若数据中心故障，跨数据中心的主节点选举耗时更长；若是多活配置，各数据中心独立运行，不受影响。</p></blockquote><blockquote><p>如果是单活配置，同步复制可能会跨数据中心，走公共网络，远不如内部的网络可靠；若是多活配置，异步复制更能容忍网络故障。</p></blockquote><p><strong>② 离线操作的客户端</strong>。应用程序在断网之后仍然需要继续工作，在这种情况下，每个设备都有一个充当领导者的本地数据库（它接受写请求），并且在所有设备上的副本之间同步时，存在异步的多主复制过程。从架构的角度来看，这种设置实际上与数据中心之间的多领导者复制类似，每个设备都是一个“数据中心”，而它们之间的网络连接是极度不可靠的。<strong>③ 协同编辑</strong>。当一个用户编辑文档时，所做的更改将立即应用到其本地副本（Web 浏览器或客户端应用程序中的文档状态），并异步复制到服务器和编辑同一文档的任何其他用户。</p><p><img src="/img/data_replication/1.png"><br>一般来说，通过 IP 属地、地理位置等可以将特定用户&#x2F;客户端的请求路由到同一个主节点（数据中心），能在一定程度上避免数据冲突，但有些情况依然有可能会发生冲突，比如负载均衡、多用户操作同一数据等，如上图。一些常见的数据冲突解决方法有：</p><ul><li><strong>由客户端解决</strong>。具体方案是，当（主节点间）异步复制发现数据冲突时将所有冲突的数据都保留下来（多个版本）。客户端下次读取发生冲突的数据时，将所有数据版本都返回给客户端，由客户端选择合适的数据再返回给主节点覆盖冲突的数据。购物车就是一个典型的例子。购物车应用解决冲突的逻辑是保留购物车中所有冲突的商品，并返回给用户，当用户看到重复的或已经被删除的物品，就会主动纠正，纠正后的内容被重新提交，就解决了该冲突。</li><li><strong>最后写入胜利</strong>。给每个写入一个唯一的 ID（例如，一个时间戳，一个长的随机数，一个 UUID 或者一个键值的哈希），挑选最高 ID 的写入作为胜利者，并丢弃其他写入。如果使用时间戳，这种技术被称为最后写入胜利（LWW, last write wins）。虽然这种方法很流行，但是很容易造成数据丢失。</li><li><strong>特殊的数据类型</strong>。如无冲突复制数据类型（conflict-free replicated datatypes，CRDT）是可以由多个用户同时编辑的集合，映射，有序列表，计数器等的一系列数据结构，它们以合理的方式自动解决冲突。</li></ul><p>总结一下，多或配置的主要优点有：① 增加主节点的容错性，一个主节点挂了不会影响其他主节点继续服务；② 可以在多个节点上执行写请求，分担写负载的压力；③ 写请求可以路由到邻近的主节点以减少响应时间。主要缺点就是它太复杂了，容易产生数据冲突。</p><h2 id="无主复制"><a href="#无主复制" class="headerlink" title="无主复制"></a>无主复制</h2><p>单主复制、多主复制都是这样的想法：客户端向一个主节点发送写请求，而数据库系统负责将写入复制到其他副本。主节点决定写入的顺序，而从节点按相同顺序应用主节点的写入。一些数据存储系统采用不同的方法，放弃主节点的概念，并允许任何副本直接接受来自客户端的写入。</p><p><strong>无主复制的主要思想是</strong>，客户端不仅向一个节点发送写请求，而是将请求发送到多个节点（某些情况下甚至会发送给所有节点），一旦得到一些节点的确认信息就认为这次写入成功了。读取数据时，客户端也不止会从一个节点读取数据，而是将读请求发送给多个节点，获取数据和数据的版本号。</p><blockquote><p>这里有两种方式，第一种是客户端直接向多个副本发送请求；第二种是引入一个协调节点，客户端将请求发送给协调节点，再由协调节点充当客户端的角色发送请求和接收回复，最后返回给真正的客户端。</p></blockquote><p>由于执行写请求时一般不会等待所有副本的成功写入回复，那么就可能会存在部分节点未写入成功或者根本没有发送写请求给它，这样一来客户端收到的多个读请求回复里面就可能包含过期的数据，所以需要一种方法能让返回的多个数据中一定存在最新的（也就是版本号最大的）。<strong>这种方法就是 Quorum 机制（法定人数机制）</strong>：</p><blockquote><p>假设总共有 $N$ 个副本，执行写请求时，当客户端（或协调节点）至少收到 $W$ 个副本成功写入回复后才认为此次写入是成功的；执行读请求时，需要至少从 $R$ 个节点上读取数据，那么只要 $W + R &gt; N$，就能保证读请求返回的数据中一定包含先前写入的最新数据。</p></blockquote><p>尽管上述机制能让客户端鉴别出最新的数据，系统依然需要修复这些不一致，在亚马逊 Dynamo 架构中有两种修复方法：</p><ul><li><strong>读修复（read repair）</strong>。既然客户端能够鉴别出旧的数据，那么就可以由客户端在收到读请求回复后向拥有旧数据的副本发起一个带有新数据的写请求，以此来更新副本的数据。</li><li><strong>反熵过程（anti-entropy process）</strong>。反熵过程会新建一个后台进程来修复数据，该进程不断查找副本之间的数据差异，并将任何缺少或陈旧的数据从一个副本复制到另一个副本。反熵过程只保证各个副本的数据最终一致，不保证写入的顺序性。Dynamo 使用了<a href="/2022/07/17/CAP%20&%20BASE/" title="梅克尔树（merkle tree）">梅克尔树（merkle tree）</a>来验证数据是否有不一致。</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Lock Manager</title>
    <link href="/2022/08/08/Lock%20Manager/"/>
    <url>/2022/08/08/Lock%20Manager/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>badgerDB 中的 LSM</title>
    <link href="/2022/08/04/badgerDB%20%E4%B8%AD%E7%9A%84%20LSM/"/>
    <url>/2022/08/04/badgerDB%20%E4%B8%AD%E7%9A%84%20LSM/</url>
    
    <content type="html"><![CDATA[<p>badgerDB 基本是按照<a href="https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf"> WiscKey 论文 </a> 实现的，在 <a href="/2022/08/04/LevelDB%20%E4%B8%AD%E7%9A%84%20LSM/" title="levelDB">levelDB</a> 基础上专门为 SSD 做了优化。</p><p><font color=red>第一个优化，Key 和 Value 分开存储</font>。在 levelDB 中 key-val 是一起插入 LSM 中的，当某两层归并时 key 和 val 都会被读入内存、排序然后再写回磁盘，事实上只有 key 参与了排序，如果将 val 存在另外一个地方，合并时只需要读取 key，就可以有效的减少写放大。</p><p><img src="/img/badgerDB/1.png"></p><p>如上图，badgerDB 将 value 单独存在一个名为 vLog（value log）的文件中，而 LSM 中存储的则是 [key, &lt;vlog_offset, val_size&gt;]，当然了要 addr 远小于 val 时才够划算，算一下每层最坏情况的写放大：</p><blockquote><p>假设 key&#x3D;16B，addr&#x3D;16B，val&#x3D;1KB，如果是普通 LSM 写放大为 10，badgerDB 则为 (10 * (16 + 16) + 1024) &#x2F; (16 + 16 + 1024) &#x3D; 1.27! </p></blockquote><p>这样一来，<u>当插入记录时</u>，val 首先会被追加到 vlog 文件，key 和该 val 的地址会被插入普通的 LSM 中；<u>当删除一条记录时</u>，仅仅只是将 [key, addr] 从 LSM 中删除（插入删除标记），不需要变动 vlog，那些没有 key 指向的 val（被删除或被重写）会在垃圾回收（garbage collection）流程中处理；<u>当查询记录时</u>，首先会在 LSM 中查找，若该记录已经被删除了，返回 not found，否则根据 LSM 中查找到的 addr 再去 vlog 中读取实际的数据。</p><blockquote><p>这里相比 levelDB 多了一步随机读操作，好像性能会更低，实际上由于 key-val 分开存储后，整个 LSM 存储的 SSTable 会小很多，比如现有 key&#x3D;16B，addr&#x3D;16B，val&#x3D;1KB 的 100GB key-val 记录，如果采用 levelDB，加上各种 index，LSM 会远大于 100GB，如果采用 badgerDB，其 LSM 只有 3GB，首先是层级会更少，其次甚至整个 LSM 都可以缓冲到内存中。</p></blockquote><br><p><font color=red>第二个优化，提升范围检索性能</font>。levelDB 将 key 和 val 放在一起，范围查询时直接顺序扫描即可，而 badgerDB 将 key-val 分开存储，范围扫描时得到的是一组 val 的地址，会存在大量的随机读，下图展示了 SSD 顺序读和随机读的吞吐量比较：</p><p><img src="/img/badgerDB/2.png"></p><p>很明显 badgerDB 必须要使用多线程来提升随机读的性能才能追上顺序读。大致流程是这样的，当上层在迭代器上调用 Next() 接口时，badgerDB 会从 LSM 中预取后面的多个记录，将这些记录中的 val 地址插入到队列中，然后多个线程在后台去 vlog 中读取数据。</p><p>如果是 HDD 的话是做不到的，还是会因为随机访问降低性能，LSM 带来的好处荡然无存。</p><br><p><font color=red>第三个优化，垃圾回收</font>。所谓垃圾回收就是将那些已经删除或覆写的记录空间腾出来。levelDB 在删除或更新记录时，只是在 LSM 中插入了新的记录或一条删除标记，并不会立即删除过时的数据，真正的空间回收发生在合并的时候，如果合并时发现过时的记录就丢弃。badgerDB 的 [key, addr] 存储在 LSM 中，并不用担心它们的回收，而 val 单独存在 vlog 中，若不删除过时的数据，vlog 将变得越来越庞大。</p><p>一个比较朴素的方式是，先扫描一遍 LSM 收集过期的 [key, addr]，然后再根据 addr 去删除 vlog 中的 val. 很明显该方法会严重干扰系统的正常服务，所以 badger 采用如下的方式：<br><img src="/img/badgerDB/3.png"><br>如上图，badgerDB 对 vlog 存储的记录格式做了一些变更，由 [val] -&gt; [key_size, val_size, key, val]. 可以设置一个阈值，当 vlog 大小达到阈值后开始垃圾回收，也可以定时清理，总之<u>当垃圾回收线程启动后</u>（下称 gcWorker），gcWorker 会从 vlog tail 位置开始一次性读取数个 val 记录，提取其中的 key 去 LSM 中查询该 key 是否过期（可以根据 addr 判断），若过期就将该 val 记录丢弃，若未过期就将该 val 记录 append 到 vlog 的 head 位置。<u>为了防止 append 的时候发生故障</u>，append 的数据会调用 fsync() 同步刷写到 vlog 中，同时将新的 [key，addr] 插入到 LSM 中，还需要将更新后的 tail 插入到 LSM 中，记录格式为 [“tail”, &lt;tail-vLog-offset&gt;]，head 倒不用记录，因为它就是文件的最后一个字节。</p><br><p><font color=red>第四个优化，干掉 WAL 日志</font>。levelDB 中有一个 WAL 日志，新的变更操作都会先刷写进 WAL 后才会进入 memTable，为的是防止 故障发生时，memTable 和 immutable 中的记录丢失。在 badgerDB 中 vlog 其实就已经充当了 WAL 日志，为什么？首先只有当 val 记录写入了 vlog 文件中，[key, addr] 才有可能写到 SSTable 中。当系统重启后，只需要扫描 vlog 文件就能恢复 SSTable. 不过这个方法太低效，需要扫描整个 vlog 文件，所以 badgerDB 会周期性的将 head 记录同步刷写到 LSM 中，格式为 [“head”, head-vlog-offset]，这样一来，系统重启后先从 LSM 中查找最近一次的 head，然后从该位置扫描到文件尾进行数据恢复，整个方法有点像 <a href="/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/" title="checkpoint">checkpoint</a>.</p><br><p><u>另外，关于 SSD</u>，最初 LSM 更多用在大量 HDD 写场景的，假设一个高端 HDD，有 10ms 寻道延迟，100MB&#x2F;s 的数据传输速率，随机访问 1KB 数据的时间大约为 10ms，而顺序读写 1KB 数据的时间只有 10μs，随机读比上顺序读高达 1000:1（写操作也差不多），所以对于存在大量随机写的场景使用 LSM 吞吐量就会好很多，虽然 LSM 会带来写放大，但也远远小于 1000. </p><p>SSD 的随机和顺序读写延迟大概为 200:1，远低于 HDD，似乎 LSM 对 SSD 效果没那么大，其实不是，首先是吞吐量的提升，其次 SSD 是有擦除（写入前必须擦除）次数限制，而且 SSD 后台还会进行垃圾回收：将分散的 block 放在一起，再擦除被移动的 block. LSM 本身就是顺序追加，会大幅度减少垃圾回收的次数。</p><p>badgerDB 针对 SSD 擦除寿命限制这个特点，将 key val 分开存储，进一步减小 LSM 的写放大，同时利用 SSD 内部并行性特点，提高范围检索时对 val 读取的吞吐量。</p><p>论文中还有很大内容，比如实验对比、崩溃一致性、相关工作等，可以去看看。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>LevelDB 中的 LSM</title>
    <link href="/2022/08/04/LevelDB%20%E4%B8%AD%E7%9A%84%20LSM/"/>
    <url>/2022/08/04/LevelDB%20%E4%B8%AD%E7%9A%84%20LSM/</url>
    
    <content type="html"><![CDATA[<h2 id="LevelDB-LSM"><a href="#LevelDB-LSM" class="headerlink" title="LevelDB - LSM"></a>LevelDB - LSM</h2><p><img src="/img/levelDB/1.png"><br>上图右边大致是 LevelDB 实现的 LSM，以下简称 LDB.</p><p>LDB 是一个采用 LSM 思想实现的存储引擎，分为内存和磁盘两部分，内存中的结构包括 memTable 和 immutable，它们使用跳表实现。<u>当要插入一条数据时</u>，先写 log（WAL），然后插入 memTable，当 memTable 达到预设的上限时，该 memTable 就变成 immutable，不再发生修改（只读），同时创建一个新的 memTable 继续服务。后台线程将该 <u>immutable 序列化</u>然后 dump 到磁盘生成一个 $L_0$ 层的 SSTable 文件，SSTable（sorted string table）文件由索引、布隆过滤器和数据构成，最后会介绍，这个过程称为 Minor Compaction。</p><p>上述大概是 LDB 的插入流程，$L_0$ 层的每一个 SSTable 都是 immutable 序列化后生成的，虽然每个 SSTable 内部 key 都是有序的，但它们中 key 范围存在交叉，当查询数据时既有较大的读放大也不支持范围查找。</p><p>像 <a href="/2022/06/30/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%89%EF%BC%9ALSM%20Tree/" title="SMI-LSM">SMI-LSM</a> 中那样 LDB 将 $L_0$ 层 SSTable 合并到下层来提升查找效率，但 LDB 同时实现了范围查找。</p><p>从 $L_1$ 到 $L_6$ 每一层的 SSTable 和 $L_0$ 的不太一样，$L_0$ 是 immutable 序列化后直接 dump 而成，而 $L_{i},\ i &gt;&#x3D; 1$ 层的所有 SSTable 都是一个大文件按 key 范围拆分出来的，每一个大小都差不多，总大小是上一层总大小的 10 倍左右（每一层的 SSTable 文件大小是不同的）。</p><p><u>当 $L_0$ 层的 SSTable 数量达到了预设的上限后</u>（8 个），就会另起一个线程将它们合并到 $L_1$ 层：合并过程是多路归并算法（见 <a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9ASorting/" title="查询处理篇：Sorting">查询处理篇：Sorting</a>），所有的 $L_0$ 层 SSTable 都参与归并，而 $L_1$ 只有部分会参与，因为它们是按 key 切分的，不存在交叉，所以只需要 key 值与 $L_0$ 层有交叉的参与合并即可，合并后再按 key 值范围切分为多个 $L_1$ 层的 SSTable，不影响未参与合并的。这个过程叫做 Major Compaction.</p><p><u>这样一来查找过程就应该是</u>，先到 memTable 中查找，若不中，到 immutable 中查找，若再不中，就到 $L_0$ 层查找，因为 $L_0$ 层的 SSTable 的 key 值可能存在交叉，所以每一个都需要查找，若还未中，就继续往下层查找，从 $L_1$ 开始的每一层中，各个 SSTable key 范围理论上没有交叉所以只需要查找一个即可，为了实现这个理论，LDB 创建了一个名为 MANIFEST 的文件记录了各层各个 SSTable 文件 key 范围等元数据，每次合并完会更新它。</p><p><u>计算一下 LDB 的写放大和读放大</u>，对于写放大，它其实在 <a href="/2022/06/30/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%89%EF%BC%9ALSM%20Tree/" title="B-LSM 和 SMI-LSM">B-LSM 和 SMI-LSM</a> 之间，按照最坏来算，就和 B-LSM 一样，每层最多为 10，平均为 5. 对于读放大，查找一条记录最坏情况下需要遍历 $L_0$ 的所有 SSTable，$L_1-L_6$ 层各一个 SSTable，一共有 14 个 SSTable，遍历时需要将一个 SSTable（或部分）读入内存，包括 index block、bloom-filter block 和 data block，假设读入时的 block 为 4KB，查找一个 1KB key-value 记录最坏需要读入 $\frac{14\ *\ 4KB\ +\ SSTable\ -\ 4KB}{1KB} \approx 56$. 假如一个 B+ 树高为 5，每个节点的大小为 4KB，查找一条 1KB key-value 记录需要访问 6 个节点，读放大为 $\frac{4\ *\ 6KB}{1KB} &#x3D; 24$.  </p><h2 id="LevelDB-SSTable"><a href="#LevelDB-SSTable" class="headerlink" title="LevelDB - SSTable"></a>LevelDB - SSTable</h2><p><a href="https://blog.csdn.net/ws1296931325/article/details/86635751/">参考文章</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>索引篇四：Parallel Index</title>
    <link href="/2022/08/03/%E7%B4%A2%E5%BC%95%E7%AF%87%E5%9B%9B%EF%BC%9AParallel%20Index/"/>
    <url>/2022/08/03/%E7%B4%A2%E5%BC%95%E7%AF%87%E5%9B%9B%EF%BC%9AParallel%20Index/</url>
    
    <content type="html"><![CDATA[<p>当数据分片后，在这些分片之上建立的索引可以分为两类（下面的一个节点代表&#x2F;存储一个分片）：</p><ul><li><u>局部索引（local indices）</u>，指的是那些构建在某个具体分片之上的索引。索引内容和数据存储在同一个节点。</li><li><u>全局索引（global indices）</u>，指的是那些构建在多个分片之上的索引，能够在众多分片之上提供高效的查找操作。索引内容一般也会分片存储在不同的节点上（为了增强拓展性）。</li></ul><p>如果一个全局索引建立在分片属性上，就称该索引为<strong>全局主索引（global primary index）</strong>。如分片方法涉及的属性为 $K$，建立在 $K$ 上的全局主索引实际上只是在每个分片上都建立了一个关于 $K$ 的局部索引。当一个请求想要查询一条 $K &#x3D; k_1$ 的数据时可以通过全局主索引：先通过分片方法确定 $k_1$ 属于哪一个分片，然后再到这个分片上通过建立在 $K$ 上的索引查找。</p><p>如果一个全局索引没有建立在分片属性上，就称该索引为<strong>全局二级索引（global secondary index）</strong>，如分片的属性为 $K_p$，索引属性为 $K_i$，且 $K_p \ne K_i$. 现在来看一下如何通过全局二级索引检索一条 $K_i &#x3D; k_1$ 的数据。第一种方法是，可以在每个分区上都建立一个关于 $K_i$ 的局部索引，当请求到来时，该请求会被发送到每一个分区上进行检索。这个方法有点低效，基本不使用，下面阐述实际更加高效的做法。</p><p>假设有一张 student 表，属性值有：[id, name, dept_name, grad]，分片属性为 id. <u>为了提供在 name 上的高效查询</u>，我们在 name 属性上构建一个全局二级索引：首先为 student 的每一条元组都创建一条 [name, id] 元组，假设这些新创建的元组构成一个新表 index_name，然后将 index_name 以 name 属性分片，再在每个分片上建立关于 name 的局部索引，最后在 id 上再建立一个全局主索引（也就是在 student 表的每个分区上建立一个关于 id 的局部索引）。当一个请求想要检索某条包含具体名字，假设为 $n_1$，的数据时，可以通过如下的步骤：</p><ul><li>通过 index_name 的分片方法找到包含 $n_1$ 的分片；</li><li>路由到分片上，检索 name 上的局部索引，找到对应的 id（可能有多个）；</li><li>最后通过构建在属性 id 上的全局主索引找到对应的数据。</li></ul><p>这个方法只会涉及到部分的分片，比上面的方法要高效得多。上面是一个例子，<u>下面给出更加形式化的描述</u>：一般地，给定一个表 $r$，它的分片属性为 $K_p$，如果要提供关于属性 $K_i,\ \ K_p \ne K_i$ 的高效检索，就需要在 $K_i$ 上构建一个全局二级索引。首先，创建一个包含下列属性的表 $r_i^s$：</p><ul><li>$K_i$ 和 $K_p$.</li><li>如果分片属性 $K_p$ 不是主键或者候选键，那么就可能存在重复，所以还得再加入其他属性，假设 $K_u$，使得 $K_p \cup K_u$ 为表 $r$ 的主键或者候选键，这样就能唯一标识 $r_i^s$ 的一条元组。</li></ul><p>即 $r_i^s &#x3D; \Pi_{K_i,\ K_p,\ K_u}(r)$. 然后将 $r_i^s$ 按照 $K_i$ 分片，在每个分片上构建关于 $K_i$ 的局部索引，最后在表 $r$ 的每个分片上构建关于 $(K_p,\ K_u)$ 的局部索引。</p><p>此时查询就可以采用如下的步骤：</p><ul><li>通过表 $r_i^s$ 的分片方法找到请求中 $K_i&#x3D;v$ 所在的分区。</li><li>路由到分区后，在 $K_i$ 局部索引上找到包含 $v$ 的元组 [$v,\ V_{K_p},\ V_{K_u}$]（可能多个）. </li><li>然后根据 $V_{K_p}$ 和表 $r$ 的分区方法找到 $V_{K_p}$ 所在的分区。</li><li>最后路由到分区检索 $(K_p,\ K_u)$ 属性上的局部索引，找到包含 $v$ 的数据。</li></ul><p>建立全局二级索引的方式其实和在 secondary index 叶子节点中存储 primary index 检索键值是一样的。上面提到的属性并不一定只有一个，如 $K_p$ 也可能是一组属性的集合；当 $r$ 中的数据变更时（插入、删除、更新等）$r_i^s$ 及其相关的局部索引也需要更新，所以在实现中，更新 $r_i^s$ 及索引也会作为 $r$ 分布式事务的一部分。</p><p><u>后两个步骤称之为回表（<a href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%BA%8C%EF%BC%9AB-plus%20Tree/" title="索引篇二：B-plus Tree">索引篇二：B-plus Tree</a>中也提到了）</u>。在某些情况下，可以通过覆盖索引（covering index）技术减少回表带来的性能损失。</p><blockquote><p>覆盖索引是什么？索引确实是一种查找数据的高效方式，但是也可以使用索引来直接获取列的数据，这样就不再需要读取数据行（或再去 primary index 中查询）。如果索引的叶子节点中已经包含要查询的数据，那么还有什么必要再回表查询呢？如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。</p></blockquote><blockquote><p>覆盖索引需要保存除了 search key 之外的其他属性值，可以通过组合索引的方式来建立。</p></blockquote><br><p><font color=red>还有个疑问：为什么要凑出 $(K_p,\ K_u)$，使得 [$v,\ V_{K_p},\ V_{K_u}$] 唯一呢？</font></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>一致性哈希</title>
    <link href="/2022/08/03/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C/"/>
    <url>/2022/08/03/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C/</url>
    
    <content type="html"><![CDATA[<blockquote><p>先介绍一致性哈希是怎么样的，然后介绍其在工业系统中的使用，如 Dynamo、redis，怎么处理范围查询的</p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>分布式一致性模型</title>
    <link href="/2022/08/02/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <url>/2022/08/02/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本章内容来自书籍《深入理解分布式系统》第三章和一些其他的资料，又或者是自己的理解。</p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><strong>本文要谈论的一致性模型和复制有着密切关系</strong>。通过复制一节我们知道，复制既带来了高可用性和高性能等好处，但也带来了多个副本如何保持数据一致这个问题，<u>尤其是写操作何时，以何种方式更新到所有副本决定了分布式系统付出怎样的性能代价</u>。按照传统冯·诺依曼体系结构的计算模型来看，读操作应当返回最近的写操作所写入的结果，这是从我们刚开始接触计算机就认为的很自然的逻辑，但关键问题在于“最近”的含义是比较模糊的，到底何时、何种情况才能读到最近的写操作的结果？针对这种情况，我们需要一种模型，能够帮助开发者预测系统中读写操作的结果。</p><blockquote><p>tips: 理解下面的一致性模型时，往往要考虑多副本之间的不一致性（可能是因为故障或延迟等原因）</p></blockquote><br><p>一致性模型就是指，在并发编程中，系统和开发者之间的一种约定，如果开发者遵循某些规则，那么开发者执行读操作或写操作的结果是可预测的。</p><p>首先，这句话最重要的是“可预测”，可预测保证了程序逻辑的确定性，如果对一个系统执行读写操作却无法返回可预测的结果，那么这样的系统是很难使用的。其次，这里的系统是一个很宽泛的概念，虽然一致性模型常用于分布式系统(如分布式文件系统、分布式存储、分布式数据库等)，但也可以指单台计算机的内存或单个寄存器。因为当多线程对内存中的变量进行并发读写操作时，同样是并发编程，也存在同样的可预测问题。同理，定义中的开发者既可以是分布式系统的使用者，也可以是客户端，或者具体的进程。最后，<strong>一致性模型本质上定义了写操作的顺序和可见性，即并发写操作执行的顺序是怎样的，写操作的结果何时能够被别的进程看见</strong>。</p><blockquote><p>一致性模型并非分布式系统独有，在多处理器和并行计算的研究中已经提出很多，本文主要讨论那些能够很好适应分布式系统的一致性模型。</p></blockquote><br><p><img src="/img/consistency-model/1.png"></p><blockquote><p>上图是对一致性模型的分类，主要有三类：</p><ul><li><strong>分区不可用</strong>：当满足这类一致性模型的系统发生网络分区时，为了保证数据一致性和正确性，系统将不再提供服务，即不可用。这类一致性模型包括线性一致性和顺序一致性。</li><li><strong>分区基本可用</strong>：满足这类一致性模型的系统可以容忍一部分节点发生故障，还未出现故障的节点仍然可用，但前提是客户端不能将请求发往不可用的副本节点。这类一致性模型包括因果一致性、PRAM 一致性和读你所写一致性。</li><li><strong>分区高可用</strong>：满足这类一致性模型的系统的可用性是最高的，即使网络发生严重分区，在没有发生故障的节点上依然保证可用，包括读后写一致性、单调读一致性和单调写一致性。</li></ul></blockquote><h2 id="线性一致性"><a href="#线性一致性" class="headerlink" title="线性一致性"></a>线性一致性</h2><p>线性一致性（linearizable consistency &#x2F; linearizability）是最强的一致性模型，通常也被称为强一致性、严格一致性、原子一致性、立即一致性或外部一致性。CAP 中的 C 指的就是它。</p><h3 id="线性一致性直观理解"><a href="#线性一致性直观理解" class="headerlink" title="线性一致性直观理解"></a>线性一致性直观理解</h3><p>线性一致性的基本想法是让一个系统看起来好像只有一个数据副本，而且所有的操作都是原子的，下面以一个例子阐述：<br><img src="/img/consistency-model/2.png"></p><blockquote><p>上图左边，有两个客户端 A 和 B 同时执行写操作，客户端 A 执行 x &#x3D; 1，客户端 B 执行 x &#x3D; 2，但由于副本之间存在同步延迟，最终导致两个副本的 x 值不同。根据线性一致性的要求，系统好像只有一个副本且操作是原子的，如果满足这个要求，就会变成右边那样，这样系统的行为就变得可预测。</p></blockquote><blockquote><p><u>为什么强调操作是原子的呢</u>？注意观察，图右中客户端的写请求发送到副本上后，我们认为副本执行的写操作是立即生效的，这时可以推断出副本上的 x 的值先等于 1 然后等于 2. 但实际上没有操作是能够瞬时完成的，即便是 CPU 访问内存也需要一定的时间。如果不强调操作是原子的，即使只有一个节点，并发操作也有可能返回非预期的结果。</p></blockquote><blockquote><p>回想在单台计算机进行并发编程时，我们仍然需要一些同步原语才能实现线性一致性。原因是，当我们在现代多核 CPU 上运行多线程程序时，由于 CPU 访问缓存速度比内存快得多，所以每个 CPU 核都有自己的缓存，这其实也构成了一个多副本的情况。假如一个线程先对某个变量执行写操作，随后不久另一个线程对该变量执行读操作，在没有任何锁之类的同步原语的情况下，后执行的读操作并不一定能读到先于它的写操作写入的值。这在编写多线程程序时是很常见的现象。</p></blockquote><blockquote><p>综上所述，线性一致性必须强调操作是原子的。开头提到，线性一致性也叫作原子一致性，它们的联系就在这里。在单台计算机上为了实现一致性，硬件会提供一些底层原语(内存屏障等)，操作系统也会根据底层原语封装一些常用的同步原语，最后大多数编程语言都实现了同步原语和原子变量，方便开发者在并发编程时实现线性一致性。</p></blockquote><p><font color=red>在一个线性一致的系统中，只要一个客户端成功完成写操作，后面所有客户端从数据库中读取数据必须能够看到刚刚写入的值。要维护数据的单个副本的假象，系统应当保障读到的值是最近的、最新的，而不是来自陈旧的缓存或副本。换句话说，线性一致性<strong>是一个新鲜度保证（recency guarantee）</strong></font>。</p><h3 id="线性一致性判定"><a href="#线性一致性判定" class="headerlink" title="线性一致性判定"></a>线性一致性判定</h3><p>上面给出了线性一致性的直观理解：单副本假象和原子操作。但应该如何判断一个系统是否实现了线性一致性呢？假设系统只有读操作和写操作，一个操作可以分为调用和响应两个事件，调用一定发生在响应之前，每个调用之后紧跟着相应的响应。记一系列操作（调用和响应事件）组成一个执行历史，如下图（图中的黑色线段表示一个操作持续的时间，注意这是一个原子操作）：<br><img src="/img/consistency-model/3.png"><br>甲是一个无并发的包含多次读写操作的执行历史，请求总是一个接一个顺序执行的，推断这种情况下的执行历史的合法性非常容易，因为没有并发，只需要按顺序检查每个操作的响应是否正确即可，图中初始情况时，x&#x3D;0，客户端执行 write(x&#x3D;1)，write(x&#x3D;3)，然后在读出 x&#x3D;3，符合逻辑，所以是正确的。</p><p>乙描述了更复杂的情况，包含并发，执行历史就如图中那样，对于这类的情况就需要一个理论来判断它是否满足线性一致。</p><p>线性一致性的严格定义：<font color=red>给定一个执行历史，执行历史根据并发操作可以拓展为多个顺序历史，只要能从中找到一个合法的顺序历史，那么该执行历史就是线性一致的</font>。合法就是指在这个拓展的顺序历史中，各个操作的响应符合逻辑。</p><p><strong>如何拓展呢</strong>？如果两个操作是顺序关系，那么拓展后保持该关系不变，若是并发关系，则可以按任一顺序排列。如上图可以拓展成下面两个顺序历史，其中 S2 是合法的，所以上图乙的执行历史是满足线性一致的。<br><img src="/img/consistency-model/4.png"></p><p>总的来说，<u>线性一致性主要有两个约束</u>：① 顺序历史中的任何一次读必须读到最近一次写入的数据；② 顺序历史要跟全局时钟下的顺序一致。正因为有了全局时钟才可能知道哪些操作是顺序的哪些是并发的。</p><h3 id="实现线性一致性"><a href="#实现线性一致性" class="headerlink" title="实现线性一致性"></a>实现线性一致性</h3><ul><li>优化过的共识算法，如 raft + readlIndex &#x2F; lease + 处理客户端重复请求。</li></ul><h2 id="顺序一致性"><a href="#顺序一致性" class="headerlink" title="顺序一致性"></a>顺序一致性</h2><p>顺序一致性（sequential consistency）是一种比线性一致性弱一些的模型。顺序一致性同样允许对并发操作历史进行重新排列，但它的约束比线性一致性要弱，只要求同一个客户端&#x2F;进程的操作在排列后保持先后顺序不变，不同客户端&#x2F;进程之间的操作顺序不做要求。</p><p>以下图为例子：</p><blockquote><p><img src="/img/consistency-model/5.png"><br>在线性一致性要求下，该执行历史只能得到一种顺序历史，且该顺序历史不满足线性一致性。但顺序一致性允许不同客户端之间的操作可以改变先后顺序，所以执行历史可以拓展为下面的顺序历史：<br><img src="/img/consistency-model/6.png"><br>该顺序历史明显是合法的，所以上面的执行历史满足顺序一致性。</p></blockquote><br><p><u>顺序一致性与线性一致性的主要区别在于没有全局时间的限制，它只关心局部（进程&#x2F;客户端内部）操作的顺序</u>。有时顺序一致性往往更实用。例如，在一个社交网络应用中，一个人通常不关心他看到的所有朋友的帖子顺序，但对于具体的某个朋友，以正确的顺序显示该朋友发的贴子会更符合逻辑。</p><h2 id="因果一致性"><a href="#因果一致性" class="headerlink" title="因果一致性"></a>因果一致性</h2><p>因果一致性（Causal Consistency）是一种比顺序一致性更弱一些的模型，它与顺序一致性一样不依赖于全局操作的顺序。因果一致性要求，必须以相同的顺序看到因果相关的操作，而没有因果关系的并发操作可以被不同的进程以不同的顺序观察到。</p><p>最典型的因果关系就是社交网络中的发帖和评论关系，根据因果关系，必须先有发帖才能有对于该帖子的评论，所以发帖操作必然在评论操作之前。</p><p>因果一致性的关键是体现了“发生于⋯⋯之前（happened before）”的关系。为了维持因果性，你需要知道哪个操作发生在哪个其他操作之前。这是一个偏序（指集合内部分元素可比较大小，并不是全部元素都可以比较）：并发操作可以以任意顺序进行，但如果一个操作发生在另一个操作之前（有因果关系），那它们必须在所有副本上以那个顺序被处理。因此，当一个副本处理一个操作时，它必须确保所有因果前驱的操作（之前发生的所有操作）已经被处理；如果前面的某个操作丢失了，后面的操作必须等待，直到前面的操作被处理完毕。</p><p>逻辑时钟（或兰伯特时间戳）可以用来维持因果顺序。</p><h2 id="最终一致性"><a href="#最终一致性" class="headerlink" title="最终一致性"></a>最终一致性</h2><p>还有一些应用，它们的操作没有因果关系，允许使用更宽松的一致性模型，只要系统最终能够达到一个稳定的状态。在某个阶段，系统各节点处理客户端的操作顺序可以不同，读操作也不需要返回最新的写操作的结果。在最终的状态下，只要不再执行写操作，读操作将返回相同的、最新的结果，这就是最终一致性（Eventual Consistency）模型。</p><p>最终一致性是最弱的一致性模型之一，所谓最终，并没有指定系统必须达到稳定状态的硬性时间，这听起来很不可靠，但是在实践中，这个模型工作得很好，当下许多追求高性能的分布式存储系统都是使用最终一致性模型的，例如 Dynamo.</p><blockquote><p>tips: 采用最终一致性的系统都必须要有冲突检测机制、数据恢复方法。</p></blockquote><br><p>由于最终一致性是一个比较笼统的说法，所以并没有具体地归为某一类，也没有在最开始的一致性模型图中画出来。</p><h2 id="单调读一致性"><a href="#单调读一致性" class="headerlink" title="单调读一致性"></a>单调读一致性</h2><p>单调读（Monotonic Read）一致性模型是一种简单的以客户端为中心的一致性模型，<u>单调读一致性必须满足</u>：如果客户端读到关键字 x 的值为 v，那么该客户端对于 x 的任何后续的读操作必须返回 v 或比 v 更新的值，也就是要保证客户端不会读到旧值。</p><p>考虑下面一个例子：</p><blockquote><p>如果副本分布在不同的数据中心，很难保证来自不同设备的连接会路由到同一数据中心。如果用户从不同从库进行多次读取，可能发生<strong>时光倒流异常</strong>：用户 A 两次进行相同的查询，首先查询了一个延迟很小的从库，然后是一个延迟较大的从库（如果用户刷新网页，而每个请求被路由到一个随机的服务器，这种情况是很有可能的。）第一个查询返回最近由用户 B 添加的评论，但是第二个查询不返回任何东西，因为滞后的从库还没有复制更新。在效果上相比第一个查询，第二个查询是在<u>更早的时间点</u>来观察系统。如果第一个查询没有返回任何内容，那问题并不大，因为用户 A 可能不知道用户 B 最近添加了评论。但如果用户 A 先看见用户 B 的评论，然后又看到它消失，那么对于用户 A，就很让人头大了。</p></blockquote><br><p>单调读保证这种异常不会发生。实现单调读取的一种方式是确保每个用户总是从同一个副本进行读取（不同的用户可以从不同的副本读取）。例如，可以基于用户 ID 的散列来选择副本，而不是随机选择副本。但是如果该副本失败，用户的查询将需要重新路由到另一个副本。</p><h2 id="单调写一致性"><a href="#单调写一致性" class="headerlink" title="单调写一致性"></a>单调写一致性</h2><p>单调写（Monotonic Write）一致性模型是一种简单的以客户端为中心的一致性模型，<u>它必须满足</u>：同一个客户端&#x2F;进程的写操作在所有的副本上都以同样的顺序执行，也就是要保证客户端的写操作是串行的。例如，客户端先执行写操作 x&#x3D;0，然后再执行 x&#x3D;1，如果有另外一个客户端不停地读 x 的值，那么有可能会先读到 x&#x3D;0，然后 x&#x3D;1，但绝不可能先读到 x&#x3D;1，再读到 x&#x3D;0.</p><h2 id="读己之写一致性"><a href="#读己之写一致性" class="headerlink" title="读己之写一致性"></a>读己之写一致性</h2><p>读己之写（Read My Write）一致性模型是一种简单的以客户端为中心的一致性模型，<u>该一致性要求</u>：当写操作完成后，在同一副本或其他副本上的读操作必须能够读到新写入的值。</p><p>许多应用让用户提交一些数据，然后查看他们提交的内容。可能是用户数据库中的记录，也可能是对讨论主题的评论，或其他类似的内容。提交新数据时，必须将其发送给领导者，但是当用户查看数据时，可以从追随者读取。如果数据经常被查看，但只是偶尔写入，这是非常合适的。一个违反读己之写的例子如下图：</p><blockquote><p><img src="/img/consistency-model/7.png"><br>客户端向副本 1 执行 x&#x3D;1 写操作成功后，发起读请求查看自己的更新，读请求被路由到了副本 2，但由于副本 2 还未从副本 1 处同步数据，所以读到了旧数据。</p></blockquote><br><p>这是一个保证，如果用户重新加载页面，他们总会看到他们自己提交的任何更新。它不会对其他用户的写入做出承诺：其他用户的更新可能稍等才会看到。</p><h2 id="PRAM-一致性"><a href="#PRAM-一致性" class="headerlink" title="PRAM 一致性"></a>PRAM 一致性</h2><p>PRAM（Pipelined RAM）一致性也称为 FIFO 一致性，直译为“流水线随机访问存储器一致性”，它由单调读、单调写和读你所写三个一致性模型组成。<u>PRAM 一致性要求</u>：同一个客户端的多个写操作，将被所有的副本按照同样的执行顺序观察到，但不同客户端发出的写操作可以以不同的执行顺序被观察到。一个违法 PRAM 一致性的例子如图所示：</p><blockquote><p><img src="/img/consistency-model/8.png"><br>图中，对于客户端 A 的操作，在副本 1 上的顺序是先存款 20 元再取款 10 元，可是在副本 2 上的顺序却是先取款 10 元再存款 20 元，假如此时客户端 B 去读取副本 2 上的余额，在某个时间可能会查到余额为 -10，这显然是违反常理的。</p></blockquote><h2 id="读后写一致性"><a href="#读后写一致性" class="headerlink" title="读后写一致性"></a>读后写一致性</h2><p>最后一种以客户端为中心的一致性模型是读后写（Write Follow Read）一致性模型，<u>读后写一致性要求</u>：同一个客户端对于数据项 x，如果先读到了写操作 w1 的结果 v，那么之后的写操作 w2 保证基于 v 或比 v 更新的值。读后写一致性其实还约束了写操作的顺序，写操作 w1 一定发生在 w2 之前。</p><p>举个例子，一个用户先阅读到某篇文章，再对该文章发表评论，那么该用户发表评论的操作一定在文章被发表的操作之后。读后写一致性看起来跟因果一致性非常相似，只不过是以单个客户端为视角。</p><h2 id="一致性与隔离性的比较"><a href="#一致性与隔离性的比较" class="headerlink" title="一致性与隔离性的比较"></a>一致性与隔离性的比较</h2><p>一致性和隔离级别对任何数据系统(无论是不是分布式的)来说都是非常重要的两个概念. 下面对一致性和隔离级别进行一个总结性的对比。</p><p>一致性模型和隔离级别的<strong>相同点是</strong>，它们本质上都是用来描述系统能够容忍哪些行为，不能容忍哪些异常行为，更严格的一致性模型或隔离级别意味着更少的异常行为，但以降低系统性能和可用性为代价。</p><p>一致性模型和隔离级别的一个<strong>主要区别是</strong>，一致性模型适用于单个操作对象（如关系数据库的一行、键值数据库的一个 key-val 对或是文档数据库的一个文档），该数据可能存在多个副本；而隔离级别通常涉及多个操作对象，比如在并发事务中修改多个数据。</p><p>对于最严格一致性模型和隔离级别——线性一致性和串行化，<strong>还有一个重要的区别是</strong>，线性一致性提供了实时保证，而串行化则没有。串行化只保证多个并发事务的效果和它们以某种串行顺序执行时的效果一致，至于串行的顺序是否与实时的顺序一样（全局时钟下的顺序），它并没有保证，比如 n 个并发的事务可以产生 $n!$ 中串行顺序。</p><p>事实上，一个数据存储系统可以同时保证线性一致性和串行化，<strong>这类系统称为严格串行化（Strict Serializable）</strong>，这个模型保证了多个事务执行的结果等同于它们的串行执行结果，同时执行顺序与实时排序一致——就像单机单线程程序那样。</p><p>基于两阶段锁定的可串行化实现通常是线性一致性的。但是，可串行化的快照隔离不是线性一致性的，按照设计，它从一致的快照中进行读取，以避免读者和写者之间的锁竞争。一致性快照的要点就在于它不会包括该快照之后的写入，因此从快照读取不是线性一致性的。</p><h2 id="还有话说"><a href="#还有话说" class="headerlink" title="还有话说"></a>还有话说</h2><p>可以将线性一致性、顺序一致性、因果一致性和最终一致性归为<strong>以数据为中心的一致性模型</strong>。以数据为中心的一致性模型旨在为数据存储系统提供一个系统级别的全局一致性视图，讨论这一类一致性模型的角度都是当并发的客户端&#x2F;进程同时更新数据时，考虑系统每个副本的数据是否一致，以及系统提供的一致性。</p><p>而将单调读一致性、单调写一致性、读己之写一致性和 PRAM 一致性归为<strong>以客户端为中心的一致性模型</strong>。这类一致性模型从客户端的角度来观察分布式系统，不再从系统的角度考虑每个副本的数据是否一致，而是考虑客户端的读写请求的结果，从而推断出系统的一致性。</p><p>用一句话来说，以数据为中心的一致性模型常常考虑多个客户端时的系统状态，而以客户端为中心的一致性模型聚焦于单个客户端观察到的系统状态。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>超有用的资源</title>
    <link href="/2022/07/26/%E8%B6%85%E6%9C%89%E7%94%A8%E7%9A%84%E8%B5%84%E6%BA%90/"/>
    <url>/2022/07/26/%E8%B6%85%E6%9C%89%E7%94%A8%E7%9A%84%E8%B5%84%E6%BA%90/</url>
    
    <content type="html"><![CDATA[<p>记录一些提供有用的网址</p><h2 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h2><blockquote><p><a href="https://www.modb.pro/u/311299">墨天轮-数据库</a></p></blockquote><blockquote><p><a href="https://tool.ztool.cc/##">PDF 转换、录屏等众多工具</a></p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Database &amp; Distributed System Read List</title>
    <link href="/2022/07/26/Database%20&amp;%20Distributed%20System%20Read%20List/"/>
    <url>/2022/07/26/Database%20&amp;%20Distributed%20System%20Read%20List/</url>
    
    <content type="html"><![CDATA[<p>维护自己阅读过&#x2F;准备阅读的书籍、论文和博客等资源</p><h3 id="论文部分"><a href="#论文部分" class="headerlink" title="论文部分"></a>论文部分</h3><blockquote><ul><li><a href="https://dl.acm.org/doi/10.1145/1323293.1294281">Dynamo: amazon’s highly available key-value store</a></li><li><a href="https://pdos.csail.mit.edu/6.824/papers/gfs.pdf">The Google File System</a></li><li><a href="https://dl.acm.org/doi/10.1145/1365815.1365816">Bigtable: A Distributed Storage System for Structured Data</a></li><li><a href="https://pdos.csail.mit.edu/6.824/papers/zookeeper.pdf">ZooKeeper: Wait-free coordination for Internet-scale systems</a></li><li><a href="https://pdos.csail.mit.edu/6.824/papers/spanner.pdf">Spanner: Google’s Globally-Distributed Database</a></li><li><a href="https://pdos.csail.mit.edu/6.824/papers/zaharia-spark.pdf">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for<br>In-Memory Cluster Computing</a></li><li><a href="https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf">MapReduce: Simplified Data Processing on Large Clusters</a></li><li><a href="https://pdos.csail.mit.edu/6.824/papers/cr-osdi04.pdf">Chain Replication</a></li></ul></blockquote><h3 id="书籍部分"><a href="#书籍部分" class="headerlink" title="书籍部分"></a>书籍部分</h3><blockquote><ul><li>Parallel Join</li><li>Query Optimization</li><li>复制（单主复制、多主复制和无主复制）</li><li>Paxos 算法（听说论文特别难，我就直接先看别人的总结）</li><li>三阶段提交（虽然听得少，但还是得了解下）</li></ul></blockquote><h3 id="博客部分"><a href="#博客部分" class="headerlink" title="博客部分"></a>博客部分</h3><blockquote><ul><li><a href="https://pingcap.com/zh/blog/tiflash-is-open-sourced">TiDB TiFlash 的实现</a></li></ul></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>查询处理篇：Parallel Join</title>
    <link href="/2022/07/26/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9AParallel%20Join/"/>
    <url>/2022/07/26/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9AParallel%20Join/</url>
    
    <content type="html"><![CDATA[<p>在 Join 操作那篇<a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9AJoin%20%E6%93%8D%E4%BD%9C/" title="文章中">文章中</a>，介绍了几种 Join 算法，但有一个假设：参与 join 的两张表都完整地存放在了一处。在<a href="/2022/07/25/%E6%95%B0%E6%8D%AE%E5%88%86%E7%89%87/" title="数据分片">数据分片</a>中谈到了系统为了提高水平拓展能力和降低宕机风险会将数据分片存放在若干个节点上，<strong>那么在这种情况下 join 操作又该如何做呢</strong>？本文就讨论这一点。</p><blockquote><p>当然了 parallel join 的用处也不只上面提到的。在 shared-memory 架构中，在多个节点上同时执行 join 操作能够极大地减少算法的时间，如果是 shared-nothing 架构，如果参与 join 的表本身没有分片，采用 parallel join 方式需要将（部分）表复制到其他节点，这样带来的加速比不一定比本地多线程来得快。<strong>所以这一篇文章讨论的是数据分片后的 join 方法，也就是假设了所有的的数据都已经分片了</strong>。</p></blockquote><br><h2 id="等值连接"><a href="#等值连接" class="headerlink" title="等值连接"></a>等值连接</h2><p>假设 join 的两张表分别是 $r$ 和 $s$，有 $m$ 个节点，$N_1,N_2,…,N_m$，参与本次的 join. 算法的主要思想是：将这两张表用同一种分块方法，将元组分成 $m$ 份，$r_1-r_m, s_1-s_m$ 分别发送（复制）到 $m$ 个节点，由于是等值连接，join 属性值相等的元组（记录）一定会被发送到同一个节点，然后在节点本地执行 join 操作，最后汇总结果即可。</p><p>分块的方式一般有两种：</p><blockquote><p>按范围分（range partitioning）<br>使用哈希分（hash partitioning）</p></blockquote><br><p>如上所述，这两张表很可能已经被分片存储在不同的节点上了，那么可以采用下面的步骤：</p><blockquote><p>每一个存储了表 $r$ 分片的节点从磁盘读取这部分元组（记录），然后使用选定的分块方法将其中的每一条记录都发送到对应的节点，同时该节点开启另外一个线程接收别的节点发送来的记录。对表 $s$ 执行同样的操作，等到两张表都分块完成，每一个参与 join 的节点上都存在两张表的一个分块，这两个小分块的 join 操作就和<a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9AJoin%20%E6%93%8D%E4%BD%9C/" title="查询处理篇：Join 操作">查询处理篇：Join 操作</a>中的情况一样了。需要注意，如果分块的方法是哈希，每个节点本地的 join 也是哈希 join，那么这两次的哈希算法不能一样。</p></blockquote><br><p>如果分块后在每个节点本地使用的是哈希 join，那么这样的 parallel join 又称为 partitioned parallel hash join，如果每个节点本地使用的是 merge join，那么这样的 parallel join 又称为 partitioned parallel merge join. 当然本地还可以使用 nested loop join 等方法，不再赘述。</p><p>当然了，如果数据分片的属性就是本次 join 属性，那么分块这一过程就可以免了，因为分块已经完成了，只需要在每个节点上并行执行 join 操作即可。</p><h2 id="Fragment-and-Replicate-Join"><a href="#Fragment-and-Replicate-Join" class="headerlink" title="Fragment-and-Replicate Join"></a>Fragment-and-Replicate Join</h2><p>上面提到的方法中，分块时不论使用的是范围分块还是哈希分块都只适用于等值连接，这样具有相同 join 属性值的元组才会分到同一个块。那么当 join 条件不再是等值连接时，如 $r \bowtie_{r.a &lt; s.b} s$，算法将不再适用。</p><p>幸好还有一种叫做 fragment-and-replicate join，下面简称 FARJ，的方法可以使用。FARJ 有两种，一种被称为非对称的 FARJ（asymmetric farj），先来介绍它。</p><p>非对称 FARJ 算法有如下步骤：</p><blockquote><ul><li>系统选择一个表，假设为 $r$，进行分块并发送到相应的参与 join 的节点上，可以使用任何分块方法，包括 round-robin.</li><li>然后将另外一张表，假设为 $s$，复制到每一个参与 join 的节点上。</li><li>最后在每一个节点上执行 $r_i$  join  $s$，在节点上可以使用任何 join 方法。</li></ul></blockquote><blockquote><p>需要注意，这里的分块并不一定按照 join 属性来，因为没有影响。由于本文假设的是数据已经分片了，那么其实只需要执行后面两个步骤就可以了。这个方法又被称作 broadcast join，<strong>既可以用于等值连接，又可以用于非等值连接</strong>，非常有用。如果 join 的两张表一张很小，一张很大，那么选择将小的那一张表复制到所有参与 join 的节点，这个代价一般要低于重新分块大表的代价。</p></blockquote><br><p>谈完非对称 FARJ，下面看看对称版的 FARJ：</p><blockquote><p>系统将表 $r$ 分成 $n$ 块，$r_1,r_2,…,r_n$，将表 $s$ 分成 $m$ 块，$s_1,s_2,…,s_m$，$n$ 和 $m$ 的大小没有比如的关系，当然了，这样一来就有 $n*m$ 个节点参与 parallel join. 非对称版本其实是一个特例，即 $m&#x3D;1$. </p></blockquote><blockquote><p>假设参与 join 的节点为：$N_{1,1}, N_{1,2},…,N_{1,m}, N_{2,1},…,N_{n,m}$. 那么节点 $N_{i,j}$ 上执行的是分块 $r_i$ 和分块 $s_j$ 的 join 操作。为了实现这一点，$r_i$ 需要发送到 $N_{i,1}, N_{i,2},…,N_{i,m}$ 节点上，$s_j$ 需要发送到 $N_{1, j}, N_{2, j},…,N_{n, j}$ 节点上。分块完成后，在每个节点上执行 join 操作，最后汇总结果即可。</p></blockquote><blockquote><p>对称版的 FARJ 同样适用于任何连接条件。两者的示意图如下：<br><img src="/img/parallel_join/1.jpg"><br>对于两者的评价，摘抄书中内容如下：<br>Fragment-and-replicate join has a higher cost than partitioning, since it involves replication of both relations, and is therefore used only if the join does not involve equi-join conditions. Asymmetric fragment-and-replicate, on the other hand, is useful even for equi-join conditions, if one of the relations is small, as discussed earlier.</p></blockquote><br><p>还有一点需要注意，那就是使用非对称 FARJ 方法计算左外连接（定义见<a href="/2022/08/22/Join%20%E7%9A%84%E7%A7%8D%E7%B1%BB/" title="Join 的种类">Join 的种类</a>），$r\ ⟕_{\sigma}\ s$，时，对于复制和分块表的选择有限制：左外连接要求保留左表中那些未被匹配上的元组，并用空值填充一些字段。如果将表 $r$ 复制，表 $s$ 分块，那么在节点 $N_i$ 上可能存在 $r$ 的一些元组不能匹配 $s_i$，但可以匹配在节点 $N_j$ 上的 $s_j$，这样一来单个节点就不能判断这些不能匹配的元组是否丢弃还是保留并填充空值。但如果将表 $r$ 分块，表 $s$ 复制，就不会存在这样的问题。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>查询处理篇：Parallel Sort</title>
    <link href="/2022/07/26/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9AParallel%20Sort/"/>
    <url>/2022/07/26/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9AParallel%20Sort/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>查询优化（Query Optimization）</title>
    <link href="/2022/07/26/%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%EF%BC%88Query%20Optimization%EF%BC%89/"/>
    <url>/2022/07/26/%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%EF%BC%88Query%20Optimization%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>查询优化是干嘛的？有什么意义？贴上书中原文吧：</p><blockquote><p><strong>Query optimization</strong> is the process of selecting the most efficient query-evaluation plan from among the many strategies usually possible for processing a given query, especially if the query is complex. <u>We do not expect users to write their queries so that they can be processed efficiently</u>. Rather, we expect the system to construct a query-evaluation plan that minimizes the cost of query evaluation. This is where query optimization comes into play.</p></blockquote><blockquote><p><u>One aspect</u> of optimization occurs at the relational-algebra level, where the system attempts to find an expression that is equivalent to the given expression, but more efficient to execute. <u>Another aspect</u> is selecting a detailed strategy for processing the query, such as choosing the algorithm to use for executing an operation, choosing the specific indices to use, and so on.</p></blockquote><h2 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h2><p><img src="/img/query_optimization/1.png"><br>假设有这样一个需求：“查找所有音乐学院老师的名字和他们教授的课程名（course title）”。假设该需求对应的关系代数表达式（由 SQL 简单转化而来）为：</p><blockquote><p>$\Pi_{name, title}(\sigma_{dept-name &#x3D; Music}(instructor \bowtie (teaches \bowtie \Pi_{course-id, titile}(course))))$</p></blockquote><blockquote><p>上图中左边是该表达式的算子树。</p></blockquote><br><p>在上面的表达式中，$instructor \bowtie (teaches \bowtie \Pi_{course-id, title}(course))$ 可能会产生非常大的中间结果，实际上我们只关心 dept-name 为音乐学院的教职工，所以，可以将该表达式做一个等价变换：</p><blockquote><p>$\Pi_{name, title}((\sigma_{dept-name &#x3D; Music}(instructor)) \bowtie (teaches \bowtie \Pi_{course-id, titile}(course)))$</p></blockquote><blockquote><p>上图中右边是该表达式的算子树。</p></blockquote><br><p>接下来给该算子树中的每一种算子选择一个具体的算法，该表达式就可以成为具体的执行计划，如下图：<br><img src="/img/query_optimization/2.jpg"></p><p>由上述例子可知，查询优化主要是将用户编写的查询做一些变换以求得到代价最小的执行计划。一般有如下步骤：</p><blockquote><ol><li>根据当前的表达式生成一组逻辑等价的表达式。</li><li>对于生成的每一个表达式，为其中每个操作选择具体的算法，生成一组执行计划。一般而言，每个操作都可以通过多个算法实现，所以每个表达式可以产生多个执行计划。</li><li>衡量每一个执行计划的代价，然后选择最小的那一个。一般是通过统计信息，比如表的大小、索引深度等来衡量一个执行计划的代价。</li></ol></blockquote><blockquote><p>在优化的过程中，上述三个步骤通常会交叉重叠。比如，可以继续等价变换第三步选择的最小代价的执行计划的表达式。</p></blockquote><h2 id="关系代数表达式的变换"><a href="#关系代数表达式的变换" class="headerlink" title="关系代数表达式的变换"></a>关系代数表达式的变换</h2>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>分布式系统的 Debug</title>
    <link href="/2022/07/25/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84-Debug/"/>
    <url>/2022/07/25/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84-Debug/</url>
    
    <content type="html"><![CDATA[<p>这篇文章很重要，真的很重要。</p><br><div class="row">    <embed src="debugging.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数据分片</title>
    <link href="/2022/07/25/%E6%95%B0%E6%8D%AE%E5%88%86%E7%89%87/"/>
    <url>/2022/07/25/%E6%95%B0%E6%8D%AE%E5%88%86%E7%89%87/</url>
    
    <content type="html"><![CDATA[<p>数据分片又叫数据分区（data partitioning），指的是将一个数据集分成多个部分分开存储在不同的磁盘或者机器，这些分片在实践中表现为一个个小数据库，为了与节点&#x2F;网络故障导致的分区相区别，之后本文统称为分片。</p><p>对于<u>关系型数据库</u>，数据以表的形式存储，因此数据分片又分为两种：</p><ul><li>水平分片（horizontal partitioning），又称为 sharding，以元组为单位，若干个元组构成一个分片。</li><li>垂直分片（vertical partitioning），又称为列式存储，以列为单位，若干个列（整列）构成一个分片。</li></ul><p>对于非关系型数据库，自然就没有这些区分了。本文主要讨论水平分片，内容同样适用于非关系型数据。</p><p><u>数据分片是为了啥</u>？主要有以下的好处：</p><blockquote><ol><li>主要是为了提高水平扩展（horizontal scaling）能力。水平扩展是指添加更多的机器，以分散负载，允许更多的流量和更快的处理（不同分片的请求可以并行，减少响应时间）。与水平拓展对应的是垂直扩展（vertical scaling），垂直扩展也称为向上扩展（scaling up），是指升级现有服务器的硬件，通常是添加更多内存或 CPU 到单台服务器中。</li><li>分片还可以通过减少宕机的影响，使应用程序更稳定可靠。对于分片数据库，宕机可能只会影响单个分片。即使这可能使某些用户无法使用应用程序或网站部分功能，但仍会低于整个数据库崩溃带来的影响。</li></ol></blockquote><h3 id="数据分片方法"><a href="#数据分片方法" class="headerlink" title="数据分片方法"></a>数据分片方法</h3><p>访问数据一般有以下三种方式：</p><ul><li>扫描整个数据集（所有分片的集合）</li><li>定位某一条数据，被称为单点查询，给定一个属性值，查询一条具体的数据。</li><li>范围查询，给定一个属性值的区间，查询所有相应属性值落在该区间的所有数据。</li></ul><p>在介绍数据分片方法时，也会讨论它们各自在访问数据时的优缺点。假设有 $n$ 个节点，$N_1, N_2, …, N_n$ 用于存储分片，有以下三种方式完成数据分片：</p><p><font color=red>Round-Robin</font>. 该方法以任一顺序扫描每一条数据，将第 $i$ 条数据发送到 $N_{((i-1)\ mod\ n) + 1}$ 节点。 </p><blockquote><p>该方法能够均匀分配数据，使得每个节点都具有相同数量的数据。该方法非常适合按顺序扫描整个数据集，相比于只使用一个节点，扫描整个数据集只需要花费 $\frac{1}{n}$ 的时间，即加速比为 $n$，然而对于单点查询和范围查询就不太又好了，需要查询每一个节点来获取想要的数据。</p></blockquote><p><font color=red>Hash Partitioning</font>. 该方法先确定分片需要的属性，然后将属性值映射到集合 $\{1,2,…,n\}$，然后将数据存储到对应的节点上。</p><blockquote><p>该方案可以说最适合单点查询了，如果查询请求中包含分配属性的值，那么映射后找到对应的节点，然后将请求路由过去就好了，其他不相干的节点就可以同时处理其他的请求。如果哈希函数选取得很好，能够将数据均匀的分配到 $n$ 个节点，那么扫描整个数据集的加速比也能接近 $n$. <u>该方式对范围查询不友好</u>，需要扫描所有的节点获取想要的数据。</p></blockquote><p><font color=red>Range Partitioning</font>. 该方法先确定属性，然后将属性值划分为多个区间，处于不同区间的属性值存储在不同的节点上。</p><blockquote><p>先确定属性（设为 $A$），和一个分片向量（partitioning vector）$[v_1,v_2,…,v_{n-1}]$，然后按照这样的方式分片：考虑一条数据 $t,\ t[A] &#x3D; x$，如果 $x &lt; v_1$，则将 $t$ 存储在 $N_1$；如果 $x &gt;&#x3D; v_{n-1}$，则 $t$ 存储在 $N_n$；如果 $v_i &lt;&#x3D; t &lt; v_{i+1}$，则 $t$ 存储在 $N_{i+1}$，如下图：<br><img src="/img/data_partitioning/1.jpg"><br>该方式对单点查询和<u>范围查询都很友好</u>。</p></blockquote><h3 id="处理数据倾斜（skew）"><a href="#处理数据倾斜（skew）" class="headerlink" title="处理数据倾斜（skew）"></a>处理数据倾斜（skew）</h3><p>除了 Round-Robin 方法，任何一种数据分片方法都不能阻止数据倾斜的存在，<u>数据倾斜是指</u>：一些分片中具有非常多的数据，而另外一些分片中的数据量却很少，被称为 data distribution skew，一般是由如下原因产生的：</p><ul><li>属性值的分布本身就不均匀（Attribute-Value Skew）。数据分片需要确定属性，而该属性的取值可能就存在倾斜，比如大学成员年龄就大多分布在[20, 30].</li><li>因为方法选择不当导致数据倾斜（Partitioning Skew）。对于范围分片来说，如果分片向量选择不当，就可能导致数据倾斜；又如哈希函数选择不当时，也会有数据倾斜。</li></ul><blockquote><p>数据分片的初衷是提升并行度和减少单个节点的负载，然而就算是数据在多个分片中存在少许的倾斜，对加速比的影响都很大（<strong>后续会写一篇关于并行系统加速比的文章</strong>）。考虑这样一个例子，如果数据集有 1000 条数据，将其分成 10 个分片，如果存在数据倾斜，那么有的分片会多于 100 条数据，有的会少于 100 条。假如某一个分片有 200 条数据，如果按照最长处理时间，那么加速比就只有 5，而不是理想的 10.<br>另外除了以上的数据分布倾斜外，还可能有处理倾斜（execution skew），即请求特别偏爱某些数据，导致某些节点的负载很高。<u>对于处理倾斜</u>，可以通过为每个分片复制多个备份来减轻负载，如果操作大多局限于某些列，那么可以采用垂直分片，减少不必要的列数据传输。</p></blockquote><br><p>下面将介绍一些处理倾斜的方法。</p><p><u>第一种方法是 Balanced Range-Partitioning Vectors</u>. 假如数据集的分布已知，那么就可以根据其分布特点设计一个不会产生数据倾斜的分片向量，一般有两种方式：1) 可以先对数据集按照某个属性进行排序，然后再均分数据。2) 方法一显然代价较高，那么可以通过记录属性值的频率或采用构造直方图的方式避免排序。</p><p>上述方法一个很明显的缺点就是它不太灵活，他需要提前知道数据的分布，而且如果随着数据集的增长数据倾斜依然会发生（重新执行分片操作代价昂贵）。</p><br><p><u>第二种方法是 Virtual Node Partitioning</u>. 该方法在真实节点之上创建数倍的虚拟节点（virtual node），若干个虚拟节点被映射到同一个真实节点，数据分片时，分片直接对应的就是虚拟节点，除此之外系统需要维护一个映射数据结构：[attribute -&gt; virtual node, virtual node -&gt; real node]，如下图：<br><img src="/img/data_partitioning/2.jpg"></p><blockquote><p>采用的是 range partitioning，tablet 就是 virtual node.</p></blockquote><p><u>为什么要多一层虚拟节点呢</u>？这其实增加了灵活性，我们可以动态地记录一个真实节点的数据量和负载，如果某个节点的数据量太多或负载太高，我们就可以将映射到该真实节点的某些虚拟节点迁移到其他负载较小的真实节点上或新节点上，而不会影响到其他的节点和数据，代价较低，最后再修改映射表</p><p>virtual node 的点子不错，<u>还可以再进一步</u>：随着数据集的增长，虚拟节点的数据量或负载会变得很大（高），无论往哪里迁移都是一个很大的负担，迁移的代价也很高，这其实是一个粒度问题。为此就有了下面的方法三。</p><br><p><u>第三种方法是 Dynamic Repartitioning</u>. 该方法同样使用 virtual node，不同之处在于，对于数据量太大或负载太高的虚拟节点，该方法选择从中分裂出一个新的虚拟节点，类似 B+ 树的插入和可拓展哈希的分裂操作。新分裂出的虚拟节点就可以迁移到其他的真实节点上，以更小的粒度均衡负载。</p><p>dynamic repartitioning 在分布式数据库和文件存储中用得非常多，如 GFS 和 BigTable，在这些系统中，它们用 table 指代一个数据集，然后一个 table 被分片为若干个 tablets，tablet 对应的其实就是 virtual node. 系统会维护一个 partition table（如上图）记录数据、tablets、真实节点直接的映射关系。partition table 一般会存储多个备份在客户端或 router（未必是路由器）中，下面是一些描述，其实大多数内容在上面已经说过了：</p><blockquote><p>Read requests must specify a value for the partitioning attribute, which is used to identify the tablet which could contain a record with that key value; a request that does not specify a value for the partitioning attribute would have to be sent to all tablets. A read request is processed by using the partitioning key value v to identify the tablet whose range of keys contains v, and then sending the request to the real node where the tablet resides. <u>The request can be handled efficiently at that node by</u> maintaining, for each tablet, an index on the partitioning key attribute.</p></blockquote><blockquote><p>Write, insert, and delete requests are processed similarly, by routing the requests to the correct tablet and real node, using the mechanism described above for reads.</p></blockquote><blockquote><p>The above scheme allows tablets to be split if they become too big; the key range corresponding to the tablet is split into two, with a <u>newly created tablet getting half the key range</u>. Records whose key range is mapped to the new tablet are then moved from the original tablet to the new tablet. The partition table is updated to reflect the split, so requests are then correctly directed to the appropriate tablet.</p></blockquote><blockquote><p>If a real node gets overloaded, either due to a large number of requests or due to too much data at the node, some of the tablets from the node can be moved to a different real node that has a lower load. <u>Tablets can also be moved similarly</u> in case one of the real nodes has a large amount of data, while another real node has less data. Finally, if a new real node joins a system, some tables can be moved from existing nodes to the new node. Whenever a tablet is moved to a different real node, the partition table is updated; subsequent requests will then be sent to the correct real node.</p></blockquote><p>看吧，十分灵活。</p><br><p><u>第四种方法是使用 Consistent Hashing</u>. 这是一个较大的话题，直接附上原文的描述，详情参阅文章<a href="/2022/08/03/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C/" title="一致性哈希">一致性哈希</a>：</p><blockquote><p>An alternative fully distributed approach is supported by a hash based partitioning scheme called <u>consistent hashing</u>. In the consistent hashing approach, keys are hashed to a large space, such as 32 bit integers. Further, node (or <u>virtual node</u>) identifiers are also hashed to the same space. A key $k_i$ could be logically mapped to the node $n_j$ whose hash value $h(n_j)$ is the highest value among all nodes satisfying $h(n_j) &lt; h(k_i)$. But to ensure that every key is assigned to a node, hash values are treated as lying on a cycle similar to the face of a clock, where the maximum hash value $Max_{hash}$ is immediately followed by 0. Then, key $k_i$ is then logically mapped to the node $n_j$ whose hash value $h(n_j)$ is the closest among all nodes, when we move anti-clockwise in the circle from $h(k_i)$.</p></blockquote><blockquote><p><u>Distributed hash tables</u> based on this idea have been developed where there is no need for either a master node or a router; instead each participating node keeps track of a few other peer nodes, and routing is implemented in a cooperative manner. New nodes can join the system, and integrate themselves by following specified protocols in a completely peer-to-peer manner, without the need for a master. </p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>PingCAP TinyKV 总结</title>
    <link href="/2022/07/22/PingCAP%20TinyKV%20%E6%80%BB%E7%BB%93/"/>
    <url>/2022/07/22/PingCAP%20TinyKV%20%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h2 id="TiDB"><a href="#TiDB" class="headerlink" title="TiDB"></a>TiDB</h2><p>TalentPlan TinyKV 2022 学习营马上结束了，项目也写得差不多了，此篇文章作为一个总结，也不免我两个月的 coding.</p><p>本来呢我是想写一篇关于 TiKV（TinyKV 是它的简单版本）的学习记录，但在看 PingCAP 官网的文档时越看越有意思（它们的的文档写得真好，详细），于是决定干脆学习下 TiDB 好了，看看现代数据库都干了什么事。所以这篇文章记录一些 TiDB 学习过程中的东西，基本就是 TiDB 文档的搬运工，汇总一下方便阅读。</p><blockquote><p>另外，知乎上这个<a href="https://www.zhihu.com/question/58767602/answer/1662140715">如何评价 TiDB</a>问题下有很多比较好的回答可以看看。而且还炸出了很大大佬，可以关注下。</p></blockquote><br><p><img src="/img/tinyKV/2.png"><br>上图是 TiDB 的架构图，下面单独介绍各个组成部分</p><h3 id="TiDB-Server"><a href="#TiDB-Server" class="headerlink" title="TiDB Server"></a>TiDB Server</h3><blockquote><p>来看看它具体干了什么事情，下面是它的架构图：<br><img src="/img/tinyKV/3.png"><br>用户的 SQL 请求会直接或者通过 Load Balancer 发送到 TiDB Server，TiDB Server 会解析 MySQL Protocol Packet，获取请求内容，对 SQL 进行语法解析和语义分析，制定和优化查询计划，执行查询计划并获取和处理数据。数据全部存储在 TiKV 集群中，所以在这个过程中 TiDB Server 需要和 TiKV 交互，获取数据。最后 TiDB Server 需要将查询结果返回给用户。TiDB 层本身是无状态的（不存储数据），实践中可以启动多个 TiDB 实例，通过负载均衡组件（如 LVS、HAProxy 或 F5）对外提供统一的接入地址，客户端的连接可以均匀地分摊在多个 TiDB 实例上以达到负载均衡的效果。</p></blockquote><blockquote><p><u>一个问题</u>：TiDB 是分布式关系型数据库，而 TiKV 是 key-val 存储引擎，TiDB 是如何将库表中的数据映射到 TiKV 中的 key-val 键值对的？<a href="https://docs.pingcap.com/zh/tidb/stable/tidb-computing">参考文档</a> </p></blockquote><h3 id="PD-placement-driver-Server"><a href="#PD-placement-driver-Server" class="headerlink" title="PD (placement driver) Server"></a>PD (placement driver) Server</h3><blockquote><p><u>PD (Placement Driver) Server</u>. 整个 TiDB 集群的元信息管理模块，负责存储每个 TiKV 节点实时的数据分布情况和集群的整体拓扑结构，并为分布式事务分配事务 ID. PD 不仅存储元信息，同时还会根据 TiKV 节点实时上报的数据分布状态，下发数据调度命令给具体的 TiKV 节点（region 分裂&#x2F;合并&#x2F;迁移），可以说是整个集群的“大脑”。此外，PD 本身也是由至少 3 个节点构成，拥有高可用的能力。建议部署奇数个 PD 节点。</p></blockquote><blockquote><p>TiKV 的每一个 raft group 都是一个 region 的冗余复制集，而 region 数据不断增减时，它的大小也会不断发生变化，因此必须支持 region 的分裂和合并，才能确保 TiKV 能够长时间稳定运行。region split 会将一段包含大量数据的 range 切割成多个小段，并创建新的 raft Group 进行管理，如将 [a, z) 切割成 [a, h), [h, x) 和 [x, z)，并产生两个新的 raft group。region merge 则会将 2 个相邻的 raft group 合并成一个，如 [a, h) 和 [h, x) 合并成 [a, x）。这些逻辑也在 raftstore 模块中实现。这些特殊管理操作也作为一个特殊的写命令走一遍上节所述的 raft propose&#x2F;commit&#x2F;apply 流程。为了保证 split&#x2F;merge 前后的写命令不会落在错误的范围，我们给 region 加了一个版本的概念。每 split 一次，版本加一。假设 region A 合并到 region B，则 B 的版本为 max(versionB, versionA + 1) + 1。更多的细节实现包括各种 corner case 的处理在后续文章中展开。</p></blockquote><blockquote><p>一些参考文档，主要是调度相关，其实是牵扯到 multi-raft 内容:</p><ul><li><a href="https://docs.pingcap.com/zh/tidb/stable/tidb-scheduling">TiDB 数据库的调度，主要是介绍调度考虑的内容，是一个概述。</a></li><li><a href="https://docs.pingcap.com/zh/tidb/stable/tikv-overview#region-%E4%B8%8E-raft-%E5%8D%8F%E8%AE%AE">这篇文章也简述了迁移&#x2F;分裂&#x2F;合并的过程，可以一看。</a></li><li><a href="https://pingcap.com/zh/blog/tikv-source-code-reading-20">当一个 region 的数据量太大时，会分裂 region，可以参考这篇文章。</a></li><li><a href="https://pingcap.com/zh/blog/tikv-source-code-reading-21">当两个相邻的 region 的数据量太小时，会合并它们。</a></li></ul></blockquote><h3 id="TiKV-Server"><a href="#TiKV-Server" class="headerlink" title="TiKV Server"></a>TiKV Server</h3><blockquote><p><u>TiKV Server</u>. 负责存储数据，从外部看 TiKV 是一个分布式的提供事务的 key-value 存储引擎。存储数据的基本单位是 region，每个 region 负责存储一个 key range（从 startKey 到 endKey 的左闭右开区间）的数据，一个 region 有多个备份，使用 raft 协议实现副本一致性，一个 region 备份会存在某个 TiKV 节点上，一个 TiKV 可能存在多个 region 备份（属于不同的 region）.</p></blockquote><blockquote><p>TiKV Server 包含这样几个重要的部分，<a href="https://docs.pingcap.com/zh/tidb/stable/tikv-overview">一篇概览性文章</a>：</p><ul><li>分布式事务的实现，可以参考<a href="https://docs.pingcap.com/zh/tidb/stable/optimistic-transaction">文章</a>。</li><li>multi-raft 部分，上面 PD 已经涉及到了。</li><li>key-val 存储引擎部分。key-val 存储引擎有两个，rocksdb 和 titan. rocksDB 是在 <a href="/2022/08/04/LevelDB%20%E4%B8%AD%E7%9A%84%20LSM/" title="levelDB">levelDB</a> 基础上开发的，主要的改变应该是 column family，可以参考<a href="https://docs.pingcap.com/zh/tidb/stable/rocksdb-overview">文档</a>. titan 也是老朋友了，其实现和 <a href="/2022/08/04/badgerDB%20%E4%B8%AD%E7%9A%84%20LSM/" title="badgerDB">badgerDB</a> 差不多，可以参考<a href="https://docs.pingcap.com/zh/tidb/stable/titan-overview">文档</a>.</li><li>还包括一些计算，是 TiDB Server 下推来的，可以参考 TiDB Server 部分是外链。</li></ul></blockquote><h3 id="TiFlash"><a href="#TiFlash" class="headerlink" title="TiFlash"></a>TiFlash</h3><blockquote><p><u>TiFlash</u>：TiFlash 是一类特殊的存储节点。和普通 TiKV 节点不一样的是，在 TiFlash 内部，数据是以列式的形式进行存储，主要的功能是为分析型的场景加速，它会实时地从 TiKV 拷贝数据以列式存储用于分析。<br>对这个不太熟，有时间可以了解下，参考<a href="https://docs.pingcap.com/zh/tidb/stable/tiflash-overview">文档</a>.</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>TiDB 属于 NewSQL，至于 SQL、NoSQL 和 NewSQL 之间的区别，因为还没有太多的实际生成经验所以也没有太多的体会，只能摘录一些别人的总结（慢慢更新）：</p><blockquote><p>SQL 一般是指传统的单机关系型数据库，它们的特点是数据存储采用关系型模型，支持事务，能够提低时延和高并发的事务处理，是典型的 OLTP. 如果业务量不大一般这类数据库是第一选择。SQL 的主要缺点在于拓展性，为什么？（后面慢慢补）<br>拓展性的一方面是指数据模型固定，不太支持图片、文档等无结构&#x2F;半结构的数据类型，于是 NoSQL 出世了？<br>NewSQL 又可以叫做分布式关系型数据库，同时继承了传统关系型数据库的事务特性和 NoSQL 的拓展性。</p></blockquote><br><p>未整理得参考资料</p><blockquote><p><a href="https://blog.csdn.net/stevensxiao/article/details/51872795">关系型数据库横向扩展的三种方法</a></p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>CAP &amp; BASE</title>
    <link href="/2022/07/17/CAP%20&amp;%20BASE/"/>
    <url>/2022/07/17/CAP%20&amp;%20BASE/</url>
    
    <content type="html"><![CDATA[<p>数据一般有多个备份，存储在多个分布式节点上。像 raft 这类共识算法所做的努力是维护多个数据备份之间的一致性。raft 算法的正常执行需要集群中大多数节点能够相互通信，如果不能满足这个条件，为了满足一致性，集群将对更新请求不可用。</p><p>而在大规模分布式系统中，节点故障和网络分区几乎无法避免。如果和某个客户端相连的分区（可能多个）中节点数都不足大多数，那该客户端将无法执行更新请求（读请求有可能，看具体实现）；如果集群的所有分区的节点数都不足大多数（如三个节点三个分区），那么整个集群也处于更新不可用状态。</p><h2 id="CAP-理论"><a href="#CAP-理论" class="headerlink" title="CAP 理论"></a>CAP 理论</h2><p>上面描述的情况中冲突在于，分区发生时，可用性和一致性往往不能兼顾（取决于分区的严重性），<u>那么有没有方法可能做到这一点呢？CAP 理论告诉我们：不可能</u>。CAP 指出，任何分布式数据库最多只能同时保证以下三个特性中的两个：</p><ul><li>一致性（Consistency），指线性一致性。</li><li>可用性（Availability）</li><li>分区容忍性（Partition-Tolerance）</li></ul><p>一致性和可用性都接触过了，这里解释下<u>分区容忍性的含义</u>，要想访问到数据就必须与存储该数据的机器联通，如果数据只存储一份，当该机器不与外界联通时（发生网络分区或节点故障），外界将彻底不能访问该数据（不能容忍分区的发生），解决办法很简单，那就是在分布式节点中存储数据的多个备份，这样只要有一台机器能与外界连通，数据就能被访问（提高了分区容忍性），有了备份自然就需要复制协议（一致性协议）。</p><p><font color=red>很明显，分区容忍性是一定需要满足的，而在大型分布式系统中，节点故障和网络分区又一定会发生，那么就只能在一致性和可用性之间抉择。</font></p><h2 id="BASE-理论"><a href="#BASE-理论" class="headerlink" title="BASE 理论"></a>BASE 理论</h2><p>CAP 指出了问题，但并没有给出问题的解决方式，<u>BASE 理论做到了这一点</u>，它包含了以下三个特性：</p><ul><li>基本可用性（Basically Available）</li><li>软状态（Soft State）</li><li>最终一致性（Eventually Consistent）</li></ul><p>CAP 中的一致性是强一致性，如银行系统，但并不是所有分布式系统都必须时时刻刻满足强一致性，比如，两个用户同时编辑同一份文本，如果网络断开或节点发生故障（分区），他们的编辑就会被保存在客户端的本地（可以理解为一个节点），待分区从分区恢复后，双方上传自己的更新时就可能会发生冲突，这个冲突并不是很严重，可以稍后解决。<u>对于这样的系统 BASE 理论给出了解决方法</u>：</p><blockquote><p>当分区发生后，系统照样处理更新请求，提供基本的可用性，但这样一来，各个节点的状态就可能不一致，也就是系统处于软状态，待分区被解决后，再处理冲突，使系统达到最终一致性。</p><p>MongoDB 等数据库系统可以配置处理写请求时的最少参与节点数目（数据备份数），如果配置为集群的大多数，那么就不会存在不一致，如果少于就会有不一致的风险。</p></blockquote><br><p><u>所以 BASE 的重点落在了最终一致性上面</u>，要做到这一点，首先要能检测出不一致（冲突），然后解决它们，下面介绍一种方法。</p><h2 id="Merkle-Tree"><a href="#Merkle-Tree" class="headerlink" title="Merkle Tree"></a>Merkle Tree</h2><p>梅克尔树（Merkle Tree）又称哈希树（hash tree），是一种用来高效检测数据集之间的不一致的数据结构。哈希树在区块链中用得很多，比如验证区块中交易记录的完整性，不过这里讨论的是它在数据复制中的作用：假设数据有多个副本，这些副本之间的数据可能存在不一致，在最终一致性系统里，就需要方法来找出不一致的数据，当然可以通过一一比较两个副本的所有数据做到，不过开销就很难接受。下面介绍哈希树，看它是如何高效地完成这个工作的。</p><p>以一个例子阐述哈希树的工作原理。<br><img src="/img/cap_base/1.jpg"><br>如图所示，有一个数据副本，它包含 8 个键值对数据。哈希树选择一个哈希函数 $h_1$ 将每个键值对的 key 映射成一个 $n$ 位的整数。</p><blockquote><p>这篇文章写完后，再写数据的复制，里面要提到哈希树的应用，自然也会提到一致性哈希</p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>查询处理篇：Selection 操作</title>
    <link href="/2022/07/15/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9ASelection%20%E6%93%8D%E4%BD%9C/"/>
    <url>/2022/07/15/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9ASelection%20%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<blockquote><p>假设一张表的所有记录都存储在一个文件中，并且该文件的 block 在磁盘上连续存储，这样计算起来简单些 ^_^</p></blockquote><p>本文研究选择操作 $\sigma_{expression}$，下面分各种情况介绍操作的执行过程和代价。</p><h3 id="采用线性扫描"><a href="#采用线性扫描" class="headerlink" title="采用线性扫描"></a>采用线性扫描</h3><p>线性扫描也就是从文件的第一个 block 开始，读取每一条记录，判断它是否满足选择条件（$expression$），这种方法的好处是在任何情况下都可以用，虽然可能性能不好。</p><p>如果选择条件是 non-key 上的等值判断或比较，<u>那么它的代价为</u>：<br>$$ t_S + b_r * t_T $$</p><blockquote><p>其中 $b_r$ 表示文件总共包含多少个 block. 在扫描过程中，初始需要一次随机 I&#x2F;O 找到第一个 block，满足条件的记录可能不止一条，而线性扫描又不考虑有序之类的情况，所以必须扫描全文件。</p></blockquote></br><p>如果选择条件是 key 上的等值判断，<u>那么它的代价为</u>：<br>$$ average &#x3D; t_S + \frac{b_r}{2} * t_T $$</p><blockquote><p>这个也挺好理解的，key 是不重复的，满足条件的记录最多只有一条，所以平均来说，只需扫描一半的 block.</p></blockquote></br><p>下面讨论使用索引的情况（当然假设相应的索引存在，叶子节点存储指向记录的指针）。这里只涉及 B+ 树索引，使用 $h_i$ 表示树高（从根节点到叶子节点需要访问 block 的数量），并且考虑没有节点提前驻留在内存中，下同。事实上，B+ 树的节点有 90% 是叶节点，非叶子节点很少，因此绝大多数数据库系统会提前将所有非叶子节点读取到内存中，因此 $h_i$ 在绝大多数情况下等于 1，不过这里不做这个假设。下面提到的 key&#x2F;non-key 表示主键（primary key），与索引的 search key 没有必然的联系，相关讨论见 <a href="/2022/07/04/%E7%B4%A2%E5%BC%95%E7%AF%87%E9%9B%B6%EF%BC%9A%E7%B4%A2%E5%BC%95%E6%80%BB%E8%AE%BA/" title="索引篇零：索引总论">索引篇零：索引总论</a>。</p><h3 id="使用-B-树-Primary-Index"><a href="#使用-B-树-Primary-Index" class="headerlink" title="使用 B+ 树 Primary Index"></a>使用 B+ 树 Primary Index</h3><p>如果选择条件是 key 上面的等值判断，<u>那么它的代价为</u>：<br>$$ (h_i + 1) * (t_T + t_S) $$</p><blockquote><p>从根节点到叶节点需要读取 $h_i$ 个 block 到内存中，它们不太可能是连续的，所以为了读取包含目标 search key value 的 block 分别需要 $h_i$ 次随机访问（定位 block）和 $h_i$ 次 block 传输，再分别需要额外的 1 次随机 I&#x2F;O 和 block 传输获取指定的记录。</p></blockquote><br><p>如果选择条件是 non-key 上面的等值判断或比较，<u>那么它的代价为</u>：<br>$$ h_i * (t_T + t_S) + t_S + b * t_T $$</p><blockquote><p>该 search key 非主键，所以不一定唯一，存在多条 search key value 相等的记录。又因为这是 primary index，所以 search key value 相等的记录在文件中必然连续存储。无论是等值判断还是比较，都会找到满足条件的第一条记录，然后顺序扫描即可。从根节点找到包含第一条记录指针的叶节点需要 $h_i$ 随机 I&#x2F;O 和 block 传输，然后根据该指针，再一次随机 I&#x2F;O 定位到包含第一条记录的 block（中的该记录的偏移量），假设这些 search key value 相等的记录存储在连续的 $b$ 个 block中，需要再加上 $b$ 次 block 传输。</p></blockquote><h3 id="使用-B-树-Secondary-Index"><a href="#使用-B-树-Secondary-Index" class="headerlink" title="使用 B+ 树 Secondary Index"></a>使用 B+ 树 Secondary Index</h3><p>如果选择条件是 key 上面的等值判断，<u>那么它的代价为</u>：<br>$$ (h_i + 1) * (t_T + t_S) $$</p><blockquote><p>没什么好解释的，最多只读取一条记录，和使用 B+ 树 primary index 时在 key 上面等值判断情况一致。</p></blockquote><br><p>如果选择条件是 non-key 上面的等值判断或比较，<u>那么它的代价为</u>：<br>$$ (h_i + n) * (t_T + t_S) &#x3D; h_i * (t_T + t_S) + n * (t_T + t_S) $$</p><blockquote><p>同样的，满足条件的记录可能存在多条，因此需要 $h_i$ 次随机 I&#x2F;O 和 block 传输将叶子节点读取到内存中。该叶子节点包含满足条件的第一条记录的指针，从该指针开始顺序扫描索引（可能跨越多个叶子节点），就能获取到所有指向满足条件的记录的指针，假设有 $n$ 条。它们的指针虽然在索引中顺序存储，但这是 secondary index，指针指向的记录却可能分布在不同的 block 中，所以要获取到这些记录，最多还需要 $n$ 次随机 I&#x2F;O 和 block 传输。</p></blockquote><p>从这里可知，如果满足条件的记录非常多，那么 secondary index 的代价很可能比线性扫描还要高！！！<strong>所以 secondary index 只适合少量记录读取的情况</strong>。</p><p>如果提前就知道需要读取记录的多少，那么就能在没有其他索引可用时在 secondary index 和线性扫描中选择最好的方案（即操作的注释，<a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E6%80%BB%E8%AE%BA/" title="查询处理篇：总论">查询处理篇：总论</a>），事实上很难提前得知。<u>为了应对这种情况</u>，PostgreSQL 使用了一种混合方法，bitmap index scan. </p><blockquote><p>search key value 在叶子节点中是按序存储的，在这些叶子节点上再建立多级稀疏索引，就能够跳过大量无效的记录。这是索引的最大好处。然而，secondary index 中相邻 search key value 指向的记录可能存储在不同的 block 中，会存在大量的随机 I&#x2F;O，比较耗时。<u>bitmap index scan 便是从这点下手</u>，该算法首先会为该表创建一个 bitmap，1bit 对应一个 block，当使用 secondary index 检索记录时，不着急读取指针所指的 block，而是获取该 block 编号，再将 bitmap 中该编号对应的位置 1，待所有包含目标记录的 block 都被标记完后，顺序扫描该 bitmap，读取那些被置 1 的 block，没有置 1 的就跳过，最后在这些 block 中检索目标记录。这样记录的读取就尽量做到了顺序扫描。</p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>算法刷题杂记</title>
    <link href="/2022/07/07/%E7%AE%97%E6%B3%95%E5%88%B7%E9%A2%98%E6%9D%82%E8%AE%B0/"/>
    <url>/2022/07/07/%E7%AE%97%E6%B3%95%E5%88%B7%E9%A2%98%E6%9D%82%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="分类目录"><a href="#分类目录" class="headerlink" title="分类目录"></a>分类目录</h2><blockquote><p>引用时名字中的字母要保持原样，不能改成小写</p></blockquote><h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><blockquote><p><a href="#%E5%8A%9B%E6%89%A3-123-%E4%B9%B0%E5%8D%96%E8%82%A1%E7%A5%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E6%97%B6%E6%9C%BA-III">力扣-123-买卖股票的最佳时机-III</a><br><a href="#%E5%8A%9B%E6%89%A3-1235-%E8%A7%84%E5%88%92%E5%85%BC%E8%81%8C%E5%B7%A5%E4%BD%9C">力扣-1235-规划兼职工作</a><br><a href="#%E5%8A%9B%E6%89%A3-124-%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%AD%E7%9A%84%E6%9C%80%E5%A4%A7%E8%B7%AF%E5%BE%84%E5%92%8C">力扣-124-二叉树中的最大路径和</a><br><a href="#%E5%8A%9B%E6%89%A3-894-%E6%89%80%E6%9C%89%E5%8F%AF%E8%83%BD%E7%9A%84%E7%9C%9F%E4%BA%8C%E5%8F%89%E6%A0%91">力扣-894-所有可能的真二叉树</a><br><a href="#%E5%8A%9B%E6%89%A3-%E5%89%91%E6%8C%87-Offer-II-091-%E7%B2%89%E5%88%B7%E6%88%BF%E5%AD%90">力扣-剑指-Offer-II-091-粉刷房子</a><br><a href="#%E5%8A%9B%E6%89%A3-96-%E4%B8%8D%E5%90%8C%E7%9A%84%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91">力扣-96-不同的二叉搜索树</a><br><a href="https://leetcode.cn/problems/partition-equal-subset-sum/">0-1 背包：力扣-416-分割等和子集</a><br><a href="https://leetcode.cn/problems/ones-and-zeroes/">0-1 背包：力扣-474-一和零</a><br><a href="https://leetcode.cn/problems/target-sum/">0-1 背包：力扣-494-目标和</a><br><a href="https://leetcode.cn/problems/profitable-schemes/">0-1 背包：力扣-879-盈利计划</a><br><a href="https://leetcode.cn/problems/coin-change/">完全背包：力扣-322-零钱兑换</a><br><a href="https://leetcode.cn/problems/coin-change-2/">完全背包：力扣-518-零钱兑换 II</a><br><a href="https://leetcode.cn/problems/form-largest-integer-with-digits-that-add-up-to-target/">完全背包：力扣-1449-数位成本和为目标值的最大数字</a></p></blockquote><h3 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h3><blockquote><p><a href="#%E5%8A%9B%E6%89%A3-1235-%E8%A7%84%E5%88%92%E5%85%BC%E8%81%8C%E5%B7%A5%E4%BD%9C">力扣-1235-规划兼职工作</a><br><a href="#%E5%8A%9B%E6%89%A3-33-%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84">力扣-33-搜索旋转排序数组</a></p></blockquote><h3 id="回溯"><a href="#回溯" class="headerlink" title="回溯"></a>回溯</h3><blockquote><p><a href="#%E5%8A%9B%E6%89%A3-15-%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C">力扣-15-三数之和</a></p></blockquote><h3 id="双指针"><a href="#双指针" class="headerlink" title="双指针"></a>双指针</h3><blockquote><p><a href="#%E5%8A%9B%E6%89%A3-15-%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C">力扣-15-三数之和</a></p></blockquote><h2 id="题目杂记"><a href="#题目杂记" class="headerlink" title="题目杂记"></a>题目杂记</h2><h3 id="力扣-96-不同的二叉搜索树"><a href="#力扣-96-不同的二叉搜索树" class="headerlink" title="力扣 96-不同的二叉搜索树"></a>力扣 96-不同的二叉搜索树</h3><blockquote><p><a href="https://leetcode.cn/problems/unique-binary-search-trees/">链接</a></p></blockquote><h3 id="力扣-33-搜索旋转排序数组"><a href="#力扣-33-搜索旋转排序数组" class="headerlink" title="力扣 33-搜索旋转排序数组"></a>力扣 33-搜索旋转排序数组</h3><blockquote><p><a href="https://leetcode.cn/problems/search-in-rotated-sorted-array/">链接</a></p></blockquote><p>题目简介</p><blockquote><p><img src="/img/leetcode/9.png" alt="图 9"></p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">//先二分找出旋转点，然后再来两个二分分段查找</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">search</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums, <span class="hljs-type">int</span> target)</span> </span>&#123;<br>    <span class="hljs-type">int</span> pos = <span class="hljs-built_in">foo</span>(nums);<br>    <span class="hljs-type">int</span> ret;<br>    <span class="hljs-keyword">if</span> ((ret = <span class="hljs-built_in">goo</span>(nums, target, <span class="hljs-number">0</span>, pos + <span class="hljs-number">1</span>)) != <span class="hljs-number">-1</span>) <span class="hljs-keyword">return</span> ret;<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">goo</span>(nums, target, pos + <span class="hljs-number">1</span>, nums.<span class="hljs-built_in">size</span>());<br>&#125;<br><br><span class="hljs-comment">// 二分查找</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">goo</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums, <span class="hljs-type">int</span> target, <span class="hljs-type">int</span> l, <span class="hljs-type">int</span> h)</span> </span>&#123;<br>    <span class="hljs-keyword">while</span> (l &lt; h) &#123;<br>        <span class="hljs-type">int</span> mid = l + (h - l) / <span class="hljs-number">2</span>;<br>        <span class="hljs-keyword">if</span> (nums[mid] &gt; target) h = mid;<br>        <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (nums[mid] == target) <span class="hljs-keyword">return</span> mid;<br>        <span class="hljs-keyword">else</span> l = mid + <span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>&#125;<br><br><span class="hljs-comment">// 二分找到最大的数</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">foo</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums)</span> </span>&#123;<br>    <span class="hljs-type">int</span> i = <span class="hljs-number">0</span>, j = nums.<span class="hljs-built_in">size</span>() - <span class="hljs-number">1</span>, mid;<br>    <span class="hljs-keyword">while</span> (i &lt; j - <span class="hljs-number">1</span>) &#123;<br>        mid = i + (j - i) / <span class="hljs-number">2</span>;<br>        <span class="hljs-keyword">if</span> (nums[mid] &lt; nums[j]) j = mid;<br>        <span class="hljs-keyword">else</span> i = mid;<br>    &#125;<br>    <span class="hljs-comment">//cout &lt;&lt; nums[i] &lt;&lt; endl;</span><br>    <span class="hljs-keyword">return</span> i;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="力扣-15-三数之和"><a href="#力扣-15-三数之和" class="headerlink" title="力扣 15-三数之和"></a>力扣 15-三数之和</h3><blockquote><p><a href="https://leetcode.cn/problems/3sum/">链接</a></p></blockquote><p>题目简介</p><blockquote><p><img src="/img/leetcode/7.png" alt="图 7"></p></blockquote><br><p>题目解析</p><blockquote><p>该题可以采用回溯和双指针两种方法解决。为什么要记录，第一学习该类问题回溯去重的方法，第二学习双指针含义以及使用方式。假设数组为 [1, 1, 0, 0, -1, -1].</p></blockquote><blockquote><p>先是回溯，下图是回溯时的递归示意图：<br><img src="/img/leetcode/8.png" alt="图 8"><br>可以看到重复主要是由于存在相同的元素，而且同一组数还可能因为排列的顺序不同而导致重复，如 [1,-1,0], [1,0,-1]…<br>可以通过对数组排序，回溯时记录上一个选取的下标，选取后面的数时，只取上一个下标后面的数，这样就能避免重新排列而导致的重复。如何去掉相同元素导致的重复呢？递归时，从图中可以看到（部分采用红圈标出），这种情况的重复出现在，以某个选中的数字为根，其包含的分支中，同一层有相同的元素，如果同一层相同的元素只访问一次就能避免这种重复。另外，需要注意（黄圈标出），不同层可以存在相同的元素。具体方法如下代码：</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs C++">vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt; <span class="hljs-built_in">UsingBacktrace</span>(vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums) &#123;<br>        <span class="hljs-built_in">sort</span>(nums.<span class="hljs-built_in">begin</span>(), nums.<span class="hljs-built_in">end</span>());<br>        <span class="hljs-built_in">backtrace</span>(<span class="hljs-number">-1</span>, <span class="hljs-number">0</span>, nums);<br>        <span class="hljs-keyword">return</span> ans;<br>&#125;<br><br>vector&lt;<span class="hljs-type">int</span>&gt; tmp;<br>vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt; ans;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">backtrace</span><span class="hljs-params">(<span class="hljs-type">int</span> idx, <span class="hljs-type">int</span> sum, vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums)</span> </span>&#123;<br>    <span class="hljs-keyword">if</span> (tmp.<span class="hljs-built_in">size</span>() == <span class="hljs-number">3</span>) &#123;<br>        <span class="hljs-keyword">if</span> (sum == <span class="hljs-number">0</span>) &#123;<br>            ans.<span class="hljs-built_in">push_back</span>(tmp);<br>        &#125;<br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = idx + <span class="hljs-number">1</span>; i &lt; nums.<span class="hljs-built_in">size</span>(); i++) &#123;<br>        <span class="hljs-comment">// 这种类型的回溯剪枝</span><br>        <span class="hljs-comment">// i != idx + 1 放掉那些不同层却相同的情况，如 [1, 2, 2]</span><br>        <span class="hljs-comment">// nums[i - 1] == nums[i] 过滤掉那些同一层相同的情况</span><br>        <span class="hljs-comment">// 为什么可以，因为排序后，相同元素都在一起，在每一层，</span><br>        <span class="hljs-comment">// 对于相同的一组元素，只取第一个，后面的全部跳过</span><br>        <span class="hljs-keyword">if</span> (i != idx + <span class="hljs-number">1</span> &amp;&amp; nums[i - <span class="hljs-number">1</span>] == nums[i]) <span class="hljs-keyword">continue</span>;<br>        tmp.<span class="hljs-built_in">push_back</span>(nums[i]);<br>        <span class="hljs-built_in">backtrace</span>(i, sum + nums[i], nums);<br>        tmp.<span class="hljs-built_in">pop_back</span>();<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><blockquote><p>然而该题使用回溯会超时，需要使用双指针，关于双指针的描述，<u>官方题解写得十分好</u>，建议阅读，这里这附上代码：</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs C++">vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt; <span class="hljs-built_in">UsingTwoPointers</span>(vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums) &#123;<br>        vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt; ans;<br>        <span class="hljs-built_in">sort</span>(nums.<span class="hljs-built_in">begin</span>(), nums.<span class="hljs-built_in">end</span>());<br><br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; nums.<span class="hljs-built_in">size</span>(); i++) &#123;<br>            <span class="hljs-keyword">if</span> (i != <span class="hljs-number">0</span> &amp;&amp; nums[i<span class="hljs-number">-1</span>] == nums[i]) <span class="hljs-keyword">continue</span>;<br>            <span class="hljs-type">int</span> k = nums.<span class="hljs-built_in">size</span>() - <span class="hljs-number">1</span>;<br><br>            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = i + <span class="hljs-number">1</span>; j &lt; nums.<span class="hljs-built_in">size</span>(); j++) &#123;<br>                <span class="hljs-comment">// find a + b + c = 0</span><br>                <span class="hljs-comment">// a, b 确定，那么 c 也就确定了，并且随着 b 的增大，c 一定会减小</span><br>                <span class="hljs-keyword">if</span> (j != i + <span class="hljs-number">1</span> &amp;&amp; nums[j<span class="hljs-number">-1</span>] == nums[j]) <span class="hljs-keyword">continue</span>;<br><br>                <span class="hljs-keyword">while</span> (k &gt; j &amp;&amp; nums[i] + nums[j] + nums[k] &gt; <span class="hljs-number">0</span>) k--;<br><br>                <span class="hljs-keyword">if</span> (k &gt; j &amp;&amp; nums[i] + nums[j] + nums[k] == <span class="hljs-number">0</span>) &#123;<br>                    ans.<span class="hljs-built_in">push_back</span>(&#123;nums[i], nums[j], nums[k]&#125;);<br>                &#125;<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> ans;<br>    &#125;<br></code></pre></td></tr></table></figure><h3 id="力扣-1235-规划兼职工作"><a href="#力扣-1235-规划兼职工作" class="headerlink" title="力扣 1235-规划兼职工作"></a>力扣 1235-规划兼职工作</h3><blockquote><p><a href="https://leetcode.cn/problems/maximum-profit-in-job-scheduling/">链接</a></p></blockquote><p>题目简介</p><blockquote><p>每个任务包括三个参数：开始时间、结束时间、报酬。现在有一堆任务，你可以从中选择任务来做，使得报酬最大，但所选择的任务的工作时间不能有重叠，如下图，选择最底下三个：<br><img src="/img/leetcode/1.png" alt="图 1"></p></blockquote><br><p>题目解析</p><blockquote><p>这个题目一看就知道用动态规划，但是状态转移方程没那么简单，最开始的想法很简单：<br>对于某个任务，它可以从在它之前的、与它不冲突的任务转移过来，那么只需要记录从这些任务转移来的最大报酬，再加上该任务的报酬，那么就能得出，做完该任务时取得的最大报酬，如下图，未连线的任务之间表示冲突：<br><img src="/img/leetcode/2.png" alt="图 2"><br>如 F 任务可以分别由 C、D、E 任务转移过来。那么将这些任务以开始时间从小到大排序后，就能写出如下的代码：</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// job = &#123; startTime, endTime, profits &#125;</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">foo</span><span class="hljs-params">(vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt;&amp; jobs)</span> </span>&#123;<br>    <span class="hljs-built_in">sort</span>(jobs.<span class="hljs-built_in">begin</span>(), jobs.<span class="hljs-built_in">end</span>()); <span class="hljs-comment">// 以开始时间递增排序</span><br>    vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt; <span class="hljs-built_in">from</span>(jobs.<span class="hljs-built_in">size</span>());<br><br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; jobs.<span class="hljs-built_in">size</span>(); i++) &#123;<br>        <span class="hljs-comment">// 因为 job 以开始时间排序，所以可以二分查找，</span><br>        <span class="hljs-comment">// 找到第一个大于等于任务 i 结束时间的任务</span><br>        <span class="hljs-comment">// 查找到第一个大于等于任务 i 结束时间开始的任务后，后面的都可以添加到 i 到 from 了</span><br><br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = i + <span class="hljs-number">1</span>; j &lt; jobs.<span class="hljs-built_in">size</span>(); j++) &#123;<br>            <span class="hljs-comment">// 如果任务 j 在任务 i 结束后开始，</span><br>            <span class="hljs-comment">// 那么任务 j 可以从任务 i 转移过来</span><br>            <span class="hljs-keyword">if</span> (jobs[i][<span class="hljs-number">1</span>] &lt;= jobs[j][<span class="hljs-number">0</span>]) &#123;<br>                from[j].<span class="hljs-built_in">push_back</span>(i);<br>            &#125;<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// dp 过程</span><br>    <span class="hljs-type">int</span> dp[jobs.<span class="hljs-built_in">size</span>()] = &#123; <span class="hljs-number">0</span> &#125;;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; jobs.<span class="hljs-built_in">size</span>(); i++) &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> f: from[i]) &#123;<br>            dp[i] = <span class="hljs-built_in">max</span>(dp[i], dp[f]);<br>        &#125;<br>        dp[i] += jobs[i][<span class="hljs-number">2</span>];<br>    &#125;<br><br>    cout &lt;&lt; *<span class="hljs-built_in">max_element</span>(dp, dp + jobs.<span class="hljs-built_in">size</span>()) &lt;&lt; endl;<br>&#125;<br></code></pre></td></tr></table></figure><blockquote><p>上面代码正确但不够快，时间复杂度未 $O(n^2)$，过不了该题。<br><u>在一次分析可知，里面存在重复计算</u>，比如从 E 转移到 F 就已经计算了从 C，D 转移来的报酬，不需要再单独计算从它们转移来的报酬了，因为必然是小于从 E 转移来的。<br><u>事实上这个问题类似背包问题</u>，对于一个任务 i，你可以选择做也可以选择不做，若选择不做，那么截止到任务 i 时的报酬 dp[i] 就需要从 dp[i-1] 转移过来，如果选择做，那么 dp[i] 需要从最近一次且与任务 i 不冲突的任务，假设为 j，转移过来，即 dp[i] &#x3D; dp[j] + profit[i]，最后在两者中取最大即可。找最近一次且不冲突的任务，可以采用二分，只需要将任务以结束时间递增排序，然后在其中找第一个小于等于当前任务开始时间的任务即可，所以可以写出如下的代码：</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// job = &#123; startTime, endTime, profits &#125;</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">foo</span><span class="hljs-params">(vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt;&amp; jobs)</span> </span>&#123;<br>    <span class="hljs-comment">//以结束时间递增排序</span><br>    <span class="hljs-built_in">sort</span>(jobs.<span class="hljs-built_in">begin</span>(), jobs.<span class="hljs-built_in">end</span>(), <br>    [](<span class="hljs-type">const</span> vector&lt;<span class="hljs-type">int</span>&gt;&amp; a, <span class="hljs-type">const</span> vector&lt;<span class="hljs-type">int</span>&gt;&amp; b) -&gt;<span class="hljs-type">bool</span> &#123;<br>            <span class="hljs-keyword">return</span> a[<span class="hljs-number">1</span>] &lt; b[<span class="hljs-number">1</span>];<br>            &#125;);<br><br>        <span class="hljs-comment">// dp[i] = p 截至任务 i 时的最大报酬</span><br>        unordered_map&lt;<span class="hljs-type">int</span>, <span class="hljs-type">int</span>&gt; dp;<br>        dp[<span class="hljs-number">0</span>] = jobs[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>];<br><br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt; jobs.<span class="hljs-built_in">size</span>(); i++) &#123;<br>            vector&lt;<span class="hljs-type">int</span>&gt; tmp = &#123; jobs[i][<span class="hljs-number">1</span>] &#125;;<br>            <span class="hljs-keyword">auto</span> last = <span class="hljs-built_in">upper_bound</span>(jobs.<span class="hljs-built_in">begin</span>(), jobs.<span class="hljs-built_in">end</span>(), tmp,<br>                [](<span class="hljs-type">const</span> vector&lt;<span class="hljs-type">int</span>&gt;&amp; a, <span class="hljs-type">const</span> vector&lt;<span class="hljs-type">int</span>&gt;&amp; b) -&gt; <span class="hljs-type">bool</span> &#123;<br>                    <span class="hljs-keyword">return</span> a[<span class="hljs-number">0</span>] &lt; b[<span class="hljs-number">0</span>];<br>                &#125;);<br><br>            <span class="hljs-comment">// 找到的是第一个大于目标的值，prev(last) 表示取前一个</span><br>            <span class="hljs-comment">// 就是最后一个小于等于目标值的了</span><br>            dp[i] = <span class="hljs-built_in">max</span>(dp[<span class="hljs-built_in">prev</span>(last) - jobs.<span class="hljs-built_in">begin</span>()] + jobs[i][<span class="hljs-number">2</span>], dp[i - <span class="hljs-number">1</span>]);<br>        &#125;<br><br>        cout &lt;&lt; dp[jobs.<span class="hljs-built_in">size</span>() - <span class="hljs-number">1</span>] &lt;&lt; endl;<br>&#125;<br></code></pre></td></tr></table></figure><p>其他知识点</p><blockquote><p>std::upper_bound(begin, end, val, pred) 函数<br>该函数在序列中二分查找，返回第一个大于目标值 val 的迭代器，用法可以看上面</p></blockquote><blockquote><p>另外，还可以自己写类似上面的二分算法：</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 返回第一个大于目标值 val 的元素的下标</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">BinarySearch</span><span class="hljs-params">(vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt;&amp; jobs, <span class="hljs-type">int</span> val)</span> </span>&#123;<br>        <span class="hljs-comment">// find the smallest v so that v &gt; val</span><br>        <span class="hljs-type">int</span> l = <span class="hljs-number">0</span>, r = jobs.<span class="hljs-built_in">size</span>(), mid;<br>        <span class="hljs-keyword">while</span> (l &lt; r) &#123;<br>            mid = l + (r - l) / <span class="hljs-number">2</span>;<br>            <span class="hljs-keyword">if</span> (jobs[mid][<span class="hljs-number">0</span>] &lt;= val) l = mid + <span class="hljs-number">1</span>;<br>            <span class="hljs-keyword">else</span> r = mid;<br>        &#125;<br><br>        <span class="hljs-keyword">return</span> l;<br>    &#125;<br></code></pre></td></tr></table></figure><blockquote><p><u>个人觉得二分算法最麻烦的地方在于，应该返回什么，这里可以这样分析</u>：<br>从 while 循环退出有两种可能:</p><blockquote><p>l &#x3D;&#x3D; r. 那么到达该种局面有两种情况：<br>A. $V_{mid} &lt;&#x3D; val$ 即 mid &#x3D; r - 1，很明显应该返回 r&#x2F;l<br>B. $V_{mid} &gt; val$ 即 mid &#x3D; l，很明显，应该返回 l<br>综上，返回 l</p></blockquote></blockquote><blockquote><blockquote><p>l &#x3D;&#x3D; r + 1. 那么达到该种局面有两种情况：<br>A. $V_{mid} &lt;&#x3D; val$ 即 mid &#x3D; r，不可能<br>B. $V_{mid} &gt; val$ 即 mid &#x3D; l - 1，不可能</p></blockquote></blockquote><h3 id="力扣-894-所有可能的真二叉树"><a href="#力扣-894-所有可能的真二叉树" class="headerlink" title="力扣 894-所有可能的真二叉树"></a>力扣 894-所有可能的真二叉树</h3><blockquote><p><a href="https://leetcode.cn/problems/all-possible-full-binary-trees/submissions/">链接</a><br><a href="https://leetcode.cn/problems/unique-binary-search-trees-ii/">相似题目</a></p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * Definition for a binary tree node.</span><br><span class="hljs-comment"> * struct TreeNode &#123;</span><br><span class="hljs-comment"> *     int val;</span><br><span class="hljs-comment"> *     TreeNode *left;</span><br><span class="hljs-comment"> *     TreeNode *right;</span><br><span class="hljs-comment"> *     TreeNode() : val(0), left(nullptr), right(nullptr) &#123;&#125;</span><br><span class="hljs-comment"> *     TreeNode(int x) : val(x), left(nullptr), right(nullptr) &#123;&#125;</span><br><span class="hljs-comment"> *     TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) &#123;&#125;</span><br><span class="hljs-comment"> * &#125;;</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br><br>    <span class="hljs-comment">// 加一个哈希，就变成了自顶向下的动态规划了（记忆化搜索）</span><br>    unordered_map&lt;<span class="hljs-type">int</span>, vector&lt;TreeNode*&gt;&gt; history;<br><br>    <span class="hljs-function">vector&lt;TreeNode*&gt; <span class="hljs-title">allPossibleFBT</span><span class="hljs-params">(<span class="hljs-type">int</span> n)</span> </span>&#123;<br>        <span class="hljs-keyword">if</span> (history.<span class="hljs-built_in">count</span>(n) != <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> history[n];<br>        vector&lt;TreeNode*&gt; roots;<br>        <span class="hljs-comment">// 如果只有一个节点，就不需要拆分了，只有一种情况</span><br>        <span class="hljs-comment">// 老实说，当有三个节点时，也不用拆分，也只有一种情况</span><br>        <span class="hljs-comment">//   *</span><br>        <span class="hljs-comment">//  / \</span><br><span class="hljs-comment">        // *   *</span><br>        <span class="hljs-keyword">if</span> (n == <span class="hljs-number">1</span>) &#123;<br>            roots.<span class="hljs-built_in">push_back</span>(<span class="hljs-keyword">new</span> <span class="hljs-built_in">TreeNode</span>(<span class="hljs-number">0</span>));<br>            <span class="hljs-keyword">return</span> roots;<br>        &#125;<br><br>        <span class="hljs-comment">// 因为一个节点的子节点个数要么为 0, 要么为 2，这是个递归定义</span><br>        <span class="hljs-comment">// 所以该子树（加上该节点）的节点个数为奇数</span><br>        <span class="hljs-comment">// i 表示以当前节点为根（总结点个数为参数 n）时，左子树节点个数</span><br>        <span class="hljs-comment">// allPossibleFBT(i) 也就表示总结点个数为 i 时，满足题目条件的所有二叉树集合</span><br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt; n - <span class="hljs-number">1</span>; i += <span class="hljs-number">2</span>) &#123;<br>            <span class="hljs-keyword">auto</span> leftRoots = <span class="hljs-built_in">allPossibleFBT</span>(i);<br>            <span class="hljs-keyword">auto</span> rightRoots = <span class="hljs-built_in">allPossibleFBT</span>(n - i - <span class="hljs-number">1</span>);<br>            <br>            <span class="hljs-comment">// 当前节点的左右子树的所有可能都求出来了，</span><br>            <span class="hljs-comment">// 那么以当前节点（下面的 new）为根的所有可能的二叉树，就可以通过排列组合计算了</span><br>            <span class="hljs-comment">// 由于左右子树的结构都符合题目，组合出来自然也符合题目</span><br>            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> left: leftRoots) &#123;<br>                <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> right: rightRoots) &#123;<br>                    <span class="hljs-keyword">auto</span> node = <span class="hljs-keyword">new</span> <span class="hljs-built_in">TreeNode</span>(<span class="hljs-number">0</span>);<br>                    node-&gt;left = left;<br>                    node-&gt;right = right;<br>                    roots.<span class="hljs-built_in">push_back</span>(node);<br>                &#125;<br>            &#125;<br>        &#125;<br><br>        history[n] = roots;<br>        <span class="hljs-keyword">return</span> roots;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h3 id="力扣-剑指-Offer-II-091-粉刷房子"><a href="#力扣-剑指-Offer-II-091-粉刷房子" class="headerlink" title="力扣 剑指 Offer II 091. 粉刷房子"></a>力扣 剑指 Offer II 091. 粉刷房子</h3><blockquote><p><a href="https://leetcode.cn/problems/JEj789/">链接</a></p></blockquote><p>题目简介<br><img src="/img/leetcode/3.png"></p><p>先上题解</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">dynamicProgramming</span><span class="hljs-params">(vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt;&amp; costs)</span> </span>&#123;<br>        <span class="hljs-comment">// dp[i][0], dp[i][1], dp[i][2]</span><br>        <span class="hljs-comment">// 分别表示使用 0，1，2 三种颜色刷完房子 i 时的最小开销</span><br>        <span class="hljs-comment">// dp[i][0] = min&#123;dp[i-1][1], dp[i-1][2]&#125; + cost[i][0]</span><br>        <span class="hljs-comment">// ...</span><br>        vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt; dp;<br>        <span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">tmp</span><span class="hljs-params">(<span class="hljs-number">3</span>)</span></span>;<br>        dp.<span class="hljs-built_in">resize</span>(costs.<span class="hljs-built_in">size</span>(), tmp);<br>        dp[<span class="hljs-number">0</span>] = costs[<span class="hljs-number">0</span>];<br><br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt; costs.<span class="hljs-built_in">size</span>(); i++) &#123;<br>            dp[i][<span class="hljs-number">0</span>] = <span class="hljs-built_in">min</span>(dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">1</span>], dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">2</span>]) + costs[i][<span class="hljs-number">0</span>];<br>            dp[i][<span class="hljs-number">1</span>] = <span class="hljs-built_in">min</span>(dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">0</span>], dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">2</span>]) + costs[i][<span class="hljs-number">1</span>];<br>            dp[i][<span class="hljs-number">2</span>] = <span class="hljs-built_in">min</span>(dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">1</span>], dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">0</span>]) + costs[i][<span class="hljs-number">2</span>];<br>        &#125;<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">min</span>(dp.<span class="hljs-built_in">back</span>()[<span class="hljs-number">0</span>], <span class="hljs-built_in">min</span>(dp.<span class="hljs-built_in">back</span>()[<span class="hljs-number">1</span>], dp.<span class="hljs-built_in">back</span>()[<span class="hljs-number">2</span>]));<br>    &#125;<br></code></pre></td></tr></table></figure><p>为什么可以这样做呢？看下图分析：<br><img src="/img/leetcode/4.png"><br>可以看到房子乙无论染哪种颜色，都有两种可能的开销，总共有六种情况，如果只有两个房子，那么比较这六种就能得出最小开销。现在增加一个房子丙，对于丙的任一一种染色方案，都有四种可能的开销，总共一十二种。</p><p><u>事实上，这里存在重复的计算</u>，假如房子乙染了 A 颜色，房子丙染了 B 颜色，在计算最小开销时，由于房子丙-B 的成本已知，那么乙-A1，乙-A2 两种方案只有一种会被采用，即最小的那一种，也就是说，房子乙没必要为每种染色保留所有可能的开销，只需要保留最小的即可。对房子丙同理。真好的剪枝啊。<u>突然发现这道题和下面的题关于状态的分析好像啊</u>。</p><h3 id="力扣-123-买卖股票的最佳时机-III"><a href="#力扣-123-买卖股票的最佳时机-III" class="headerlink" title="力扣 123-买卖股票的最佳时机 III"></a>力扣 123-买卖股票的最佳时机 III</h3><blockquote><p><a href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-iii/">链接</a></p></blockquote><p><img src="/img/leetcode/5.png"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">maxProfit</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;&amp; prices)</span> </span>&#123;<br>        <span class="hljs-comment">// write code here</span><br>        <span class="hljs-comment">/*</span><br><span class="hljs-comment">          最多对股票进行两笔交易，那么在每一天你必然处于以下五种状态之一：</span><br><span class="hljs-comment">          1. 没有买卖任何股票</span><br><span class="hljs-comment">          2. 已完成第一笔交易买进（在今日或以前买进）</span><br><span class="hljs-comment">          3. 已完成第一笔交易卖出（在今日或以前卖出）</span><br><span class="hljs-comment">          4. 已完成第二笔交易买入（在今日或以前买入）</span><br><span class="hljs-comment">          5. 已完成第二笔交易卖出（在今日或以前卖出）</span><br><span class="hljs-comment">        </span><br><span class="hljs-comment">          无论处于哪一种状态，你都有一个收益，题目的答案就是处于状态5的且最大的收益</span><br><span class="hljs-comment">          设 dp[i][j] 为第 i 天处于状态 j 时的最大收益</span><br><span class="hljs-comment">          状态 1 的收益始终为零，根本不用记录，下面考虑状态转移：</span><br><span class="hljs-comment">        </span><br><span class="hljs-comment">          状态 2 的状态转移：dp[i][2] = max(dp[i-1][2], -prices[i])</span><br><span class="hljs-comment">          为什么是这样？第 i 天处于状态 2，有两种可能：</span><br><span class="hljs-comment">             1）维持现状（之前就处于该状态）；</span><br><span class="hljs-comment">             2）刚刚进入状态 2，也就是买了今天的股票，收益自然为价格的负数</span><br><span class="hljs-comment">        </span><br><span class="hljs-comment">          状态 3 的状态转移：dp[i][3] = max(dp[i-1][3], dp[i-1][2] + prices[i])</span><br><span class="hljs-comment">          为什么是这样？第 i 天处于状态 3，有两种可能：</span><br><span class="hljs-comment">             1）维持现状；</span><br><span class="hljs-comment">             2）刚刚进入状态 3，也就是在今天卖出第一笔买入的股票，</span><br><span class="hljs-comment">                收益自然需要加上卖出的钱，需要注意，是买入的收益+卖出的收益</span><br><span class="hljs-comment">        </span><br><span class="hljs-comment">          状态 4 的状态转移：dp[i][4] = max(dp[i-1][4], dp[i-1][3] - prices[i])</span><br><span class="hljs-comment">          为什么？不解释，同上</span><br><span class="hljs-comment">        </span><br><span class="hljs-comment">          状态 5 的状态转移：dp[i][5] = max(dp[i-1][5], dp[i-1][4] + prices[i])</span><br><span class="hljs-comment">          为什么？不解释，同上</span><br><span class="hljs-comment"></span><br><span class="hljs-comment">        */</span><br>        <br>        <span class="hljs-comment">// 综上所述，可以写出下面的代码：</span><br>        vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt;<span class="hljs-built_in">dp</span>(prices.<span class="hljs-built_in">size</span>(), <span class="hljs-built_in">vector</span>&lt;<span class="hljs-type">int</span>&gt;(<span class="hljs-number">5</span>));<br>        dp[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>;<br>        dp[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] = -prices[<span class="hljs-number">0</span>];<br>        dp[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>] = <span class="hljs-number">0</span>; <span class="hljs-comment">// 同一天买入又卖出可不为0吗</span><br>        dp[<span class="hljs-number">0</span>][<span class="hljs-number">3</span>] = -prices[<span class="hljs-number">0</span>];<br>        dp[<span class="hljs-number">0</span>][<span class="hljs-number">4</span>] = <span class="hljs-number">0</span>; <span class="hljs-comment">// 可不为零吗</span><br>        <br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt; prices.<span class="hljs-built_in">size</span>(); i++) &#123;<br>            dp[i][<span class="hljs-number">1</span>] = <span class="hljs-built_in">max</span>(dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">1</span>], -prices[i]);<br>            dp[i][<span class="hljs-number">2</span>] = <span class="hljs-built_in">max</span>(dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">2</span>], dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">1</span>] + prices[i]);<br>            dp[i][<span class="hljs-number">3</span>] = <span class="hljs-built_in">max</span>(dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">3</span>], dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">2</span>] - prices[i]);<br>            dp[i][<span class="hljs-number">4</span>] = <span class="hljs-built_in">max</span>(dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">4</span>], dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">3</span>] + prices[i]);<br>        &#125;<br>    <br>        <span class="hljs-keyword">return</span> dp.<span class="hljs-built_in">back</span>().<span class="hljs-built_in">back</span>();<br>        <br>        <span class="hljs-comment">/*</span><br><span class="hljs-comment">        </span><br><span class="hljs-comment">        另外，由于 dp[i] 只与 dp[i-1] 有关，所以可以压缩以下空间</span><br><span class="hljs-comment">        对上面的四个状态（状态1不记录）分配采用四个变量记录：</span><br><span class="hljs-comment">        buy1  --&gt; 状态 2</span><br><span class="hljs-comment">        sell1 --&gt; 状态 3</span><br><span class="hljs-comment">        buy2  --&gt; 状态 4</span><br><span class="hljs-comment">        sell2 --&gt; 状态 5</span><br><span class="hljs-comment"></span><br><span class="hljs-comment">        */</span><br>        <br>        <span class="hljs-comment">// 可以写出下面的代码：</span><br>        <span class="hljs-type">int</span> buy1 = -prices[<span class="hljs-number">0</span>];<br>        <span class="hljs-type">int</span> sell1 = <span class="hljs-number">0</span>;<br>        <span class="hljs-type">int</span> buy2 = -prices[<span class="hljs-number">0</span>];<br>        <span class="hljs-type">int</span> sell2 = <span class="hljs-number">0</span>;<br>        <br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt; prices.<span class="hljs-built_in">size</span>(); i++) &#123;<br>            buy1 = <span class="hljs-built_in">max</span>(buy1, -prices[i]);<br>            sell1 = <span class="hljs-built_in">max</span>(sell1, buy1 + prices[i]);<br>            buy2 = <span class="hljs-built_in">max</span>(buy2, sell1 - prices[i]);<br>            sell2 = <span class="hljs-built_in">max</span>(sell2, buy2 + prices[i]);<br>        &#125;<br>        <br>        <span class="hljs-keyword">return</span> sell2;<br>    &#125;<br></code></pre></td></tr></table></figure><h3 id="力扣-124-二叉树中的最大路径和"><a href="#力扣-124-二叉树中的最大路径和" class="headerlink" title="力扣 124-二叉树中的最大路径和"></a>力扣 124-二叉树中的最大路径和</h3><blockquote><p><a href="https://leetcode.cn/problems/binary-tree-maximum-path-sum/">链接</a></p></blockquote><p><img src="/img/leetcode/6.png"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">maxPathSum</span><span class="hljs-params">(TreeNode* root)</span> </span>&#123;<br>        <span class="hljs-keyword">auto</span> ret = <span class="hljs-built_in">recursion</span>(root);<br><br>        <span class="hljs-comment">//情况3、4、6</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">max</span>(ans, ret);<br>    &#125;<br><br>    <span class="hljs-comment">/*</span><br><span class="hljs-comment">        对于一颗以 root 为根的树，和最大的路径有以下几种可能：</span><br><span class="hljs-comment">        1. 在左子树取得</span><br><span class="hljs-comment">        2. 在右子树取得</span><br><span class="hljs-comment">        3. 在根到左子树的某个节点取得（不一定到叶节点）</span><br><span class="hljs-comment">        4. 在根到右子树的某个节点取得（不一定到叶节点）</span><br><span class="hljs-comment">        5. 在左子树某个节点 + 根 + 右子树的某个节点取得（不一定到叶节点）</span><br><span class="hljs-comment">        6. 在根节点取得</span><br><span class="hljs-comment"></span><br><span class="hljs-comment">        遍历到当前节点时，由于可能需要借助根连接路径，因此当节点返回时，需要返回</span><br><span class="hljs-comment">        3、4、6 三种情况中的最大的一种</span><br><span class="hljs-comment"></span><br><span class="hljs-comment">        其他情况可以直接采用一个变量记录取得的最大值。</span><br><span class="hljs-comment"></span><br><span class="hljs-comment">    */</span><br><br>    <span class="hljs-type">int</span> ans = <span class="hljs-number">-999</span>;<br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">recursion</span><span class="hljs-params">(TreeNode* root)</span> </span>&#123;<br>        <span class="hljs-keyword">if</span> (!root) <span class="hljs-keyword">return</span> <span class="hljs-number">-999</span>;<br>        <span class="hljs-keyword">if</span> (!root-&gt;left &amp;&amp; !root-&gt;right)&#123;<br>            <span class="hljs-keyword">if</span> (root-&gt;val &gt; ans) ans = root-&gt;val;<br>            <span class="hljs-keyword">return</span> root-&gt;val;<br>        &#125; <br><br>        <span class="hljs-keyword">auto</span> leftRet = <span class="hljs-built_in">recursion</span>(root-&gt;left);<br>        <span class="hljs-keyword">auto</span> rightRet = <span class="hljs-built_in">recursion</span>(root-&gt;right);<br><br>        <span class="hljs-comment">// 情况1</span><br>        ans = <span class="hljs-built_in">max</span>(ans, leftRet);<br>        <span class="hljs-comment">// 情况2</span><br>        ans = <span class="hljs-built_in">max</span>(ans, rightRet);<br>        <span class="hljs-comment">// 情况5</span><br>        ans = <span class="hljs-built_in">max</span>(ans, rightRet + root-&gt;val + leftRet);<br><br>        <span class="hljs-comment">//返回情况3、4、6中最大的</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">max</span>(<span class="hljs-built_in">max</span>(rightRet, leftRet) + root-&gt;val, root-&gt;val);<br><br>    &#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>索引篇零：索引总论</title>
    <link href="/2022/07/04/%E7%B4%A2%E5%BC%95%E7%AF%87%E9%9B%B6%EF%BC%9A%E7%B4%A2%E5%BC%95%E6%80%BB%E8%AE%BA/"/>
    <url>/2022/07/04/%E7%B4%A2%E5%BC%95%E7%AF%87%E9%9B%B6%EF%BC%9A%E7%B4%A2%E5%BC%95%E6%80%BB%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇文章介绍一些索引的基础内容，了解完基础内容后，可以单独阅读如下内容：</p><a href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%80%EF%BC%9A%E5%93%88%E5%B8%8C%E7%B4%A2%E5%BC%95/" title="索引篇一：哈希索引">索引篇一：哈希索引</a>   <br /><a href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%BA%8C%EF%BC%9AB-plus%20Tree/" title="索引篇二：B-plus Tree">索引篇二：B-plus Tree</a> <br /><a href="/2022/06/30/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%89%EF%BC%9ALSM%20Tree/" title="索引篇三：LSM Tree">索引篇三：LSM Tree</a>    <br /><a href="/2022/08/04/badgerDB%20%E4%B8%AD%E7%9A%84%20LSM/" title="badgerDB 中的 LSM">badgerDB 中的 LSM</a>    <br /><a href="/2022/08/04/LevelDB%20%E4%B8%AD%E7%9A%84%20LSM/" title="LevelDB 中的 LSM">LevelDB 中的 LSM</a>    <br /><a href="/2022/08/03/%E7%B4%A2%E5%BC%95%E7%AF%87%E5%9B%9B%EF%BC%9AParallel%20Index/" title="索引篇四：Parallel Index">索引篇四：Parallel Index</a> <br><br><p>下文所涉及到的表 instructor 具有如下的视图：<br><img src="/img/introduction_of_indices/1.jpg"></p><h2 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h2><p>索引技术有很多，一般衡量一种索引技术有如下的几个标准：</p><ul><li>Access Type. access type 包括高效地查找包含特定属性值的记录和包含的属性值落在某一特定范围的记录。</li><li>Access Time. 找到特定数据或一组数据所花费的时间。</li><li>Insert Time. 插入一条新记录的时间，包括查找和更新索引结构的时间。</li><li>Deletion Time. 删除一条特定记录的时间，包含查找和更新索引结构的时间。</li><li>Space Overhead. 索引结构所占用的额外空间。</li></ul><p>通常一个文件不会只有一个索引结构，如查找图书时，既可以根据分类查找、也可以根据作者查找，还能根据时间查找。一个索引结构只能建立在某个 <font color=red>search key</font> 上面，search key 由某个属性或某组属性构成。search key 是属性（组）的名字，如 name，age，而具体的属性取值，如 Tom，22，被称为 search-key value 或 search key 值。</p><p>一般某个文件在存储记录时，可能依照某种顺序存储，比如依照表的自增主键从小到大，如果某个索引的 search key 恰好也定义了这种顺序，那么称该 index 为 <font color=red>clustering index 或者 primary index</font>. 注意，primary key 和 primary index 没有必然的联系，因为 primary index 可以建立在任何的 search key 上面。如果某个 index 的 search key 所指定的顺序和文件记录的存储顺序不同，那么这样的 index 被称为 <font color=red>nonclustering index 或者 secondary index</font>. 两种 index 如下：<br><img src="/img/introduction_of_indices/2.jpg" alt="图 1"><br><img src="/img/introduction_of_indices/3.jpg" alt="图 2"></p><p>假设 instructor 表的文件以 ID 为顺序存储记录，那么图 1 的索引结构是 primary index，因为它的 search key 为 ID，注意，若以 primary key（数据库自动增加的）为 search key，该索引结构同样是 primary index. 而图 2 索引结构的 search key 是 dept_name（第三列），该 search key 定义的顺序和文件本身存储的顺序不一致，所以是 secondary index.</p><p>如果索引结构是 primary index，search key 定义的顺序和记录在文件中存储的顺序一致，<font color=red>那么索引结构既可以采用稀疏索引（sparse index），也可以采用稠密索引（dense index）</font>：</p><ul><li>在稠密索引中，对一个文件中的每一个 search key 值都有一条索引条目（index entry）与之对应。每条索引条目包含 search key 值和一个指针，指针指向具有相同 search key 值记录的第一条记录，如图 4（注意表是以 dept_name 顺序存储的）. 而如果是 secondary index 的稠密索引，那么必须记录所有具有相同的 search key 值的记录的指针。 </li><li>在稀疏索引中，文件中若干个相邻的记录共享一条索引条目，条目中的指针指向第一条记录。查找过程就是先找最大的小于等于目标的索引条目，然后遍历这些相邻的记录。如图 5.</li></ul><p>图 2 的 secondary index 其实有两层索引，第一层是建立在 search key 上面的稠密索引，第二层是建立在第一层索引上的稠密索引。</p><p><img src="/img/introduction_of_indices/4.jpg" alt="图 4 - 稠密索引"><br><img src="/img/introduction_of_indices/5.jpg" alt="图 5 - 稀疏索引"></p><p>想象这样一种场景，假设要为有 1,000,000 个元组的表构建一个稠密索引。一般索引条目要小于文件记录（元组），假设一个 4KB 的 block，能够容纳 100 个索引条目，那么总共需要 10,000 个 block. 如果元组的数量为 100,000,000，那么索引条目就要占 1,000,000 个 block，即 4GB. 一般索引条目也很大，需要存储在文件中。当查找记录时，需要先查找索引，而索引文件很大，全部读入内存需要非常多的 I&#x2F;O. 当然可以采用二分查找，对于上面所说的 10,000 个 block 需要 14 次磁盘 I&#x2F;O（block 为单次传输单位）。</p><p><font color=red>为减少磁盘读写，可以采用多级索引结构（multilevel indices）</font>。在多级索引结构中，像对待普通记录那样对待索引条目，再为它们建立一层索引，如下图：<br><img src="/img/introduction_of_indices/6.jpg" alt="图 6 - 两层稀疏索引"></p><p>图中，第一层索引（inner index）既可以稀疏也可以稠密，第二级索引（outer index）是在 inner index 条目（以索引条目为 search key）上面建立的稀疏索引，因为 inner index 必然是按索引条目顺序存储的，所以可以在上面很方便的建立 primary index。当查找一条记录时，首先在 outer index 中找到最大的且小于等于目标 search key value 的索引条目，然后读出（可能）包含目标索引（inner index 条目）的 block，再在该 block 中查找（取决于采用哪种索引方式）。这样加上读取文件记录的 block，总共只需要三次磁盘读写，远远小于二分查找。</p>]]></content>
    
    
    <categories>
      
      <category>索引</category>
      
    </categories>
    
    
    <tags>
      
      <tag>索引</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>查询处理篇：总论</title>
    <link href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E6%80%BB%E8%AE%BA/"/>
    <url>/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E6%80%BB%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>查询处理（Query Processing）指的是那些涉及从数据库中读取数据的操作（不包括写，当然了，如果有必要，本部分也会涉及）。</p><p>本部分讨论的单机数据库中（后面有时间会拓展到并行&#x2F;分布式系统），这些单个的查询操作执行的过程以及如何衡量它们的代价等等。</p></br><p><font color=red>单机部分</font></p><a href="/2022/07/15/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9ASelection%20%E6%93%8D%E4%BD%9C/" title="查询处理篇：Selection 操作">查询处理篇：Selection 操作</a> </br><a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9AJoin%20%E6%93%8D%E4%BD%9C/" title="查询处理篇：Join 操作">查询处理篇：Join 操作</a> </br><a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9ASorting/" title="查询处理篇：Sorting">查询处理篇：Sorting</a> </br><a href="/2022/08/24/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%9E%8B/" title="查询处理篇：查询执行模型">查询处理篇：查询执行模型</a> <br></br><p><font color=red>并行&#x2F;分布式部分</font></p><a href="/2022/07/26/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9AParallel%20Join/" title="查询处理篇：Parallel Join">查询处理篇：Parallel Join</a> </br><a href="/2022/07/26/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9AParallel%20Sort/" title="查询处理篇：Parallel Sort">查询处理篇：Parallel Sort</a> </br><h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><p><img src="/img/query_processing_0/0.jpg"><br>如上图，执行一条 query 的步骤通常包括以下三步：</p><ul><li>parsing and translation：将 sql 语句翻译成关系代数（relational algebra）。</li><li>optimization：衡量代价，选择合适的执行方案（query-execution plan）。</li><li>evaluation：执行具体的方案，然后输出结果。</li></ul><p>optimization 和 evaluation 内容很多，可以分为三部分：</p><ol><li>将关系代数做等价变换，生成多个不同的执行方案</li><li>衡量每个执行方案的代价，选择最优的一个作为最后方案</li><li>执行方案。</li></ol><p>第 1、2 部分将在<a href="/2022/07/26/%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%EF%BC%88Query%20Optimization%EF%BC%89/" title="查询优化（Query Optimization）">查询优化（Query Optimization）</a>介绍，<u>本部分（查询处理篇）主要涉及 3 执行方案中的单个操作的执行和代价衡量</u>，参考下面例子：<br><img src="/img/query_processing_0/1.jpg"></p><p>假设有一个 query 如上图，经过 parsing and translation 可以产生如下的代数表达式：</p><p><img src="/img/query_processing_0/2.jpg"></p><p>其中 $\Pi,\ \sigma$ 分别表示投影（projection）和选择（selection）操作。有了表达式还不够，代数表达式包含多个操作，所以还需要告诉数据库系统应该如何执行这一系列操作，比如应该如何执行 $\sigma_{salary&lt; 7500}$ 这个操作，是顺序读取表文件还是采用索引，这个步骤称为注释（annotation）。一个代数表达式操作 + 注释被称为执行原语（evaluation primitive），如 $\sigma_{salary&lt; 7500}\ (use\ index\ 1)$，执行一条 query 所包含的一组执行原语就叫做<strong>执行方案或执行计划（query-execution plan）</strong>.</p><p><font color=red>执行一个方案需要执行方案中的每一个操作，本部分的内容就是研究每个操作是如何执行的，以及衡量它们的代价</font>.</p><h2 id="代价模型（Cost-Model）"><a href="#代价模型（Cost-Model）" class="headerlink" title="代价模型（Cost Model）"></a>代价模型（Cost Model）</h2><p>这里介绍在后续篇章中使用的衡量操作代价的模型，在 PostgreSQL（2018）中代价包括：</p><ol><li>每个元组的的 CPU 开销</li><li>每个 Index entry 的 CPU 开销</li><li>每个其他操作（如数学运算、比较、函数）等的 CPU 开销</li><li>I&#x2F;O 开销</li></ol><p>一般数据库对上述参数都有默认值，为了简单起见，本部分以 I&#x2F;O 开销为主要因素衡量代价，主要有：需要传输的 block 数量和随机 I&#x2F;O 访问次数。用 $t_T,\ t_S$ 分别作为传输一个 block 的时间和寻道等（包含旋转延迟）时间，若一个操作需要传输 $b$ 个 block，执行 $S$ 次随机访问，那么总的开销为 $b * t_T + S * t_S$ 秒。</p><p>另外，buffer pool 的数量也很重要，在该模型中使用 $M$ 衡量。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>查询处理篇：Join 操作</title>
    <link href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9AJoin%20%E6%93%8D%E4%BD%9C/"/>
    <url>/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9AJoin%20%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<p>关于 join 操作，可以看这篇<a href="/2022/08/22/Join%20%E7%9A%84%E7%A7%8D%E7%B1%BB/" title="Join 的种类">Join 的种类</a>。</p><p>为了方便计算，考虑有 student 和 takes 两张表，需要执行它们的 join 操作，即 $student \bowtie takes$. 同时这两张表具有如下的特点：</p><blockquote><p>student 表的记录数量 $n_{student} &#x3D; 5000$<br>student 表文件的 block 数量 $b_{student} &#x3D; 100$<br>takes 表的记录数量 $n_{takes} &#x3D; 10,000$<br>takes 表文件的 block 数量 $b_{takes} &#x3D; 400$</p></blockquote><h3 id="Nested-Loop-Join-算法"><a href="#Nested-Loop-Join-算法" class="headerlink" title="Nested-Loop Join 算法"></a>Nested-Loop Join 算法</h3><p>该算法的伪代码为：<br><img src="/img/join_op/1.jpg"></p><p>该算法非常简单易懂，<u>在最坏的情况下</u>，每个表只分配一个 buffer block，那么总共需要传输 $n_r * b_s + b_r$ 个 block，对于外层表 r 的每一条记录都需要完整地扫描一遍内层表 s，如果每个表文件的所有 block 都按顺序存储，那么总共需要 $n_r + b_r$ 次随机 I&#x2F;O. <u>在最好的情况下</u>，两张表都能放在内存，那么总共需要传输 $b_s + b_r$ 个 block，2 次随机 I&#x2F;O.</p><p>很明显，如果某个表能够完全放入内存中，应该将它放在算法的内层，其代价和两个表都能放进内存时一样。如果都不能，将较小的表放在外层较好。</p><p>现在应用上述算法到上面的例子，<u>假设 student 在外层，takes 在内层</u>：</p><ul><li>最坏情况下，需要传输 $5000*4000+100&#x3D;2000100$ 个 block，$5000+100&#x3D;5100$ 次随机 I&#x2F;O. </li><li>最好情况下，需要传输 $100+400&#x3D;500$ 个 block，2 次随机 I&#x2F;O.</li></ul><p><u>假设 takes 在外层，student 在内层</u>：</p><ul><li>最坏情况下，需要传输 $10000*100+400&#x3D;1000400$ 个 block，$10000+400&#x3D;10400$ 次随机 I&#x2F;O.</li><li>最好情况下，需要传输 $100+400&#x3D;500$ 个 block，2 次随机 I&#x2F;O.</li></ul><h3 id="Block-Nested-Loop-Join-算法"><a href="#Block-Nested-Loop-Join-算法" class="headerlink" title="Block Nested-Loop Join 算法"></a>Block Nested-Loop Join 算法</h3><p>该算法的伪代码如下：<br><img src="/img/join_op/2.jpg"><br>基础的 nested-loop join 算法中，对于外层表的每一条记录就会完整遍历一次内层表，而在 block nested-loop join 算法中，对这一点做了改进：对于外层的每一个 block，访问一次内层表，如上图。</p><p><u>在最坏的情况下</u>，每个表只能各有一个 block 驻留内存，对于外层的每一个 block，内层表的每个 block 只需要访问一次，再加上外层表的 block 数量，所以总共需要传输 $b_r * b_s + b_r$ 个 block. 在每一个外层的 block 需要随机 I&#x2F;O 一次内层表（block 连续存储），再加上外层表的每一个 block 需要随机 I&#x2F;O 一次，所以总共需要 $2 * b_r$ 次随机 I&#x2F;O. <u>在最好的情况下</u>，两个表都能放在内存，总共需要传输 $b_r + b_s$ 个 block，2 次随机 I&#x2F;O.</p><p>很明显，如果某个表能够完全放入内存中，应该将它放在算法的内层，其代价和两个表都能放进内存时一样。如果都不能，将较小的表放入外层较好。</p><p>现在应用上述算法到上面的例子，<u>假设 student 在外层（较小），takes 在内层</u>：</p><ul><li>最坏情况下，需要传输 $100 * 400+100&#x3D;40100$ 个 block，$2*100&#x3D;200$ 次随机 I&#x2F;O.</li><li>最好情况下，需要传输 $100+400&#x3D;500$ 个 block，2 次随机 I&#x2F;O.</li></ul><p>和 nested-loop join 相比，最坏情况下的代价大幅减少，最好情况两种一致。nested-loop join 和 block nested-loop join 两个算法通过<u>以下方式还能提升其性能</u>：</p><ul><li>如果 join 条件是在内层表主键&#x2F;候选键上的等值连接，那么内层循环就能在第一条匹配记录找到后结束。</li><li>block nested-loop join 算法中 nested-loop 的单位是 block，如果增大这个单位，代价就能进一步减少。假设有 $M$ 个 buffer block，给外层表分配 $M-2$ 个，内层表一个，那么内层表的扫描次数就会从 $b_r$ 降到 $\lceil b_r &#x2F;(M-2) \rceil$. </li><li>buffer block 淘汰是有策略的，如果使用的是 LRU，那么可以采用电梯扫描的方法，尽快复用那些还没换出内存的 block.</li><li>如果内层表在 join 属性上有索引的话，就可以不用文件扫描的方法了。</li></ul><h3 id="Indexed-Nested-Loop-Join"><a href="#Indexed-Nested-Loop-Join" class="headerlink" title="Indexed Nested-Loop Join"></a>Indexed Nested-Loop Join</h3><p>这一部分附上原文：</p><blockquote><p>In a nested-loop join, if an index is available on the inner loop’s join attribute, index lookups can replace file scans. For each tuple $t_r$ in the outer relation r,<br>the index is used to look up tuples in s that will satisfy the join condition with tuple $t_r$. This join method is called an indexed nested-loop join; it can be used with existing indices, as well as with temporary indices created for the sole purpose of evaluating the join.</p></blockquote><blockquote><p>Looking up tuples in s that will satisfy the join conditions with a given tuple $t_r$ is<br>essentially a selection on s. For example, consider $student\bowtie takes$. Suppose that we have a student tuple with ID “00128”. Then, the relevant tuples in takes are those that satisfy the selection “ID &#x3D; 00128”.</p></blockquote><blockquote><p>The cost of an indexed nested-loop join can be computed as follows: For each tuple in the outer relation r, a lookup is performed on the index for s, and the relevant tuples are retrieved. In the worst case, there is space in the buffer for only one block of r and one block of the index. Then, $b_r$ I&#x2F;O operations are needed to read relation r, where $b_r$ denotes the number of blocks containing records of r; each I&#x2F;O requires a seek and a block transfer, since the disk head may have moved in between each I&#x2F;O. For each tuple in r, we perform an index lookup on s. Then, the cost of the join can be computed as $b_r*(t_T + t_S ) + n_r *c$, where $n_r$ is the number of records in relation r, and c is the cost of a single selection on s using the join condition. We have seen in <a href="/2022/07/15/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9ASelection%20%E6%93%8D%E4%BD%9C/" title="查询处理篇：Selection 操作">查询处理篇：Selection 操作</a> how to estimate the cost of a single selection algorithm (possibly using indices); that estimate gives us the value of c.</p></blockquote><h3 id="Merge-Join-算法"><a href="#Merge-Join-算法" class="headerlink" title="Merge Join 算法"></a>Merge Join 算法</h3><p><u>算法可用于等值和自然连接</u>，其伪代码如下：<br><img src="/img/join_op/3.jpg"></p><p>该算法基于这样一个思想：如果参与 join 操作的两个表都已经各自在 join 操作所涉及的属性上有序了，那么完成等值&#x2F;自然连接就只需要各扫描两张表一遍。</p><p>在算法的执行过程中，对于每一个 join 属性值，算法会第一个表维护一个集合 $S_s$，保存了所有具有该 join 属性值的记录，然后对于第二个表的每一条具有该 join 属性值的记录与 $S_s$ 中的所有记录 join 操作。</p><p>假设表已经<u>有序了且每个 $S_s$ 都能放进内存</u>，那么算法共需要传输 $b_r + b_s$ 个 block，如果给每个表分配 $b_b$ 个 buffer block，那么共需要 $\lceil b_r&#x2F;b_b \rceil + \lceil b_s&#x2F;b_b \rceil$ 次随机磁盘 I&#x2F;O. 如果将该算法应用在上面例子上，同时令 $b_b&#x3D;1$，那么共需要传输 $400+100&#x3D;500$ 个 block，$400+100&#x3D;500$ 次随机 I&#x2F;O.</p><p>如果<u>未排序</u>，那么就需要先使用<a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9ASorting/" title="外部排序算法">外部排序算法</a>排序。如果 $S_s$ <u>不能放进内存</u>，那么可以再套一个 block nested-loop join，当然需要加上它们的代价。</p><h3 id="Hash-Join-算法"><a href="#Hash-Join-算法" class="headerlink" title="Hash Join 算法"></a>Hash Join 算法</h3><p><u>为了减少无效的比较次数</u>，除了先排序再 join 和使用索引外，hash 也是一种选择，它的基本思想是这样的：使用同样的哈希函数分别将两个表的元组映射（在 join 属性值上映射）为多个分区（partition），那么具有相同 join 属性值的元组一定会被映射到同一个分区，那么 join 操作只需要在两个表的具有相同下标的分区之间进行就可以了，开始细节描述前，先定义如下的符号：</p><blockquote><p>使用 $h$ 表示哈希函数，将 join 属性值映射为整数 $\{0,1,…,n_h\}$.<br>$\{r_0,r_1,…,r_{n_h}\}$ 是表 r 映射的分区，元组 $t_r$ 将放进 $r_i$，$i&#x3D;h(t_r[JoinAttrVal])$<br>$\{s_0,s_1,…,s_{n_h}\}$ 是表 s 映射的分区，元组 $t_s$ 将放进 $s_i$，$i&#x3D;h(t_s[JoinAttrVal])$</p></blockquote><p>下面是算法伪代码和示意图：<br><img src="/img/join_op/4.jpg"><br><img src="/img/join_op/5.jpg"></p><p>比较简单，<u>该算法可以用于等值连接和自然连接</u>。从代码中可以看到，$n_h$ 的选值很重要，如果选的好，那么 build relation（表 s）的每一个分区 $s_i$ 和在其上的 hash index 就都能放进内存，而 probe relation（表 r）的分区可以不做此要求。</p><p>假设当前系统有 $M$ 个空闲 buffer block，build relation 有 $b_s$ 个 block，如果哈希函数能够满足均匀和随机特性，join 属性值会被平均映射到各个分区，为了能让 build relation 的每个分区都能放进内存，那么 $n_h &gt;&#x3D; \lceil b_s&#x2F;M \rceil$.</p><p>很可能算出来的 $n_h$ 过大，build relation 不能一次性完成 partition（因为 buffer block 不够用），那么就需要多次 partition，下面是示意图：<br><img src="/img/join_op/6.jpg"><br>图中展示了这样一种情况，为了使 build relation 的某个分区能放进内存，$n_h$ 的最小取值为 9，但空闲 buffer block 只有 3 个，一轮不能将完成全部的映射，所以采用 recursive partitioning.</p><p>其实可以计算，要满足什么样的情况就可以不用 recursive. 当 $M &gt; n_h + 1$ 时，就可以不用，等价地，$M &gt;(b_s&#x2F;M) + 1$，继续可以近似为 $M &gt; \sqrt{b_s}$. 假设空闲 buffer pool 总容量为 12MB，可以分成 3072 个 4KB 的 block，那么这样的 buffer pool 最多能够支撑 3072 * 3072 * 4KB &#x3D; 36GB 的 build relation 映射时不需要 recursive.</p><p><u>那么 hash join 的开销又是多少呢</u>? 假设不需要 recursive partitioning，每个分区都能装进内存。首先，r 和 s 都需要分别读入内存 partition，再然后将分区写回磁盘，这过程需要传输 $2 * (b_r + b_s)$ 个 block. 在 build 和 probe 阶段，所有分区又都需要读入内存一次，这个过程需要传输 $b_r + b_s$ 个 block. 考虑到 partition 后，映射到一个分区的记录不一定能填满一个 block，也就是读入写出分区时，也必须传输没有填充的 block 部分，两个表加起来大约有 $4 * n_h$ 额外的 block（每个分区半满），所以一共需要传输的 block 数量为：<br>$$3 * (b_r+b_s) + 4 * n_h$$</p><p>假设共有 $b_b$ 个 block 分配给 partitioning 阶段，两个表读出和写入共需要 $2(\lceil b_r&#x2F;b_b\rceil + \lceil b_s&#x2F;b_b\rceil)$ 次随机 I&#x2F;O. 在 build 和 probe 阶段，读出（不需要再写入）一个分区需要一次随机 I&#x2F;O，那么对于两个表的所有分区，共需要 $2 * n_h$ 次随机 I&#x2F;O，所有总共的随机 I&#x2F;O 次数为：<br>$$<br>2(\lceil b_r&#x2F;b_b\rceil + \lceil b_s&#x2F;b_b\rceil) + 2 * n_h<br>$$</p><p><a href="https://pingcap.com/zh/blog/tidb-source-code-reading-9">看看 TiDB 的 hash join 是怎么做的</a><br><a href="https://mp.weixin.qq.com/s/9EWY2w-il1c-jPvE9VQRwg">看看云和恩墨的 hash join 是怎么做的</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>查询处理篇：Sorting</title>
    <link href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9ASorting/"/>
    <url>/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9ASorting/</url>
    
    <content type="html"><![CDATA[<p>在数据库系统中会经常读取<strong>大量记录</strong>，如果这些记录有序的话，在一些场景中非常有用，比如：</p><ul><li>query sql 语句指定输出有序。</li><li>一些 Join 算法的高效实现，见 <a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9AJoin%20%E6%93%8D%E4%BD%9C/" title="查询处理篇：Join 操作">查询处理篇：Join 操作</a>。</li><li>向 B+ 树中插入大量的索引条目或高效地构造 B+ 树，见 <a href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%BA%8C%EF%BC%9AB-plus%20Tree/" title="索引篇二：B-plus Tree">索引篇二：B-plus Tree</a>.</li></ul><p><u>再区分下一些相关的概念</u>。如果文件上面已经建立了索引，那么使用该索引也能实现顺序读取记录，但这样的顺序可能只是记录的逻辑顺序，而非在磁盘上存储的物理顺序（如 secondary index），所以可能每条记录都会涉及 block 的传输。</p><p>另外本文讨论的是排序算法的<u>输入数据很大，内存不能一次装下</u>，会涉及多次的 block 传输，这种问题被称为 external sorting，比较常用的算法是 external sort-merge（外部归并排序），这也是本文的主角。</p><h2 id="外部归并排序"><a href="#外部归并排序" class="headerlink" title="外部归并排序"></a>外部归并排序</h2><p>假设 buffer pool 中空闲 buffer（用来缓冲 block）的个数为 $M$，需要排序的表为 $A$，该算法可以分为两个阶段：</p><p><font color=red>初始化阶段</font><br><img src="/img/query_processing_sort/0.png"></p><blockquote><p>该阶段的主要工作是创建多个 <em>run</em> 文件，每个 <em>run</em> 文件存储的都是排好序的记录。</p></blockquote><blockquote><p>先读取 $M$ 个表 $A$ 的 block（有可能不足 $M$ 个）到内存中，然后采用如快排等方法将这 $M$ 个 block 的记录排好序，最后将这 $M$ 个 block 的记录全部写入一个 <em>run</em> 文件中，记为 $R_i$. 重复该过程直到扫描完整个表 $A$，创建了若干个 <em>run</em> 文件。为什么这里不需要留一个 buffer 作为写磁盘缓冲呢？因为快排不需要额外的空间，这 $M$ 个 buffer 本身就可以作为缓冲。</p></blockquote><br><p><font color=red>归并阶段</font><br><img src="/img/query_processing_sort/1.png"></p><blockquote><p>这一阶段的主要工作就是合并上一阶段创建的 <em>run</em> 文件。</p></blockquote><blockquote><p><u>先来考虑最简单的情况（如上图）</u>，假设上一阶段产生了 $N$ 个 <em>run</em> 文件，且 $N &lt; M$，这样就能给每个 <em>run</em> 文件分配至少一个 block，还能剩下至少一个 buffer 缓冲输出。在内存中采用归并排序合并这 $N$ 个 <em>run</em> 文件，并将已合并部分的记录写入输出缓冲 buffer，假设输出文件为 $F$，以减少磁盘 I&#x2F;O. 由上一阶段可知，这里的每个 <em>run</em> 文件最多有 $M$ 个 block，所以，当某个 <em>run</em> 文件在内存中的 block 为空时，还需要去磁盘中读取剩余的 block，直到该 <em>run</em> 文件被读取完。直到所有 <em>run</em> 文件都被扫描完，文件 $F$ 即为排好序的表 $A$.</p></blockquote><blockquote><p><u>然而，世界并不总是如此美好</u>。如果表很大，上一阶段产生的 <em>run</em> 文件数量很大，即 $N &gt; M$，那么就需要重复多次上面的过程。对于当前的 $N$ 个 <em>run</em> 文件，每次将其中的 $M-1$ 个（可能不足）合并成一个新的 <em>run</em> 文件，待所有 $N$ 个 <em>run</em> 文件合并完成后（最多需要 $\lceil \frac{N}{M-1} \rceil$ 次），就产生了一组新的 <em>run</em> 文件，记为 $N_1$，继续重复该过程，直到只产生一个 <em>run</em> 文件。将 $N_i -&gt; N_j$ 这个过程称为一次 <em>pass</em>. <u>下图描述了这个过程</u>，假设一个 block 中只有一条记录，内存中有 3 个空闲 buffer，总共需要经历两次 <em>pass</em>.</p></blockquote><p><img src="/img/query_processing_sort/2.png" alt="pass-example"></p><h2 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><p>本节主要讨论外部归并算法的磁盘访问开销，下面汇总下用到的变量：</p><ul><li>$M$：buffer pool 中空闲的 buffer 数量。</li><li>$b_r$：表 $A$ 文件包含的总 block 数量。</li><li>$b_b$：在上面归并阶段中，会给每一个 <em>run</em> 文件分配一个 buffer（block），合并完后再读取该 <em>run</em> 的下一个 block，这样的话会涉及多次随机 I&#x2F;O（因为有多个 <em>run</em> 文件，磁头会经常来回移动），所以这里并不做这个限制，而是将分配给每个 <em>run</em> 文件的 buffer 数量设置为 $b_b$ 参数，这样一次就有 $\lfloor M&#x2F;b_b\rfloor-1$ 个 <em>run</em> 文件同时参与合并。</li></ul><p>由上可知，初始的 <em>run</em> 文件有 $\lceil b_r &#x2F; M \rceil$ 个，每次 <em>pass</em> 后，<em>run</em> 文件的数量都以 $\lfloor M&#x2F;b_b\rfloor-1$ 因子减少，所以，<u>总共的 <em>pass</em> 次数为</u>：<br>$$<br>\lceil log_{\lfloor M\ &#x2F;\ b_b\rfloor\ -\ 1}\ (b_r\ &#x2F;\ M) \rceil<br>$$<br>在每一次 <em>pass</em> 中，表 $A$ 的每一个 block 都会被从磁盘读取一次和写入一次磁盘，最后一次 <em>pass</em> 可以不用写入磁盘（某些场景中）再加上第一阶段的传输次数（读取全部 block 一次，再写一次），<u>那么对该表排序，总共的 block 传输（读+写）次数为</u>（减去了最后一次的写）：<br>$$<br>b_r(2\lceil log_{\lfloor M\ &#x2F;\ b_b\rfloor\ -\ 1}\ (b_r\ &#x2F;\ M) \rceil + 1)<br>$$</p><p>还需要计算随机 I&#x2F;O 的次数，在每一次 <em>pass</em> 中，磁头会来回移动从不同的 <em>run</em> 文件中读取 block，所以随机读大概需要的 I&#x2F;O 次数为 $\lceil b_r &#x2F; b_b \rceil$，假设写入的磁盘和输入的磁盘是同一个，那么随机写的 I&#x2F;O 次数和随机读一样，所以每一次 <em>pass</em> 总的随机读写 I&#x2F;O 次数为 $2\lceil b_r &#x2F; b_b \rceil$. 再加上第一阶段的随机读写 I&#x2F;O 次数，<u>那么对该表排序，总共的随机 I&#x2F;O 次数为（减去最后一次的写随机 I&#x2F;O 次数）</u>：<br>$$<br>2\lceil b_r\ &#x2F;\ M \rceil + \lceil b_r\ &#x2F;\ b_b \rceil\ (2\lceil log_{\lfloor M\ &#x2F;\ b_b\rfloor\ -\ 1}\ (b_r\ &#x2F;\ M) \rceil\ -\ 1)<br>$$</p><p>如果以上面 pass-example 图中的数据为例，假设 $b_b$ &#x3D; 1，那么需要的总的 block 传输数量和随机 I&#x2F;O 次数分别为：$12 * (4 + 1) &#x3D; 60$，$8 + 12 * (2 * 2 - 1) &#x3D; 44$.</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数据库缓冲池（buffer pool）</title>
    <link href="/2022/07/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%86%B2%E6%B1%A0%EF%BC%88buffer-pool%EF%BC%89/"/>
    <url>/2022/07/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%86%B2%E6%B1%A0%EF%BC%88buffer-pool%EF%BC%89/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数据库系统分类</title>
    <link href="/2022/07/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%88%86%E7%B1%BB/"/>
    <url>/2022/07/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>索引篇三：LSM Tree</title>
    <link href="/2022/06/30/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%89%EF%BC%9ALSM%20Tree/"/>
    <url>/2022/06/30/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%89%EF%BC%9ALSM%20Tree/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>考虑这样一种场景，有大量的数据需要通过 B+ 树索引写入（update&#x2F;insert&#x2F;delete）数据库，相邻的待写入数据很可能处在不同的叶节点上，而该索引结构又很大，buffer pool 容纳不下所有的叶子节点，那么完成这样的写入，文件 I&#x2F;O 次数很可能与数据量成正比。<u>若是使用 magnetic disk（磁盘）</u>，当前最快的磁盘能做到 0.05ms 每 block（4KB）的传输速度，seek time（寻道时间）大概是 4ms 每次，也就是单单考虑寻道时间，每秒也只能做 250 次磁盘 I&#x2F;O. <u>若是使用 flash memory（如 SSD）</u>，因为 flash 支持随机读写，可以不需要寻道，大概 0.01ms 每页（4KB），但是 flash 不支持原地更新（in-place update），更新单位为 erase block（256KB-1MB），通常需要 3ms，就算是只更新一条记录也必须如此，更何况 flash 还有寿命限制，最多能擦除 1,000,000 次。</p><p>所以，<u>在每秒存在大量随机 insert&#x2F;update&#x2F;delete 场景中</u>，B+ 树并不是十分理想的结构。为此许多 write-optimized 索引结构被提出，LSM（log-structed merge tree）便是其中一种。LSM 有许多变种，本文先介绍基本的 LSM，称为 B-LSM，然后再介绍一种变体，stepped-merge index LSM，以下简称 SMI-LSM.</p><h2 id="Basic-LSM"><a href="#Basic-LSM" class="headerlink" title="Basic LSM"></a>Basic LSM</h2><p><img src="/img/LSM/0.jpg" alt="图 0"><br>如上图所示，B-LSM 包含若干个 B+ 树，它们分为两部分：一个在内存中的 B+ 树和若干个在磁盘上的 B+ 树，它们的叶节点都存储实际的记录。在内存中的 B+ 树称为 $L_0$，在磁盘上的 B+ 树称为 $L_1,\ L_2,\ …,\ L_k$，图中 $k&#x3D;3$.</p><p>当插入一条数据时，数据首先插入到内存中的 $L_0$，当 $L_0$ 的数据达到预设的阈值后，就需要将其写入磁盘。如果 $L_1$ 为空时，整个 $L_0$ 写入磁盘创建一个新的 $L_1$；当 $L_1$ 不为空时，需要将 $L_1$ 按顺序读出与 $L_0$ 的数据归并，最后在归并的数据上以自底向上的方式创建一个新的 $L_1$，此时旧的 $L_1$ 就可以删除了，$L_0$ 也可以清空以插入新的数据。</p><p>如果只有两层，那么每次将 $L_0$ 的数据写到磁盘上时都需要与整个 $L_1$ 合并再写入磁盘创建新的 $L_1$，随着数据越来越多，显然会写入过多的额外数据，为此有两种优化：</p><ul><li>如图 0 所示，分多层，$L_{i+1}$ 层大小是 $L_i$ 层大小的 $k$ 倍，当 $k$ 个 $L_0$ 合并到 $L_1$ 后，$L_1$ 就满了，此时就可以将 $L_1$ 与 $L_2$ 合并，这样一来，每一条数据在每一层最多被写入 $k$ 次（考虑每一层 B+ 树的第一个叶子节点写入的次数），每条记录在每一层平均被写入 $\frac{k}{2} &#x3D; \frac{k + (k-1) + .. 1}{k}$ 次，即总的写放大平均为 $k * k&#x2F;2$.</li><li>如 Stepped-Merge Index 小节所示。该方式进一步将写放大减少为，每一条数据在每一层仅写入一次，总的写放大为层数。</li></ul><p>两种方式各有特点，第一种的写放大要比第二种大很多，第二种方式的读放大又比第一种要高，不过在下面一节中也有对读放大的优化。</p><h2 id="Stepped-Merge-Index-LSM"><a href="#Stepped-Merge-Index-LSM" class="headerlink" title="Stepped-Merge Index LSM"></a>Stepped-Merge Index LSM</h2><p><img src="/img/LSM/1.jpg" alt="图 1"><br>如上图，SMI-LSM 的结构大体和 B-LSM 类似，不同在于磁盘中的每一层包含 $k$ 个相同的 B+ 树，且 $L_{i+1}$ 层的 B+ 树的大小（叶子节点数量）是 $L_i$ 的 $k$ 倍，也就是说磁盘上的每一层的所有 B+ 树合并后刚好构成下一层的一个 B+ 树，但 $L_0$ 的 B+ 树和 $L_1$ 的 B+ 树一样大。 </p><br><p><font color=dark-green>LSM 的插入操作</font><br>当插入数据时，数据首先被插入内存中的 $L_0$ 树，当 $L_0$ 树容量达到限制时，将其写入磁盘并清空，也就是 $L_1$. 当 $L_0$ 再次达到上限后，再将其写入磁盘，以此往复，磁盘中就可能存在多个 $L_1:\ L_1^1,\ L_1^2,\ …$ 当磁盘中存在 $k$ 个 $L_1$ 时，就可以将这些树合并成一个 $L_2$ B+ 树，当磁盘中存在 $k$ 个 $L_2$ 时，又可以合并成 $L_3$. 这个过程可以一直执行下去。</p><blockquote><p><strong>为什么要合并呢？</strong>考虑查询操作，当要某条数据时（借助 search key），按照上面所述的步骤，查询过程应该是这样的：先查询 $L_0$，不中，再依次查询 $L_1^1,\ L_1^2,\ …$ 若还是不中，继续查询 $L_2^1,\ L_2^2,\ …$ 也就是说查询一条数据可能会搜索多颗 B+ 树。<u>为了限制查找开销（读放大），有两点优化，其一便是合并</u>，将若干个上一层的树合并成下一层的一个较大的树，这样原本会查询多颗小树的操作就只需要查询一颗更矮更胖的大树。<u>另外一个优化是使用布隆过滤器（bloom filter）</u>。尽管合并在一定程度上降低了查询开销，但仍旧不能避免在某一层需要检索多颗 B+ 树。布隆过滤器能够给出某个集合（这里就是一颗 B+ 树）中包含某一个 search key 的概率：若判断包含，则有一定概率不存在（假阳性），若判断不存在，那一定不存在。所以，可以为每颗 B+ 树分配一个布隆过滤器，查询时可以跳过一些树。布隆过滤器建立在 search key value 上，如果一棵树包含 $n$ 个 search key value，布隆过滤器大小为 $10n\ bits$，使用 7 个哈希函数，那么假阳性约为 $\frac{1}{100}$，也就是能跳过大量无效查询，开销略微高于常规的 B+ 树索引。<u>但是，对于范围查询</u>，布隆过滤器无能为力，还是需要检索每棵树判断是否有在范围内的数据。<u>除了减少读放大</u>，合并也能够减少写放大。</p></blockquote><p>在合并 $L_i$ 层时，先顺序读取 $L_i$ 层每棵树的叶子节点，然后将这些数据按照 search key value 的顺序合并，最后在上面以 bottom-up construction 的方式构建一颗 $L_{i+1}$ 的树。很明显，合并过程没有多余的随机 I&#x2F;O 读写，$L_i$ 的每棵树是按顺序从左至右读取每个叶子节点，合并时采用的排序算法（见 <a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9ASorting/" title="查询处理篇：Sorting">查询处理篇：Sorting</a>）也没有多余的随机读写，构建下一层树时，也是顺序读写。</p><p><u>可以使用写放大（write amplification）指标来衡量 SMI-LSM 和 B+ 树的磁盘写开销</u>。当插入一条数据时，需要写磁盘的总数据量（bytes）除以单条数据的大小，其值就是写放大。写放大衡量的主要是，当更新一条数据时，需要多少额外的写开销，若写放大为 10，那么更新一条数据，平均需要引起 10 条数据的写磁盘。</p><p><u>对于 B+ 树索引</u>，假设每个叶子节点写回磁盘前平均有 $x$ 次更新，那么写放大为：叶子节点的字节数（block 大小）除以 $x$ 条数据的字节数。假设一个 block 能够容纳 100 条数据，那么写放大为 $\frac{100}{x}$. <u>对于 SMI-LSM</u>，若 $k &#x3D; 5$，$L_0$ 的数据条数为 $M$，所有层总共的数据条数为 $I$，且 $100 &#x3D; \frac{I}{M}$. 那么该 LSM 大概有 $log_5(\frac{I}{M}) &#x3D; 3$ 层，写放大也就为 3，因为同一条数据只会在一层写入一次：写入 $L_1$ 层的一次来自 $L_0$，写入 $L_2$ 层的一次来自 $L_1$ 层的合并……</p><p><font color=dark-green>LSM 的删除操作</font><br>一般来说，删除一条数据首先是找到它，然后再删除它，SMI-LSM 并不采用这种方式，而是通过<u>插入删除标记（deletion entry）</u>的方式来标记某条数据已经被删除了。因此 SMI-LSM 的删除操作过程和插入过程一致，只不过插入的是一条 deletion entry. </p><p>插入 deletion entry 后，LSM 的查找和合并操作有些不一样，先看原文：</p><blockquote><p>However, lookups have to carry out an extra step. As mentioned earlier, lookups retrieve entries from all the trees and merge them in sorted order of key value. If there is a deletion entry for some entry, both of them would have the same key value. Thus, a lookup would find both the deletion entry and the original entry for that key, which is to be deleted. If a deletion entry is found, the to-be-deleted entry is filtered out and not returned as part of the lookup result.</p></blockquote><blockquote><p>When trees are merged, if one of the trees contains an entry, and the other had a matching deletion entry, the entries get matched up during the merge (both would have the same key), and are both discarded.</p></blockquote><p>需要解释下，可以确定，$L_0$ 中不会同时存在 deletion entry 和原数据，因为内存中删除操作很快也简单，当 $L_0$ 满了写入磁盘变成 $L_1$ 的 1 棵树，那么在 $L_1$ 的每棵树中也不会同时存在删除标记和纪录，在不同的树中可能同时存在。同一层中，每棵树的数据新旧程度不一样，后写入的更新一些，当合并时，当遇到 search key value 相同的删除标记和记录（可能有多条）时，若删除标记更加新，那么丢弃记录即可，若记录更新，忽略删除记录即可。这样，在下一层中，同一颗树中，同样不会同时存在删除标记和原记录。</p><p><font color=dark-green>LSM 的更新操作</font><br>LSM 的更新操作和删除操作过程类似，并不是找到记录然后更新它，而是插入一条<u>更新标记</u>。查找、合并时的过程和上面叙述的类似。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>由上面的叙述可知，LSM 在插入数据时（删除、更新一样）不会涉及到将数据读出磁盘然后再写入，它采用只追加（顺序写）的方式，拥有非常好的写性能。这是它最主要的优点。</p><p>B-LSM 具有很大的写放大，SMI-LSM 做出了一些改变减少了写放大，但增加了读放大，所以有了布隆过滤器等优化，不过 <u>SMI-LSM 不支持范围查询</u>，因为每层的各个 B+ 树之间会存在 key 范围的重叠，而 B-LSM 是支持的，因为每一层只有一个 B+ 树。</p><p>LevelDB 是一个存储引擎，和 B-LSM 结构类似，不过也做了很多优化，在一定程度上减少了写放大，依然支持范围查找，详见文章：<a href="/2022/08/04/LevelDB%20%E4%B8%AD%E7%9A%84%20LSM/" title="LevelDB 中的 LSM">LevelDB 中的 LSM</a>. </p><p>badgerDB 也实现了 LSM，进一步减少了写放大，详见文章：<a href="/2022/08/04/badgerDB%20%E4%B8%AD%E7%9A%84%20LSM/" title="badgerDB 中的 LSM">badgerDB 中的 LSM</a>.</p><p>另外，因为 LSM 的插入、更新、删除操作都是只追加的，<u>所以 LSM 很自然就支持 MVCC</u>：查找时，找到第一个版本就好了，合并时，刚好可以删除过期的版本，不过，内存中的结构也得保存多个版本。</p>]]></content>
    
    
    <categories>
      
      <category>索引</category>
      
    </categories>
    
    
    <tags>
      
      <tag>索引</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关系型数据库磁盘存储</title>
    <link href="/2022/06/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%A3%81%E7%9B%98%E5%AD%98%E5%82%A8/"/>
    <url>/2022/06/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%A3%81%E7%9B%98%E5%AD%98%E5%82%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文主要介绍关系型数据库的一张表在磁盘中是如何存储的。</p><p>绝大多数数据库系统将一张表（relation）存储在一个或多个文件中（file），由于 file 可能很大，为方便元组（tuple，也称 record）的增删改查，file 内部又被分为了多个 page（或 block，一般 4KB-8KB），真正的元组存储在 block 中。同时 block 也是内存和磁盘之间的数据传输单位。</p><p>以上的结构都是逻辑结构，存储在磁盘时，通常会由多个物理块（比如扇区）连续存储，来表示一个 block.</p><p>后面会依次介绍元组如何组织各个属性值，block 如何存储元组，以及 file 对元组的管理。</p><h2 id="属性值在元组中的存储"><a href="#属性值在元组中的存储" class="headerlink" title="属性值在元组中的存储"></a>属性值在元组中的存储</h2><p>假设创建如下的表：<br><img src="/img/storage_in_disk/1.jpg"><br>该表的四个属性有三个是变长的，最简单的一种方式是给每个属性值都分配其声明的最大值，在该例子中为 53 bytes。很多时候，属性值并不能达到其要求的上限，而且还可能存在 null 值，十分浪费空间。</p><p>为满足属性值变长特性，通常采用如下的方式组织一个元组内部的值：<br><img src="/img/storage_in_disk/2.png"><br>整个元组分为两部分：头部（header）和实际的数据存储部分，header 用来存储一些元数据，若表的模式确定，其头部大小也就固定了：</p><ul><li><u>空值位图（null bitmap）</u>。除了主键外，其他属性值可能会为 null，空值位图用每一位来表示某一属性值是否为空，若为空该位置 1. 在一些实现中，若属性值为空，则不分配空间，适合空值较多的表；一些实现中，若为空，依然分配空间，读取时忽视即可，如例子（例子中空值位图没有放置在最前面）。</li><li><u>各属性值的偏移与大小数组</u>。由于属性值可能是变长的，所以需要同时记录这些属性值在该元组中的偏移量和大小，如例子（例子中，内存占用固定的 salary 属性值放置在该数组后面）。</li><li><u>可见性标识</u>。用于并发控制，一般是锁标志。</li></ul><p>数据部分的 a, b, c, d 就是实际的属性值。对于大对象的存储，比如图片、视频等，属性值会保存一个指针（如指向另外一个 file 或者 page），然后将大对象存储在其他地方。</p><p><strong>注意</strong>，头部并没有存储表模式信息（schema），因为如何解析一条元组的数据是由数据库上层决定的。当一张表被创建时，其模式信息就保存在了数据库元信息中，当读取一条元组时，就依据这些元信息来解析元组中各属性值。</p><h2 id="元组在-block-中的存储"><a href="#元组在-block-中的存储" class="headerlink" title="元组在 block 中的存储"></a>元组在 block 中的存储</h2><p>在 block 中存储元组（tuple&#x2F;record），slotted-page structure 是用得最多的一种结构，如下图：<br><img src="/img/storage_in_disk/3.png"></p><p>可以看出也分为头部 header 和数据部分，头部包含三个部分：</p><ul><li><u>Record 的数量</u>。也就是元组的数量。</li><li><u>空闲空间的结束位置</u>。在插入元组时，由该位置能快速找到空闲空间放置元组。</li><li><u>记录 Record 偏移和大小的 Entry 数组</u>。由于元组的属性值变长，所以尽管属于同一张表，元组同样是变长的，该数组能方便的提取元组。</li></ul><p>当往一个 block 中插入一条 record 时，首先检查空闲空间能否放下该 record（由图很容易计算），若有，则从空闲空间的最后一个字节开始<strong>往头部移动</strong>分配空间给该 record，同时插入一个 entry.</p><p>当从一个 block 中删除一条 record 时，会将指向该记录的 entry 设置为删除（比如将 size，图中 y，设置为 -1），然后移动那些后面插入的 record 覆盖删除的 record。由于总的 record 数量不算太多，移动的代价还能接受。</p><h2 id="元组在文件中的组织方式"><a href="#元组在文件中的组织方式" class="headerlink" title="元组在文件中的组织方式"></a>元组在文件中的组织方式</h2><p>一张表包含一系列元组，由单个或多个文件（如分表）组成，而一个文件又可能存在多个 block，上一节是介绍元组在 block 中如何存储，这一节则是介绍如何为元组选取合适的 block.</p><h3 id="堆文件组织方式"><a href="#堆文件组织方式" class="headerlink" title="堆文件组织方式"></a>堆文件组织方式</h3><p><img src="/img/storage_in_disk/8.png"></p><blockquote><p>pages 相当于 blocks</p></blockquote><p>堆文件（heap file）组织方式中，总是将 record 放置到有空闲空间的 block 中（如第一个有空闲空间的 block）。在寻找具有空闲的 block 时，为了避免按顺序扫描所有的属于该文件的 block，绝大多数数据库采用了 free-space map 的结构。</p><p>free-space map 通常是一个整数数组，一个数组元素对应表的一个 block，元素的值与 block 空闲空间比例有关系，该数组存储在某个 block 中，一般在一开始就读进了内存中。如下例子：<br><img src="/img/storage_in_disk/4.jpg"></p><p>假设某个表具有 16 个 block，因此 free-space map 具有 16 个元素。假如使用 3-bit 来表示一个数组元素，那么将某个元素存储的值除以 $2^3$ 就能得到该 block 中空闲空间（至少）占据的比例，如元素值为 7 表示该 block 至少有 7&#x2F;8 的比例是空闲的。为何是这样？如果直接存储 $f &#x3D; \frac{block\_free\_space}{block\_total\_space}$ 那么都会为零，所以，存储的实际是 $f*2^n$，$2^n$ 表示该元素能表示的最大的数，如 PostgreSQL 就采用的 8-bit free-space map.</p><p>如果表很大，那么 block 数量也可能很大，free-space map 也就可能很大，为了进一步减少扫描的时间，可以采用多级 free-space map 结构，如下，采用两级：<br><img src="/img/storage_in_disk/5.png"><br>很好理解，其实就是将第一级分组，然后选出其中最大的构成第二级，类似折半查找，如果第二级数量依旧庞大，可以继续分下去。</p><p>每当插入元组后，block 的空闲空间比例可能会变小，free-space map 结构也就需要更新，更新的 free-space map 存在于内存中，所以需要将其写入磁盘。然而，每次更新都写磁盘代价高昂，一般采用延迟写（周期性），这样就会导致磁盘的文件过时，若未来得及写磁盘就发生了故障，重启后，free-space map 就记录了过时的数据。不过，这没啥大不了的问题，数据过时导致查找的 block 与实际不符，多来几次也就将过时的信息更新了，当然也可以周期性的扫描 block 来更新该结构。</p><h3 id="序列文件组织方式"><a href="#序列文件组织方式" class="headerlink" title="序列文件组织方式"></a>序列文件组织方式</h3><p>考虑到这样一种场景，某些记录总是以某种方式频繁检索，如某个年龄区间 + 性别，为了实现这样的高效检索，记录一般以检索键（search key）顺序存储。search key 由任何属性或一组属性构成，不一定是主键。</p><p>为了使得它们按 search key 顺序存储，每条记录还会存储下一条记录的位置（指针），检索时按照该指针就能快速检索相关记录；插入时，需要找到该记录的前一条记录，然后修改指针即可；删除时也需要修改指针。这些操作和链表一致。例子如下图：<br><img src="/img/storage_in_disk/6.jpg"></p><p>比如，若插入一条记录，则可以按照如下方式：</p><ul><li>定位待插入记录的前一条记录所在的 block. 可以通过记录每个 block 中最大最小 search key 做到。</li><li>如果该 block 有空闲空间，插入，并修改相应的指针，若无，则插入一个 overflow block（就如图片展示的那样），并修改指针。</li></ul><h3 id="散列文件组织方式"><a href="#散列文件组织方式" class="headerlink" title="散列文件组织方式"></a>散列文件组织方式</h3><p>哈希结构通常用在内存中存储数据，它的特点是能够快速定位记录的位置。在内存中，哈希通常是一个指针数组，每个指针是一个链表头，该链表存储定位到该数组元素的实际数据（链地址法），比如数据库在内存中的哈希索引结构。而在 disk-based 哈希中，哈希结构一个 bucket（可以理解为 block） 数组，每个 bucket 存储实际的记录。</p><p>当插入一条记录时，计算该记录的哈希值，$buket\_idx &#x3D; h(r\_key)\ \%\ buket\_num$，然后再将记录插入该 bucket，若 bucket 满了，那么就会新分配一个 bucket 链接在后面作为 overflow bucket，将记录插入新的 bucket 中。</p><p>当查找某条记录时，采用同样的方式计算 bucket_idx，然后在 bucket 及其 overflow bucket 中查找（一般是顺序检索）是否存在。具体结构如下：<br><img src="/img/storage_in_disk/7.png"></p><p>之所以会出现 overflow bucket，有以下原因：record_key 分布不均匀，或者是哈希函数选择得不够好，导致某些 bucket 记录特别多，而有些又特别少。一般通过选取好一点的哈希函数，或者通过分配更多 bucket 缓解（如 bucket_num &#x3D; $(n_r &#x2F; f_r) * (1 + d)$，$n_r, f_r$ 分别表示总的记录数和每个 bucket 能够存储的记录数，d 表示一个经验系数，一般取 0.2）。而 record 的数量很多时候根本无法预估，固定分配的哈希结构（static hashing），就可能不能适应记录数量的增减，一般有以下方式解决：</p><ul><li>rehashing. 此方法需要重新计算所有的记录，比较耗时。</li><li>dynamic hashing. 具体见<a href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%80%EF%BC%9A%E5%93%88%E5%B8%8C%E7%B4%A2%E5%BC%95/" title="索引篇一：哈希索引">索引篇一：哈希索引</a></li><li>采用其他结构。</li></ul><h3 id="B-tree-文件组织方式"><a href="#B-tree-文件组织方式" class="headerlink" title="B+ tree 文件组织方式"></a>B+ tree 文件组织方式</h3><p>B+ tree 文件组织方式，见 <a href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%BA%8C%EF%BC%9AB-plus%20Tree/" title="索引篇二：B-plus Tree">索引篇二：B-plus Tree</a></p>]]></content>
    
    
    <categories>
      
      <category>数据库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据库文件存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>索引篇二：B-plus Tree</title>
    <link href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%BA%8C%EF%BC%9AB-plus%20Tree/"/>
    <url>/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%BA%8C%EF%BC%9AB-plus%20Tree/</url>
    
    <content type="html"><![CDATA[<h2 id="B-Tree-的结构"><a href="#B-Tree-的结构" class="headerlink" title="B+ Tree 的结构"></a>B+ Tree 的结构</h2><p><img src="/img/B_plus_tree/1.jpg" alt="图 1 B+ tree 节点结构"><br>如图 1 所示，B+ 树的每个节点包含 $n-1$ 个 search-key value，$K_1, K_2, …, K_{n-1}$，和 $n$ 个指针，$P_1, P_2, …, P_n$. 这些 search-key value 在节点中按序存储，即若 $i &lt; j$, 有 $K_i &lt; K_j$. 在实际中 search-key value 是可能存在重复的，<font color=red>目前只考虑不重复的情况，后面再介绍重复的情况</font>。</p><p>B+ 树的节点分为三类，叶子节点（leaf node）、非叶子节点（non-leaf node），下面一个一个介绍。<br><img src="/img/B_plus_tree/2.jpg" alt="图 2 叶子节点"></p><p><strong>首先是叶子节点</strong>。如图 2，是一个 n &#x3D; 4 的叶子节点，search-key 为 instructor 表的 name 属性. 对于 $i&#x3D;1, 2, …, n-1$，指针 $P_i$ 指向存储在文件中的、search-key value 等于 $K_i$ 的记录，$P_n$ 指向右边相邻的同级节点或者为 null（如果右边不存在节点）</p><p>每一个叶子节点<u>能够容纳的指针</u>的数量为 $[\lceil \frac{n}{2} \rceil,\ n]$，对于 n &#x3D; 4，其区间为 [2, 4]. </p><p>对于两个叶子节点 $L_i, L_j$，若 $i &lt; j$，即 $L_i$ 在 $L_j$ 的左边，那么对于所有在 $L_i$ 中的 search-key value，都小于 $L_j$ 中的每一个 search-key value. </p><p>B+ 树是一个多级索引结构，叶子节点常采用稠密索引，也就是说，出现在文件记录中的每一个 search-key value，也会出现在叶子节点中（一个指针对应一条记录）。<br><img src="/img/B_plus_tree/3.jpg" alt="图 3"><br><strong>然后是非叶子节点</strong>，非叶子节点是对叶子节点建立的多级稀疏索引。非叶子节点和叶子节点的不同是，非叶子节点的指针指向的是树节点而非记录，而且非叶子节<u>能够容纳的指针</u>数量也是 $[\lceil \frac{n}{2} \rceil, n]$. </p><p>在一个非叶子节点中，$P_i\ \ 1 &lt; i &lt; n$，指向一个子树，该子树包含的 search-key value 小于 $K_i$，但大于等于 $K_{i-1}$；指针 $P_1$ 指向包含小于 $K_1$ 的 search-key value 的子树；而 $P_n$ 指向其右边的相邻同级节点或者为 null（如果右边不存在节点）。</p><p>图 3 展示了一个在 instructor relation 文件上建立的完整 B+ 树，其中 n &#x3D; 4.</p><p><u>B+ 树是一颗平衡多叉树，即从根节点到每个叶节点的路径长度总是一样的，正是如此，才提供了良好的增删改查性能，而且每个节点至少要半满</u>。</p><p><font color=red>对于 search key value 存在重复的情况</font>，可以有如下的方式解决：</p><ul><li>叶子节点存储每一个出现的 search key value，无论它是否重复，那么 对于 $i &lt; j$, 就应该有 $K_i &lt;&#x3D; K_j$.</li><li>在叶子节点上，对于每一个 search key value，不再存储单个指针，而是一组指针，指向具有相同 search key value 的记录，由于每组指针的数量不确定，节点中的 search key value 个数也就不一定了。</li></ul><p>这两种方式都会增加操作的复杂度，不常使用。而绝大多数的数据库系统通过组合多个属性来构成不重复的 search key. 假如某个索引的 search key 为 $a_i$，并且它可能重复（不是主键或候选键），这时系统选择一个不可能重复的属性，假设为 $A_p$，将它们两个组合在一起构成新的 search key $(a_i, A_p)$，称为 composite search-key，也就是联合索引. <u>还有一个问题</u>，联合索引中各属性的顺序如何确定？</p><blockquote><p>在创建多列索引时，我们根据业务需求，where 子句中使用最频繁的一列放在最左边，因为索引查询一般会遵循最左前缀匹配的原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。</p></blockquote><blockquote><p>最左前缀匹配原则：索引的底层是一颗 B+ 树，那么联合索引的底层也就是一颗 B+ 树，叶子节点存储的 Key 是组合键值。由于构建一棵 B+ 树只能根据一个值来确定索引关系，所以数据库依赖联合索引最左的字段来构建。举个例子，以 (a, b) 组合属性建立联合索引时，会优先使用属性 a 的大小构建索引，当遇到 a 相等时，再按 b 的顺序，那么叶子节点的 Key 就可能是：[1, 3], [1, 4], [2, 8], [3, 1]…</p></blockquote><blockquote><p>对于 (a,b,c) 组合属性构成的联合索引，如果 where 子句中只包含 b 或 c 而不包含 a，那么该查询就不能使用该索引。</p></blockquote><h2 id="在-B-树上的查询"><a href="#在-B-树上的查询" class="headerlink" title="在 B+ 树上的查询"></a>在 B+ 树上的查询</h2><p>假设现在要查询一个 search-key 值为 $v$ 的记录，下图 4 展示了其伪代码，比较简单就不解释了：<br><img src="/img/B_plus_tree/4.jpg" alt="图 4"></p><p>由于每个节点有一个 $P_n$ 指针指向下一个同级节点，所以 B+ 树支持范围查询（range queries），其伪代码如下图 5：<br><img src="/img/B_plus_tree/5.jpg" alt="图 5"></p><p>在范围查询的伪代码中，先采用单值查询 find(lb) 的方式找到落在 [lb, ub] 中且最小的记录，然后向后遍历（可能跨节点）找到所有合适的记录。<u>需要注意</u>，伪代码中 n 表示的是 $K$ 的个数，和上文中的 n 意义不一样，上文中 n 表示指针的个数，在伪代码中，节点的最后一个指针应该是 $P_{n+1}$.</p><p><strong>现在计算下 B+ 树查询的时间复杂度</strong>。假设文件中共有 $N$ 条记录，从根节点到叶子节点的路径长度不超过 $\lceil log_{\lceil\frac{n}{2}\rceil}(N)\rceil$. </p><p>一般情况下，B+ 树节点的大小和 block 的大小一致，同为 4KB-8KB，假设 search key 的大小为 32B（一般没这么大），指针的大小为 8B，那么一个节点能存储 100 个 search key value. 如果文件中的记录总共包含 1,000,000 个 search key value，查找某个特定的记录最多只需要访问 4 个节点，即最多读 4 次磁盘，远远少于平衡二叉树的次数，最后只需要根据指针的值再读一次磁盘就能得到目标记录。</p><p><u>而范围查询代价更高一些</u>，假设最后返回的指针个数为 M，这 M 个指针在叶子节点上必然是连续的，所以要读取这 M 个指针，最多需要读取 $\lceil\frac{M}{n&#x2F;2}\rceil + 1$ 次磁盘，因为一个叶子节点最少有 $\frac{n}{2}$ 个指针。为了获取这些指针所指向的记录，还需要计算一些开销，<u>如果是 secondary index</u>，尽管记录的 search key value 是相邻的，但这些记录在文件中却不一定相邻，所以最多需要 M 次磁盘 I&#x2F;O. <u>如果是 primary index</u>，那么这些记录在文件中也是相邻的，所以一次磁盘 I&#x2F;O 读取一个 block，其中包含多条记录，磁盘 I&#x2F;O 大大减少。</p><p><font color=red>假如 search key value 存在重复，如何查找指定的记录？</font>使用上面所说的使用组合 search key 的方式去重，假设组合后的 search key 为 $(a_i, A_p)$，当给定 $a_i &#x3D; v$ 时，如何在组合索引中找到所有包含 search key 为 $v$ 的记录？使用范围查找，其范围为 $[lb, ub]$，且 $lb &#x3D;(v, min(A_p)),\ ub &#x3D;(v, max(A_p))$.</p><h2 id="B-树文件组织方式"><a href="#B-树文件组织方式" class="headerlink" title="B+ 树文件组织方式"></a>B+ 树文件组织方式</h2><p>本节讨论，如何以 B+ 树结构组织文件，需要与 B+ 树索引区分开。普通的 B+ 树索引是在文件上建立索引（叶子节点存储的是指针），而 B+ 树文件的叶子节点存储实际的记录。<br><img src="/img/B_plus_tree/6.jpg" alt="图 6"></p><p>如上图 6，在 B+ 树文件组织方式中，叶子节点（叶子 block）存储实际的记录，而不是指向记录的指针，而非叶子节点还和 B+ 树的定义一样。因为记录要比指针大很多，这样一来叶子节点存储的记录就会大大减少，但依旧要求至少半满。</p><p>查找、插入和删除操作与在 B+ 树上定义的一样。</p><p><strong>当要插入一条 search key value 为 $v$ 的记录时</strong>，搜索该 B+ 树，找到 $&lt;&#x3D; v$ 的最大的 search key value（或者最小的 $&gt;&#x3D; v$ 的 search key value），同样也就找到了待插入的 block（叶子节点）。<u>如果该 block 还有空间</u>，那么按 search key value 的顺序插入该记录（可能涉及到记录在 block 中的移动，参考数组的插入操作）。<u>如果没有空间</u>，执行分裂操作（split），将待插入纪录和原记录按照 search key value 顺序平均分成两部分，同时创建一个新的 block（叶子节点），将前一半放在原 block 中，后一半放入新 block中。还需要索引新 block，即放入一条记录 $(key, P_i)$ 到分裂 block 的父节点中，其中 $key$ 是新 block 中最小的 search key value，$P_i$ 则是指向新 block 的指针。很明显，这是一个递归操作，父节点的插入依然可能引起分裂。</p><p><strong>当要删除一条记录时</strong>，同样需要定位记录的 search key value，然后删除该记录（如果存在多个相等的 search key value，需要遍历它们直到找到目标记录），删除后，需要移动后面的记录填补删除记录的空白。由于每个节点都需要满足半满，如果删除一条记录后，节点不满足该要求，就需要从兄弟节点借一些记录，如果两个兄弟都不够，就只能释放 block，合并兄弟，同样地，这些操作也会影响到父节点。</p><p>B+ 树文件组织方式还能用于大文件，具体地，将大文件划分成多记录，再采用 B+ 树组织。</p><h2 id="一些其他的内容"><a href="#一些其他的内容" class="headerlink" title="一些其他的内容"></a>一些其他的内容</h2><p><font color=dark-green>Secondary Indices And Record Relocation</font><br>假设在 instructor 表文件上建立了两个索引，$A_{idx}$ 建立在 dept_name 属性上的，$B_{idx}$ 建立 ID 上，很明显 $A_{idx}$ 是一个 secondary index，而 $B_{idx}$ 是 primary index。不妨假设 $B_{idx}$ 采用 B+ 树文件组织方式，$A_{idx}$ 采用何种索引结构都无所谓，但一个文件只能有一种索引结构能够改变记录的存储位置，所以 $A_{idx}$ 的索引项只能存储指向记录的指针。当 $B_{idx}$ 节点分裂时，会有部分的记录移动到新的磁盘位置，这时候，那些 secondary index 就需要更新（记录的位置变了，之前的指针也就失效了），包括 $A_{idx}$. 这样的索引结构可能很多，可能涉及很多的记录，那么就可能需要非常多次的磁盘读写。</p><p><font color=red>一个比较广泛的解决方法是</font>，在 secondary index 中，对于某个 search key value，不再存储指向对应记录的指针，而是存储 primary index 的 search key 在该记录中的值。当使用 secondary index 查找记录时，就需要两个步骤：</p><ul><li>先在 secondary index 中找到该记录对应的 primary index 的 search key value.</li><li>然后借助该 value 再到 primary index 中查找具体的记录。</li></ul><p>这样就避免了更新大量 secondary index 中指向记录指针。举个例子，假如有一张表 $R$ 包含三个属性列：id，salary, name，其中 id 为主键。假如现在要在 name 上建立一个 secondary index，那么在索引叶子节点中存储的不再是 [$V_{name},\ ptr$]，而是 [$V_{name},\ id$]. 如果现在执行如下的 sql 语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-number">1.</span> <span class="hljs-keyword">select</span> id, name, salary <span class="hljs-keyword">from</span> R <span class="hljs-keyword">where</span> id <span class="hljs-operator">=</span> <span class="hljs-number">10</span><br><span class="hljs-number">2.</span> <span class="hljs-keyword">select</span> id name <span class="hljs-keyword">from</span> R <span class="hljs-keyword">where</span> name <span class="hljs-operator">=</span> &quot;Tim&quot;<br><span class="hljs-number">3.</span> <span class="hljs-keyword">select</span> id name salary <span class="hljs-keyword">from</span> R <span class="hljs-keyword">where</span> name <span class="hljs-operator">=</span> &quot;Joe&quot;<br></code></pre></td></tr></table></figure><p>执行第一条 sql 时直接走关于 id 的 primary index；执行第二条 sql 时，可以直接走关于 name 的 primary index；执行第三条 sql 时，需要先在关于 name 的 primary index 中找到对于的 id（可能有多个），然后再走关于 id 的 primary index，<u>这个过程称为回表</u>，减少回表带来的性能损失可以参与文章<a href="/2022/08/03/%E7%B4%A2%E5%BC%95%E7%AF%87%E5%9B%9B%EF%BC%9AParallel%20Index/" title="索引篇四：Parallel Index">索引篇四：Parallel Index</a>。</p><br><p><font color=dark-green>Search Key Value 长度可变</font><br>如果 B+ 树的 search key 为字符串，长度可变，那么每个节点的容量就不一定，出度不一定。同时，插入、删除等过程也会发生变化，不再根据 search key value 的个数判断是否指向分裂还是合并，而是 block 是否能够容纳新的内容。为了增加每个节点能够容纳的 search key value 的数量（尽量使树又矮又胖），<u>一种前缀压缩（prefix compression）的技术被提出来</u>。</p><p>对于每个非叶子节点，在索引子树时（或叶子节点），不需要存储完整的 search key value，而是只存储一部分前缀，只要在节点内部，能够区分各个子树即可，如下图 7。<br><img src="/img/B_plus_tree/7.jpg" alt="图 7"></p><p><font color=dark-green>B+ 树的高效构造</font><br>考虑这样这种场景，在一个非常大的表上（很多记录）建一个 secondary index 类型的 B+ 树索引，表和索引都很大，内存是放不下的。<u>按照最简单的方式构建</u>，扫描表的每一条记录，然后往 B+ 树中插入条目，可能是 search key value 和指向记录的指针，或者是前面提到的 search key value 和 primary index 中该记录对应的 search key value. 由于是 non-clustering index，所以相邻的记录可能要插入到不同的节点中（block），也就是可能存在大量的磁盘读写（表文件可以顺序扫描，每个 block 只需要一次磁盘 I&#x2F;O，而 B+ 树的各个节点 block 可能需要多次磁盘 I&#x2F;O，因为不同的记录需要插入到不同的节点，而 buffer pool 的容量有限，这些节点已经替换出了内存）。</p><p>像上面这样的大量插入条目的操作被称为该索引的 bulk loading，<u>通常采用的高效的方法是</u>，创建一个临时的索引条目文件。扫描表文件，为每条记录创建一条索引条目，追加到该临时文件，然后对该临时文件排序（见 <a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9ASorting/" title="查询处理篇：Sorting">查询处理篇：Sorting</a>），最后扫描已经排好序的文件（非此临时文件，具体见连接），将这些索引条目插入到 B+ 树中。</p><p><u>先排序再插入有什么好处？</u>上面提到的大量的磁盘 I&#x2F;O 是因为，这些在表文件中相邻的记录的索引条目不一定在索引结构中相邻（不在同一个节点 block 中），那如果先将这些索引条目按照 search key value 排好序，那么插入时，属于同一个节点的条目就能一次性插入该节点，也就是说每个节点 block 只需要一次磁盘 I&#x2F;O 即可。</p><p><u>性能上还能更近一步</u>，如果索引条目要插入的 B+ 树本身是空的，那么在排完序后，不需要采用插入的方式，而是直接在排完序后的文件上构建。一个文件包含多个 block，这些 block 就是叶子节点，然后取出这些 block 中的最小的 search key value，加上一个指向 block 的指针，就可以构成上一层的条目（分配新的 block，将这些条目写入），继续下去直到创建根节点。<u>这种方式称为 bottom-up B+ tree construction</u>，能够减少大量的磁盘读写（全部的叶节点也就创建好了，也写好了）。 </p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这里总结下 B+ 树的各种操作的时间复杂度，假设，文件中共有 $N$ 条记录，B+ 树的每个节点能存储 $n$ 个 search key value（也就是 $n+1$ 个指针），并且 search key value 不重复：</p><p><u>对于单个记录的查询操作</u>，找到目标记录的（如果存在）search key value 最多需要 $\lceil log_{\lceil \frac{n}{2}\rceil}(N)\rceil$ 次磁盘 I&#x2F;O，如果是采用 B+ 树组织文件，那么不需要额外的磁盘读写，如果是指针或 primary index 的 search key value 则还需要其他磁盘读写，这里考虑最简单的情况，采用 B+ 树组织文件，下同。</p><p><u>对于单个记录的插入操作</u>，找到待插入的节点（block），最多需要 $\lceil log_{\lceil \frac{n}{2}\rceil}(N)\rceil$ 次磁盘 I&#x2F;O，插入后，这些 block 还需要写回磁盘，所以一共需要 $\lceil log_{\lceil \frac{n}{2}\rceil}(N)\rceil + 1 + x$ 次磁盘 I&#x2F;O，其中 1 表示写回更新的 block，$x$ 表示可能分裂引起的 block 更新的数量（包括新分配的 block）。</p><p><u>对于单个记录的删除操作</u>，同插入操作。</p>]]></content>
    
    
    <categories>
      
      <category>索引</category>
      
    </categories>
    
    
    <tags>
      
      <tag>索引</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>索引篇一：哈希索引</title>
    <link href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%80%EF%BC%9A%E5%93%88%E5%B8%8C%E7%B4%A2%E5%BC%95/"/>
    <url>/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%80%EF%BC%9A%E5%93%88%E5%B8%8C%E7%B4%A2%E5%BC%95/</url>
    
    <content type="html"><![CDATA[<h2 id="哈希函数"><a href="#哈希函数" class="headerlink" title="哈希函数"></a>哈希函数</h2><p>本节内容是，如何选择一个好的哈希函数。一个理想的哈希函数能够将 search key 的值均匀地映射到各个 bucket 中，很多时候无法提前预知 search key 会取哪些值，所以功夫主要在哈希函数的选择上，一般就可能要具有如下的特质：</p><ul><li><u>映射均匀</u>。尽管无法精确知道 search key 会取哪些值，但可以能预知所有可能的取值（比如具有长度上限的字符串，整数等），那么只要保证该哈希函数能够将这一集合均匀地映射到每一个 bucket 即可。需要明白，这个集合中各元素出现的频率并不一定是相同的，需要考虑到频率。比如大学里的人年龄区间为 [14-80]，但 [20-30] 的频率远远大于其他区间。</li><li><u>映射随机</u>。哈希值的计算不能跟 search key 取值的任何外部可见性，比如字符顺序等，产生关联，这个不好理解，具体看下面例子。</li></ul><p>如图 1 是一个静态哈希的例子：<br><img src="/img/hash_indices/1.jpg" alt="图 1"></p><p>上图中，索引的 search key 为 ID，哈希函数为 ID 各位数之和模 8，共有 8 个哈希 bucket，每个的大小为 2，而且该 instructor 表是以 dept_name 顺序存储的，所以这是一个 secondary index 结构。图中展示的是已经映射完成情况，其中 bucket 5 被映射了 4 个值，超出了容量，所以分配了一个 overflow bucket.</p><p><font color=red>现在重新映射 instructor 表，看看如何选取哈希函数</font>。假设以 dept_name 为 search key，哈希函数为取 dept_name 的第一个字符，它处于 26 个字母的第几位，就将它映射到第几个 bucket 中（当然本例 bucket 数量需要增加到 26）。该哈希函数虽然简单，<strong>却并不满足映射均匀</strong>，因为在 dept_name 所有可能的取值中，以 B、R 开头的 dept_name 要多于 Q 和 X，这会导致 bucket 2 和 18 的元素远多于 bucket 17 和 24 中的元素。再假设，以 salary 为 search key，哈希函数为将薪水区间分为 10 份，比如 [30001, 40000], [40001, 50000], …, <strong>该函数均匀（均分了薪水区间），但却不满足随机</strong>，因为薪水处于区间 [60001, 70000] 的人数远多于 [30001, 40000] 的人数。</p><h2 id="可拓展哈希"><a href="#可拓展哈希" class="headerlink" title="可拓展哈希"></a>可拓展哈希</h2><p>表的记录数量在一段时间内既可能变得很大，也可能很小，静态哈希索引结构不适应这样的变化。单单考虑表的大小增长，静态哈希索引将有如下方式可以应对：</p><ul><li>不额外操作。那么随着记录的增多，overflow bucket 的数量剧增，各种操作将会变得很慢。</li><li>提前分配空间。该表的增长范围不太可能预知，那么很可能只能是扬汤止沸或者过犹不及（浪费空间）。</li><li>rehashing. 非常费时，很可能导致索引在一段时间内不可用。</li></ul><p>因此，哈希表能够随着记录的增长而增长，缩小而缩小的技术被设计出来，称为 dynamic hashing. 其中一种广泛使用的动态哈希是可拓展哈希（extendible hashing）。</p><p>总的来说，可拓展哈希通过合并某些空的 bucket 或者分裂某个溢出的 bucket 来应对 search key 值得减少和增长。可拓展哈希依然会存在 rehashing，但每一次只有一个 bucket 中的记录会被重新映射，开销可以接受。</p><br><p><font color=dark-green>可拓展哈希的数据结构</font><br><img src="/img/hash_indices/2.jpg" alt="图 2"><br>我们依然按照上面的原则选取哈希函数，可拓展哈希的哈希函数将 search key 的值映射为二进制串（一般 32 位），如上图，search key 为 dept_name. <strong>那如何将具体的哈希值与 bucket 绑定呢？</strong><br><img src="/img/hash_indices/3.jpg" alt="图 3"><br>可拓展哈希采用这样的方式，如上图，左边的 bucket address table 又可以称为目录页，其中存储的是指向 bucket 的指针，右边的 bucket 是实际存储记录的结构。总的来说，可拓展哈希根据哈希值的某几位二进制来决定将某个 search key 值映射到哪一个 bucket 中。<strong>如何决定使用哪几位呢？</strong>目录页使用一个全局的 $i$ 来决定使用的二进制串长度（global length），图中展示的是前缀，从最高位开始，而在实现中，为了方便，常常使用后缀，从最后一位开始，后面统一使用前缀。同时每个 bucket 还记录本地实际使用的位数 $i_j$（local length），满足 $i &gt;&#x3D; i_j$。在同一个 bucket 中的记录，其 search key 值的高 $i_j$ 位都相同，为什么这样先不管，后面会逐步清晰。初始情况下 $i &#x3D; 1, i_j &#x3D; 1$.</p><br><p><font color=dark-green>可拓展哈希的查询和更新操作</font><br>下面通过查询（lookup）、插入（insertion）和删除（deletion）几个操作，来理解可拓展哈希的运作机理。</p><p><strong>首先是查询</strong>。第一步是通过哈希函数计算哈希值 $val &#x3D; h(Key)$，然后取该值的高 $i$ 位，<font color=red>得到目录页的下标（十进制）</font>，再根据该下标得到目录条目，该条目含有指针指向具体的 bucket. 假设当前的 global length 为 2，key &#x3D; biology，如图 2，最高两位为 00，所以目录页的下标为 0，其指向 bucket 1. 然后具体的查找过程在 bucket 中执行。</p><p><strong>然后是插入</strong>。先执行上面描述的查询过程，找到具体的 bucket，不妨设为 $j$，如果 bucket 还有剩余的空间，则将记录插入到该 bucket，如果 bucket 已经满了，将要执行分裂操作（split），将该 bucket 中的记录 rehash. <font color=red>分裂操作做了什么？</font>当 bucket $j$ 满了，说明 $i_j$ 太小了，search key 中具有相同前缀（高 $i_j$ 位）的太多，区分度不够，需要增加位数，以将部分记录散列到其他的 bucket 中，有如下两种可能：</p><ul><li><u>如果当前 $i&#x3D;i_j$</u>，需要先增加目录页。通过 $i&#x3D;i+1$，增加新的一位参加映射，目录页数量就能加倍（ $2^{i+1}$）。增加的一倍的目录项都是空的（没有指向具体的 bucket），所以，会将这些新增的目录项指向与他们高 $i$ 位相同的目录项所指向的 bucket，如图 3 的 01 和 00 都指向同一个 bucket（即表示新增的一位在大多数目录项中还未用上）。<font color=red>此时，可以安全地 rehashing 了</font>。假设 bucket $j$ 中记录的 search key 都有前缀 $b_1b_2b_3$，当 $i_j&#x3D;i_j+1$ 后，新的前缀有两种可能，$b_1b_2b_30$ 和 $b_1b_2b_31$，我们让具有前者前缀的 search key 依旧映射到 bucket $j$，而后者则会被映射到新增的目录项所指的 bucket，如上面上述，该目录项依旧指向 bucket $j$，所以，需要分配一个新的 bucket，让该目录项指向它。所以 bucket $j$ 中的记录会有一部分被映射到新分配的 bucket 中。然后再次哈希待插入的记录，将其插入哈希值所指的 bucket 中。最后增加这两个 bucket 的 local length.</li><li><u>如果当前 $i &gt; i_j$</u>，若是该种情况，则不需要再增加目录项，可以直接 rehashing bucket $j$，和上面描述的过程一样（红色字体开始）。</li></ul><p>下面通过一个例子阐述其插入过程，search key 如图 2 所示。<br><img src="/img/hash_indices/4.jpg" alt="图 4"><br>假设可拓展哈希的初始情况如图 4，此时 $i, i_1, i_2$ 都为 1，，已经有三个记录映射到了 bucket 中。现在插入记录（22222, Einstein, Physics, 95000），因为 $h(Physics)$ 的最高位为 1，需要将其插入 bucket 2 中。而现在 bucket 2 已经满了，且 $i_2 &#x3D;&#x3D; i$，所以需要先增加 $i$，让目录项翻倍，然后 rehash bucket 2 中的记录，最后插入新记录，结果如图 5：<br><img src="/img/hash_indices/5.jpg" alt="图 5"><br>可以看到，11，01 是新增的目录项，原二进制串 ‘1’ 指向的 bucket 2 被分裂了，一部分记录被映射到新分配的 bucket 3 中（由 11 指向），新记录插入了 bucket 2 中。而对于 01，新增的 1 位还没用上，所以指向了与它具有相同前缀的 00 目录项所指向的 bucket.</p><p><strong>最后是删除</strong>，同样先通过查找操作找到 bucket，假设位 $j$，然后在 bucket $j$ 中查找指定的记录，若未找到，不做任何操作。若找到了，删除该记录。<font color=red>删除后，bucket 可能为空，若为空，则可以执行合并操作（coalescing）</font>. 合并操作是当 $i&gt;j$ 时对 bucket $j$ 执行分裂操作的逆操作，即回收（释放）bucket $j$，让指向 bucket $j$ 的目录项指向其他 bucket，设为 $p$. 寻找 bucket $p$ 是一个递归的过程，如图 6：<br><img src="/img/hash_indices/6.jpg" alt="图 6"></p><p>图中黄色所示的 bucket 删除后为空。左边和右边是两种不同的情况。<font color=red>左边</font>是该变空的 bucket 释放后，目录项指向新的 bucket，这个 bucket 所具备的特点是，local length 比原先小 1，并且，这两个目录项拥有相同的前缀，前缀长度刚好是这个新 bucket 的 local length. <font color=red>右边</font>，bucket 变空后，释放，然后按照左边那样找新的 bucket，但发现它们都已经变空释放了，所以就一路找到了 local length 为 1 的 bucket，两个目录项的前缀当然还是一样的，只不过前缀的长度为 1 了。所以这是一个递归的过程。</p><p>另外，当太多的的 bucket 变空后会被释放，所有剩余的 bucket 的 local length 的长度都小于目录页的 global length，就可以将目录页减半，即 global length 减少 1，释放内存，如图 6 右边。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>可拓展哈希既可以用在内存中存储数据，也可以用作磁盘数据索引结构。当用作磁盘索引结构时，bucket 是一段连续的空间，比如 4KB，里面插入了若干条记录，整个 bucket 会作为一个整体写入磁盘，读出时，再将这段空间解析为 bucket 数据结构（内存中），进行增删改查：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Bucket</span> &#123;<br>    <span class="hljs-comment">// function members (not occupy space)</span><br><br>    <span class="hljs-comment">// data members</span><br>    Pair&lt;KeyType, ValType&gt; records[MAX_NUM]<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">WriteToDisk</span><span class="hljs-params">(Bucket* bucket)</span> </span>&#123;<br>    <span class="hljs-keyword">return</span> io.<span class="hljs-built_in">write</span>((<span class="hljs-type">char</span>*)bucket, <span class="hljs-built_in">sizeof</span>(bucket))<br>&#125;<br><br><span class="hljs-type">void</span> <span class="hljs-built_in">ReadFromDisk</span>(Bucket* bucket, <span class="hljs-type">uint32_t</span> bucket_id) &#123;<br>    <span class="hljs-type">char</span> buffer[<span class="hljs-number">4096</span>];<br>    io.<span class="hljs-built_in">seek</span>(bucket_id * <span class="hljs-number">4096</span>)<br>    io.<span class="hljs-built_in">read</span>(buffer);<br>    bucket = <span class="hljs-built_in">reinterpret</span>&lt;Bucket*&gt;(buffer)<br>&#125;<br></code></pre></td></tr></table></figure><p>下面是书中对可拓展哈希的评价，不想翻译了，直接看原文吧：</p><p><strong>Static Hashing versus Dynamic Hashing</strong></p><blockquote><p>We now examine the advantages and disadvantages of extendable hashing, compared with static hashing. The main advantage of extendable hashing is that performance does not degrade as the file grows. Furthermore, there is minimal space overhead. Although the bucket address table incurs additional overhead, it contains one pointer for each hash value for the current prefix length. This table is thus small. The main space saving of extendable hashing over other forms of hashing is that no buckets need to be reserved for future growth; rather, buckets can be allocated dynamically.</p></blockquote><blockquote><p>A disadvantage of extendable hashing is that lookup involves an additional level of indirection, since the system must access the bucket address table before accessing the bucket itself. This extra reference has only a minor effect on performance. Although the hash structures that we discussed in Section 24.5.1 do not have this extra level of indirection, they lose their minor performance advantage as they become full. A fur- ther disadvantage of extendable hashing is the cost of periodic doubling of the bucket address table.</p></blockquote><blockquote><p>The bibliographical notes also provide references to another form of dynamic hash- ing called linear hashing, which avoids the extra level of indirection associated with extendable hashing, at the possible cost of more overflow buckets.</p></blockquote><br><p><strong>Comparison of Ordered Indexing and Hashing</strong></p><blockquote><p>We have seen several ordered-indexing schemes and several hashing schemes. We can organize files of records as ordered files by using index-sequential organization or B+- tree organizations. Alternatively, we can organize the files by using hashing. Finally, we can organize them as heap files, where the records are not ordered in any particular way.</p></blockquote><blockquote><p>Each scheme has advantages in certain situations. A database-system implementor could provide many schemes, leaving the final decision of which schemes to use to the database designer. However, such an approach requires the implementor to write more code, adding both to the cost of the system and to the space that the system occupies.</p></blockquote><blockquote><p>Most database systems support B+-trees for indexing disk-based data, and many databases also support B+-tree file organization. <u>However</u>, most databases do not sup- port hash file organizations or hash indices for disk-based data. <u>One of the important reasons</u> is the fact that many applications benefit from support for range queries. <u>A second reason</u> is the fact that B+-tree indices handle relation size increases gracefully, via a series of node splits, each of which is of low cost, in contrast to the relatively high cost of doubling of the bucket address table, which extendable hashing requires. <u>Another reason</u> for preferring B+-trees is the fact that B+-trees give good worst-case bounds for deletion operations with duplicate keys, unlike hash indices.</p></blockquote><blockquote><p>However, hash indices are used for in-memory indexing, if range queries are not common. In particular, they are widely used for creating temporary in-memory indices while processing join operations using the hash-join technique, see <a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9AJoin%20%E6%93%8D%E4%BD%9C/" title="查询处理篇：Join 操作">查询处理篇：Join 操作</a>.</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>索引</category>
      
    </categories>
    
    
    <tags>
      
      <tag>可拓展哈希</tag>
      
      <tag>索引</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>事务篇五：两阶段提交协议</title>
    <link href="/2022/06/25/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%8D%8F%E8%AE%AE/"/>
    <url>/2022/06/25/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%8D%8F%E8%AE%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这一篇文章主要是介绍如何在分布式数据库中保证事务的原子性和数据库的一致性。<font color=red>注意，两阶段提交协议只是用于协调在多个节点执行的事务（同一个事务被分成了多个子事务，并执行在多个节点上）的提交过程，并没有限制各个子事务在本地执行时使用的并发协议</font>。为方便叙述，给出一个分布式数据库的简化模型。<br><img src="/img/two-phase-commit/1.jpg" alt="系统架构"><br>其中 TC 表示 transaction coordinator，负责协调在本地节点发起的本地或全局事务（若是全局事务，则会将其分成若干个子事务并分发给其他节点）。 TM 表示 transaction manager，负责本地或全局（子）事务在本地节点的执行（包括数据访问、锁等）。</p><h2 id="两阶段提交协议（Two-phase-Commit-Protocol）"><a href="#两阶段提交协议（Two-phase-Commit-Protocol）" class="headerlink" title="两阶段提交协议（Two-phase Commit Protocol）"></a>两阶段提交协议（Two-phase Commit Protocol）</h2><p><font color=dark-green>协议执行步骤</font><br>假设节点 $N_i$ 的协调器 $C_i$ 发起了一个全局事务 T，当 $C_i$ 收到了<strong>所有</strong>执行 T （子）事务的节点的消息 —— <strong>T 已经执行完成，$C_i$ 开始 2PC 协议</strong>：</p><ul><li><u>第一个阶段</u>。$C_i$ 向 log 文件中追加一条 &lt;prepare T&gt; 记录并将其 <em>output</em> 到 stable storage。 然后，它发送一条 prepare T 消息给所有执行事务 T 的节点。当某个节点收到该消息后，它的 TM 将决定是否提交事务 T（在该节点执行的部分）。若不同意提交，该节点追加一条 &lt;no T&gt; 记录到 log 文件中，并回复一条 abort T 消息 给 $C_i$. 若同意提交，该节点追加一条 &lt;ready T&gt; 记录到 log 文件中，并将其 <em>output</em> 到 stable storage 中，然后回复一条 ready T 消息到 $C_i$.</li><li><u>第二个阶段</u>。只有当 $C_i$ 收到来自所有节点的 ready T 消息后，T 才能被提交，否则事务 T 必须被中止。如果 $C_i$ 收到了所有的 ready T 消息，它向 log 文件追加一条 &lt;commit T&gt; 记录，否则追加一条 &lt;abort T&gt; 记录，并将记录 <em>output</em> 到 stable storage 中。取决于上一步操作， $C_i$ 将发送一条 commit T 或者 abort T 消息给所有参与的节点，当其他节点收到该条消息后，追加 &lt;commit T&gt; 或 &lt;abort T&gt; 记录到 log 中，并执行相应的提交或中止操作。</li></ul><p>下图展示了，三个节点共同参与事务，并同意提交的情况：<br><img src="/img/two-phase-commit/2.jpg" alt="系统架构"></p><p><font color=dark-green>协议的一些细节</font><br>为防止参与者故障离线，协调器 $C_i$ 会在发送 prepare 消息后开启一个定时器，如果定时器超时后还存在没有回复的节点，那么 $C_i$ 就可以决定中止该事务，并发送 abort T 消息给所有节点。</p><p>在执行两阶段提交协议时，任一一个参与者，可以在发送 ready T 消息之前无条件的中止事务，一旦节点将 &lt;ready T&gt; 追加到日志文件中，事务在该节点就处于 ready state. <strong>事实上， ready T 消息是一个承诺：</strong>该节点对事务的处理将完全按照协调器 $C_i$ 的指令。节点为了在故障恢复后已经信守承诺，所以将必要的日志记录到 stable storage 中。而且，事务所获取到的锁也必须持有直到事务结束。需要注意，协调器所在的节点也会执行事务的一部分，因此 $C_i$ 可以单方面的决定事务的提交还是中止，尽管它收到了所有节点的 ready T 消息。</p><p>当事务的命运被决定后，协调器会发送 commit T 或者 abort T 消息给所有参与的节点，为防止节点未收到该消息，<strong>一些实现中会增加一个步骤：</strong>协调器发送完消息后，会等待节点回复 acknowledge T 消息，当收到所有回复后，协调器追加一条 &lt;complete T&gt; 到日志中。该步骤未完成前，协调器需要记录对事务的决定，因为可能一些未收到消息的节点会询问，该步骤完成后，即可丢弃该事务的信息。</p><h2 id="协议中的故障处理"><a href="#协议中的故障处理" class="headerlink" title="协议中的故障处理"></a>协议中的故障处理</h2><p>假设某次全局事务 T 的发起节点为 $N_i$，协调器为 $C_i$.</p><p><font color=dark-green>非协调器节点 $N_k$ 故障离线</font><br>事务两阶段提交协议执行中，协调器 $C_i$ 检测到节点 $N_k$ 发生故障，会做出如下反应：</p><ul><li>如果故障发生在协调器收到 ready T 消息之前，$C_i$ 会假定 $N_k$ 拒绝提交该事务。</li><li>如果故障发生在协调器收到 ready T 消息之后，$C_i$ 走正常流程，忽视故障。</li></ul><p>当该故障节点 $N_k$ 重新上线后，该节点会检查 log 文件进入恢复流程（详见<a href="/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/" title="事务篇四：Log-Based Recovery System">事务篇四：Log-Based Recovery System</a>），假设事务 T 存在于日志文件中：</p><ul><li>如果日志包含了 &lt;commit T&gt; 记录，$N_k$ 对事务 T 执行 redo 操作。</li><li>如果日志包含了 &lt;abort T&gt; 记录，$N_k$ 对事务 T 执行 undo 操作。</li><li>如果日志只包含了 &lt;ready T&gt; 记录，说明节点故障发生在节点做出承诺之后，事务的后续结果如何无从得知，所以该种情况下，节点必须询问协调器 $C_i$. <u>如果协调器在线</u>，会将结果回复给 $N_k$，该节点根据回复的结果执行 redo 或 undo；<u>如果协调器故障离线</u>，$N_k$ 会向系统中的所有节点询问，收到该询问的节点会查询 log 文件，若有相关的记录则回复，否则忽略，如果没有任何节点能告知 $N_k$，则 $N_k$ 必须周期地发送询问给所有节点直到有答案产生（$C_i$ 必然有相关的记录）。</li><li>如果日志中不包含任何关于 T 的记录，即 abort、commit、ready 都没有，那么可以肯定，$N_k$ 在回复 prepare T 消息之前就发生故障了。结合协调器 $C_i$ 对该种情况的处理，$N_k$ 必须对事务 T 执行 undo.</li></ul><br><p><font color=dark-green>协调器节点 $C_i$ 故障离线</font><br>如果协调器 $C_i$ 在执行提交协议过程中故障离线，剩下的节点就必须决定事务 T 的命运，可能存在以下可能：</p><ul><li>如果某个节点包含了 &lt;commit T&gt; 记录，事务 T 必须提交。</li><li>如果某个节点包含了 &lt;abort T&gt; 记录，事务 T 必须中止。</li><li>如果存在节点不包含 &lt;ready T&gt; 记录（还未回复&#x2F;收到 prepare T 消息），说明这种局势下，就算 $C_i$ 没有发生故障，也不能决定事务的提交与否。比起等待 $C_i$ 恢复，中止 T 更有好处（因为不确定的因素太多）。</li><li>如果上面情况都不满足，说明所有的节点都包含 &lt;ready T&gt; 记录，但是没有额外的记录（即 commit 或 abort）。这种情况 $C_i$ 既有可能已经决定了事务 T 的命运，也可能没有，<font color=red>因此必须要等待协调器 $C_i$ 恢复</font>。</li></ul><br><p><font color=dark-green>网络分区故障（Network Partition）</font><br>网络分区故障是指，整个分布式系统因为网络连接问题被划分成了几个子系统（partitioned），这几个子系统互相之间没有网络连接。单个子系统中可能存在多个节点，也可能只存在一个节点。</p><p>提交协议执行中，当网络分区故障发生后，有如下两种可能：</p><ul><li>协调器 $C_i$ 和其他参与者仍旧处于同一个分区，该种情况下，网络分区故障对该次提交协议没有任何影响。</li><li>协调器 $C_i$ 和其他参与者属于多个分区。不妨假设两个分区，第一个分区包含协调器和部分参与者，第二个分区包含其他参与者。第一个分区内的处理方式和正常情况下处理非协调器故障离线方式一样；第二个分区内的处理方式和正常情况下处理协调器故障离线的方式一样。</li></ul><h2 id="2PC-协议存在的问题"><a href="#2PC-协议存在的问题" class="headerlink" title="2PC 协议存在的问题"></a>2PC 协议存在的问题</h2><p>协调器的失败可能使得某个事务处于模糊状态（remain in doubt）。这段时间内（hours or days），事务需要持持有系统的资源，比如事务持有某些数据的排它锁，在参与该事务提交的节点上，其他新的事务就可能必须等待。因此数据不仅在失败的节点上不可用，而且在活跃的节点上也变得不可用。</p><p>由以上内容可知，<font color=red>2PC 协议最大的缺点是，可能导致多个节点阻塞（blocking），即对某个事务的决策（提交或者中止）需要推迟到协调器重新上线。</font></p><p>另外，引入分布式提交后，故障节点重启后的恢复过程也有些不一样了。这些不一样体现在恢复算法（见<a href="/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/" title="事务篇四：Log-Based Recovery System">事务篇四：Log-Based Recovery System</a>）对待处于不确定状态的事务，即对于该事务，存在 &lt;ready T&gt; 记录在 log 文件中，但不存在 &lt;commit T&gt; 或 &lt;abort T&gt; 记录，因此该节点必须询问其他节点后才能决定该事务提交与否。<font color=red>要知道询问过程可能维持很长一段时间，这段时间内该节点将变得不可用。</font>为了避免该问题，节点收到来自协调器的 prepare T 消息并同意提交后，不再记录 &lt;ready T&gt;，而是 &lt;ready T, L&gt; 到日志文件中，L 表示该事务持有的排它锁。等到重启恢复时，如果存在状态不确定的分布式事务，节点便重新获取这些锁，然后就可以开始正常的其他事务处理了，不需要再等待那些不确定状态的分布式事务，但需要注意，与这些排它锁冲突的事务依然需要等待。</p><h2 id="2PC-协议问题的解决"><a href="#2PC-协议问题的解决" class="headerlink" title="2PC 协议问题的解决"></a>2PC 协议问题的解决</h2><p><font color=dark-green>解决方案一：共识算法</font><br>通过上面的讨论，分布式事务的提交决议过程不能避免等待，问题出在等待的时长。如果协调器故障离线，那么这样的阻塞等待可能持续数小时或数天，使得大规模的节点不可用。在<a href="/2022/06/14/raft-%E6%9D%82%E8%AE%B0/" title="Raft 算法问答录">Raft 算法问答录</a>一文中介绍的共识算法就可以用来处理协调器（可以理解为leader）失效的情况。只要大部分的参与者可用，那么等待时间就不会太长。</p><p>采用共识算法处理 2PC blocking 问题的建议早在 1980s 就已经提出了，在 Google Spanner 分布式数据库系统中就采用了该算法。</p><br><p><font color=dark-green>解决方案二：3PC 协议</font><br><a href="https://blog.csdn.net/qq_31960623/article/details/116429261">见博客</a></p><br><p><font color=dark-green>解决方案三：Persistent Messaging</font><br>未完待续</p>]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式事务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>事务篇三：事务的并发控制</title>
    <link href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/"/>
    <url>/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E6%80%BB%E8%AE%BA/" title="事务篇一：事务总论">事务篇一：事务总论</a>中介绍了事务基本概念和具有特性，在<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/" title="事务篇二：事务的可串行化">事务篇二：事务的可串行化</a>中介绍了调度要具备何种性质，才能在并发事务执行后使系统处于一致性状态和事务失败后能让系统回到安全状态，还论证了隔离性与一致性的关系。</p><p><strong>并发控制的工作就是，在事务并发时，确保只会产生具备这种性质的调度方案</strong>，具体地，并发控制策略的目标是同时保证两点：</p><ul><li>使系统具有较高的事务并发度</li><li>确保生成的调度方案，同时是冲突可串行化（conflict serializable），可恢复（recoverable）和非失败连锁式（cascadeless）的。</li></ul><p>并发控制策略有许多，<font color=red>本文主要集中在两阶段加锁（two-phase locking）和快照隔离（snapshot isolation）</font>。介绍完这两个并发控制协议后，再讨论各个隔离级别的实现，如何避免事务总论中提到的现象。</p><h2 id="Two-Phase-Locking-Protocol"><a href="#Two-Phase-Locking-Protocol" class="headerlink" title="Two-Phase Locking Protocol"></a>Two-Phase Locking Protocol</h2><p>为了简单，目前讨论的事务只有两种操作：访问（read）和更新（write），实际上还有如：插入（insert）、删除（delete）和含条件的读取（predicate read），它们会放在以后慢慢讨论。</p><p>同时，现在只讨论操作针对的是单个数据（如元组），这其实是锁的粒度问题，后面会讨论大粒度锁的情况。</p><p>为此，最简单的锁具有两种模式：</p><ul><li>共享锁（shared-mode lock: S） </li><li>排他锁（exclusive-mode lock: X）</li></ul><p>在基于锁的的协议中，执行 read 操作时会对数据加共享锁，执行 write 操作时会对数据加排他锁，两种锁的相容性（compatibility）如下：</p><table><thead><tr><th align="center"></th><th align="center">S</th><th align="center">X</th></tr></thead><tbody><tr><td align="center">S</td><td align="center">true</td><td align="center">false</td></tr><tr><td align="center">X</td><td align="center">false</td><td align="center">false</td></tr></tbody></table><p><img src="/img/concurrency-control/txn-3.jpg" alt="事务3"></p><h3 id="两阶段加锁协议"><a href="#两阶段加锁协议" class="headerlink" title="两阶段加锁协议"></a>两阶段加锁协议</h3><p><font color=dark-green>基础版两阶段加锁协议</font><br>该协议要求一个事务请求加锁和释放锁分别集中在两个阶段：</p><ul><li>growing phase. 该阶段事务可以申请加锁，而不能释放任何锁</li><li>shrinking phase. 该阶段事务可以释放锁，但不能申请新的锁</li></ul><p>如图事务3满足两阶段加锁要求，需要注意，基本的两阶段加锁协议并不要求所有的释放锁操作都必须放在最后，如事务3将 unlock(B) 可以移到 lock-X(A) 后面，仍然满足两阶段加锁的定义。</p><br><p><font color=dark-green>基础版两阶段加锁协议的特点</font>：</p><ul><li><u>保证了 conflict serializability</u>. 假设一个调度内的事务都遵守该两阶段假设协议，同时将事务 growing phase 获取到的最后一个锁的位置定义为 lock-point，那么各个事务就可以根据其 lock-point 排序，该顺序一定是可串行化的。（证明：可以构造该调度的先行图，先行图有两种可能，要么无环，要么有环。若无环调度冲突可串行化；若有环，事务必然是相互等待对方到达 shrinking phase 释放冲突的锁，这是死锁。）</li><li><u>可能存在死锁</u>。如上</li><li><u>不能保证 recoverable 和 cascadeless rollback</u>.</li></ul><p>先行图、recoverable 和 cascadeless 的定义见<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/" title="事务篇二：事务的可串行化">事务篇二：事务的可串行化</a></p><br><p><font color=dark-green>如何保证 recoverable 和 cascadeless？</font><br>这里引入两种升级版两阶段加锁协议，一步到位之间使得满足这些协议的调度满足 cascadeless（自然就满足了 recoverable）。</p><p><strong>Strict Two-Phase Locking Protocol</strong>:<br>在基础版两阶段加锁协议上，增加了这样一条要求，所有的 exclusive-mode lock 必须要等到事务提交之后再释放。</p><p><strong>Rigorous Two-Phase Locking Protocol</strong>：<br>在基础版两阶段加锁协议上，增加了这样一条要求，所有的 lock 必须要等到事务提交之后再释放。</p><p>解释下为什么满足 cascadeless：</p><blockquote><p>cascadeless 要求，若事务 A 依赖事务 B（事务 A 读取了事务 B 所做的更新），事务 A 必须要等事务 B 体提交后才能执行依赖的操作，由于事务 B 会拥有排它锁直到提交，自然就满足了这个要求。</p></blockquote><p>这两种方式被广泛用于商业数据库系统中，也就是锁的释放通常要等到事务完成（提交或中止）。</p><br><p><font color=dark-green>如何处理死锁？</font><br>事务存在循环等待即进入了死锁，处理死锁一般有两种思路，这两种思路都会涉及到事务的回滚：</p><ul><li><u>死锁避免</u>。该思路是让系统永远不会进入死锁状态，一般用于死锁发送频率高的场景中。</li><li><u>死锁检测 + 恢复</u>。该思路是在死锁发生后在人为干预，解除死锁状态，一般用于死锁发生频率较低的场景。</li></ul><h3 id="死锁避免"><a href="#死锁避免" class="headerlink" title="死锁避免"></a>死锁避免</h3><p>实现死锁避免也有许多思路：</p><ul><li><u>破除循环等待条件</u>。实现方法有 1)通过给所有数据规定一个次序，加锁只能按该顺序加锁；2)事务开始之前完成对所有数据的加锁。最大的问题在于，如何准确的知道事务会涉及哪些数据。</li><li><u>基于抢占和回滚事务方式</u>。抢占很好理解，当两个事务发生冲突了，优先级高的事务可以让优先级低的事务回滚，下面主要介绍这种方式。</li></ul><p><font color=dark-green>基于抢占和回滚事务方式的死锁避免算法</font><br>抢占需要确定优先级，下面的两个算法采用时间戳（timestamp）的方式决定优先级的大小。每个事务在其开始时得到一个时间戳，时间戳越小，代表事务越老，若事务回滚重新开始，那么它保持时间戳不变。注意，该时间戳只用于两个事务发生锁冲突时：</p><ul><li><u>wait-die 算法</u>。当事务 TA 对某个数据加锁时，发现事务 TB 已经已经持有该数据的排它锁，若 TA 时间戳小于 TB 的时间戳，那么 TA 更老，选择等待；若 TA 的时间戳更大，则 TA 回滚。很明显，该算法不涉及抢占。该算法为何能避免死锁？因为它避免了循环等待的条件，假设形成了环，环首持有环尾需要的锁，那么环尾会直接回滚，矛盾。</li><li><u>wound-wait</u>。和上个算法不同的是，该算法涉及抢占，若 TA 的时间戳更小（TA 更老），那么 TA 会直接抢占该锁，TB 则回滚；若 TA 的时间戳更大，则 TA 等待。其避免死锁的证明同上。</li></ul><p>以上两个算法最大的缺点在于可能会造成不必要的回滚，一个简单的优化是在回滚之前先等待一段时间，不过这段时间对每个事务最好是随机的，而且其长度难以确定。</p><p>死锁检测与恢复不想写了，有时间再写吧。</p><h3 id="更丰富的锁粒度"><a href="#更丰富的锁粒度" class="headerlink" title="更丰富的锁粒度"></a>更丰富的锁粒度</h3><p>一个简单的例子，当锁粒度只有元组时，一个事务想更新一张表，那它就必须对表的所有元组加锁，显然不合适，反之，若只更新几个元组，用不上对整张表加锁。</p><p>多粒度一般采用分级结构实现，该结构可以称为 multiple-granularity tree，如下图：<br><img src="/img/concurrency-control/multiple-granularity.jpg"></p><p>（未完待续）</p><h2 id="Snapshot-Isolation"><a href="#Snapshot-Isolation" class="headerlink" title="Snapshot Isolation"></a>Snapshot Isolation</h2><p>快照隔离（snapshot isolation）属于多版本并发控制技术的一种。多版本并发技术通过维护一份数据的多个版本，让事务可以访问（read）数据的最新的已提交版本，而不是当前未提交事务正在修改的版本（该未提交事务可能更老，即开始于当前事务之前；也可能发生在未来，即如果按照串行执行，当前事务本应该看不到后面事务的更新，但并发执行，就有可能了）。</p><p>版本的概念并不容易理解，后面详细介绍 snapshot 后，再理解它在 percolator 的实现，这样就清楚得多了。</p><br><p><font color=dark-green>基本概念</font><br>当事务开始执行时，数据库给该事务一个 snapshot，里面包含的是该事务需要的、最新的已被提交的数据，之后该事务对数据的所有操作都在该 snapshot 上操作（该 snapshot 暂存在事务私有内存中，其他事务不可见）。对于只读事务，操作完就可以结束了（提交或者失败），不需要等待，也不会因为并发而被回滚；对于更新事务，它还需要将更新写入数据库（写入应当是一个原子操作），因此还涉及到一个验证步骤，后面会详细结束。</p><p>在快照隔离中，只读事务不需要等待，也不会 abort，<font color=red>那会不会有不可重复读的可能呢？毕竟多次读取，可不可能在某一次就读到了刚刚被更新的值呢？</font>不会的，因为读取到快照后，该快照被该事务独享（或者说被只读事务共享），不会涉及到更新。</p><br><p><font color=dark-green>实现细节</font><br>该协议给进入系统的每个事务两个时间戳：</p><ul><li><u>startTS</u>. 在事务开始时获取</li><li><u>commitTS</u>. 在事务准备写入更新到数据库时获取（或者是开始验证阶段时获取）</li></ul><p>每个被更新的数据都带有一个时间戳（即数据的版本号），它等于执行该更新事务的 commitTS，也就是说同一份数据可能存在多个版本。时间可以是系统时间，也可以是逻辑时间（计数器），只要保证不会存在相同的时间戳即可。</p><p>当一个事务 TA 读取一个数据时，读取到的数据具有这样的特点：在所有版本号小于等于 startTS(TA) 的数据中具有<strong>最大的版本号</strong>。从事务角度理解，一个事务看不到，任何在该事务开始之后提交的事务所作的更新。</p><br><p><font color=dark-green>更新验证阶段</font><br>按照上面所说，事务读取数据后各自操作，其他事务感知不到，那么就可能会出现两个并发的事务更新同一个数据，如果允许这两个事务都修改数据库，那么就会发生值覆盖，也就是更新丢失（lost update）。如何解决冲突，是验证阶段的工作。</p><p>一般有两种方法可以防止更新丢失，介绍之前先看一下 snapshot isolation 中的并发定义，若 TA 和 TB 的更新存在交集，当下面情况中任一发生时，就说事务 TA 和 TB 并发：</p><ul><li>startTS(TA) &lt;&#x3D; startTS(TB) &lt;&#x3D; commitTS(TA)</li><li>startTS(TB) &lt;&#x3D; startTS(TA) &lt;&#x3D; commitTS(TB)</li></ul><p><strong>第一种方法称为先提交先获胜（first committer wins）</strong>. </p><blockquote><p>在该方式中，当事务 TA 来到验证阶段时，先获取提交时间戳 commitTS(TA)，然后执行下面的步骤：</p><ul><li>检查<u>每一个</u> TA 更新的数据，看该数据是否已经存在一个落在 [startTS(TA), commitTS(TA)] 区间的版本。</li><li>如果存在这样的数据，TA abort.</li><li>如果<u>所有数据</u>都通过检查，TA 将其更新写入数据库，然后提交。</li></ul></blockquote><br><p><strong>第二种方法称为先更新先获胜（first updater wins）</strong>. </p><blockquote><p>在该方式中，当事务 TA 尝试更新一个数据时，先获取该数据的排它锁，<u>如果该锁获取成功</u>，执行下列步骤：</p><ul><li>如果该数据已经被并发事务更新了（也可以通过获取一个时间戳来判断），则 TA abort.</li><li>如果该数据没有被并发事务更新，则 TA 写入更新到数据库（结合其时间戳），然后继续其常规流程。</li></ul><p>但是，如果该数据的排它<u>锁已经被并发事务 TB 持有</u>，TA 选择等待下面的事情发生：</p><ul><li>TB abort，则 TA 就可以再次申请该锁，重复上述步骤。</li><li>TB commit，则 TA 只有 abort 了。</li></ul><p>这个过程获取的排它锁会一直持有直到事务提交或中止</p></blockquote><br><p><u>比较一下这两种方式</u>，先提交先获胜其实是将所有更新操作本地执行完后，统一提交时刻验证，可以将其所有的数据验证看成一个原子操作，这也是为什么它叫做先提交先获胜。先更新先获胜中，只要是更新某个数据，就进入验证阶段，每一个数据的更新都会有一个验证步骤。如果发生冲突了，先提交先获胜不会回滚数据库中的更新，而先更新先获胜则会。</p><blockquote><p>tips：多版本并发控制是一种乐观并发控制。何为乐观？先对数据执行操作，更新时才检测是否冲突。何为悲观？先对要操作的数据加锁，再对数据执行操作。上面的两种方式都属于乐观派，在 TiDB 的悲观 MVCC 实现中，当 SQL 语句被解析后就对涉及到的数据加锁，然后再开始乐观的 MVCC，具体见<a href="https://docs.pingcap.com/zh/tidb/stable/pessimistic-transaction">文章</a>。</p></blockquote><br><p><font color=dark-green>Snapshot Isolation 问题与解决</font><br>snapshot isolation 吸入的地方在于，读写分离，互不干扰。<strong>但它有一个致命的缺陷：不保证可串行化（serializability）</strong>，如下例子：<br><img src="/img/concurrency-control/snapshot-isolation-1.jpg"></p><p>如果构造该例子的先行图，会发现存在环：该调度非冲突可串行化。在两阶段加锁协议中该调度会产生死锁，不可能顺利执行下去。然而，在快照隔离策略中却可以执行、提交。如果这两个事务以可串行化的调度方案并发，那么 A、B 的值是一样的，具体的值取决于谁先执行，而快照隔离的结果却是将它们的值交换。</p><p><strong>这种现象被称为写偏斜（write skew）</strong>：两个并发的事务读取了对方会更新的数据，但这两个事务更新的数据却没有交集，如 Ti 读取了会被 Tj 更新的 B，Tj 读取了会被 Ti 更新的 A，而它们分别更新 A 和 B，没有交集，所以在验证阶段无法被检测出来。</p><p><font color=red>发生写偏斜的根本原因是什么？snapshot isolation 不能有效追踪防止 read-write conflict，验证阶段的两个方法都只限制了 write-write conflict</font>. 借助上面例子可以发现，发生写偏斜时，调度方案的先行图存在两条 read-write 冲突边，一条入边，一条出边：对于事务 Ti 写入了 A 的一个新版本，而 Tj 读取了 A 之前的一个版本（Tj -&gt; Ti, read-write edge）；对于事务 Tj 写入了 B 的一个新版本，而 Ti 读取了 B 之前的一个版本（Ti -&gt; Tj, read-write edge）：<br><img src="/img/concurrency-control/snapshot-isolation-2.jpg"></p><p><strong>由此，一种称为 Serializable Snapshot Isolation(SSI) 的新技术被提出以保证快照隔离可串行化</strong>。SSI 会追踪并发事务之间的所有 read-write 冲突，检查是否同时存在出边和入边，如果存在，则其中一个事务会被回滚。</p><p>趁热打铁读一读 snapshot isolation 的工业级实现：<a href="/2022/06/18/Percolator-%E9%9A%8F%E7%AC%94/" title="事务篇六：Percolator 随笔">事务篇六：Percolator 随笔</a>呀。</p><h2 id="隔离级别的实现"><a href="#隔离级别的实现" class="headerlink" title="隔离级别的实现"></a>隔离级别的实现</h2><p><font color=dark-green>如何实现读已提交级别？</font><br>读已提交有如下特点：</p><ul><li>从数据库中读时，只会读到已经提交了的数据，即<u>没有脏读（dirty read）</u></li><li>写入数据库时，只会覆盖已经提交的数据，即<u>没有脏写（dirty write）</u></li><li>可能发生不可重复读异常</li></ul><p><strong>何为脏读？</strong>能读取到尚未 commit&#x2F;abort 事务所做的更新，就叫脏读。脏读可能引发只能看到部分更新的问题，比如转账事务提交前就读取余额，很可能会出现账户余额蒸发的怪现象。</p><p><strong>何为脏写？</strong>当两个事务更新同一对象时，通常后者会覆盖前者所做的更新，但若是覆盖的是尚未 commit&#x2F;abort 事务的更新，就叫做脏写。脏写同样会产生问题，考虑自动发送邮件例子，一个事务刚刚填入收件人地址，还未提交，另一个事务却覆盖了这个邮件地址，邮件就会发往错误的地方。</p><p>一般数据库通过行锁（row-level lock）<strong>防止脏写</strong>。当事务想要修改特定对象时，它必须首先获得该对象的排它锁，持有到事务提交或中止。可以通过读锁<strong>防止脏读</strong>，然而该方法中，写事务会阻塞读事务，所以数据库通常会为数据保留一个最近的已提交的旧值，和正在更新的新值，若新值还未提交，任何读事务都会拿到旧值，若新值提交了，就会读到新值（发生不可重复读异常）。</p><br><p><font color=dark-green>如何实现读未提交级别？</font><br>读未提交有如下特点：</p><ul><li>写入数据库时，只会覆盖已经提交的数据，即<u>没有脏写（dirty write）</u></li><li>可能发生脏读</li><li>可能发生不可重复读异常</li></ul><p>防止脏写的实现方式和读已提交级别一致。</p><br><blockquote><p>不可重复读异常可能引发一致性问题。假设数据库中存在两个字段 A、B，这两个字段需要时时刻刻相等（数据约束）。假设一个并发调度为 T1-read(A), T2-write(A,B), T1-read(B)，如果允许不可重复读异常，那么事务 T1 读取出来的 A 和 B 不满足约束条件，在外部看来这是一个不一致状态。</p></blockquote><br><p><font color=dark-green>如何实现可重复读级别？</font><br>可重复读有如下特点：</p><ul><li>没有脏读</li><li>没有脏写</li><li>不会发生不可重复读异常</li><li>若使用快照隔离提供稳定的视图，则不会出现幻读</li></ul><p>读已提交维护数据的两个版本，但可能会读取到已提交的新值，而对于可重复读级别，可以使用快照隔离，让事务始终读取最开始读取的那一个版本，维持稳定的视图，来<strong>防止不可重复读异常，同时也解决了幻读异常</strong>。</p><p>这里再解释下幻读。下面是一张教师（instructor）薪资表：</p><table><thead><tr><th align="center">ID</th><th align="center">name</th><th align="center">salary</th></tr></thead><tbody><tr><td align="center">001</td><td align="center">Jery</td><td align="center">1000</td></tr><tr><td align="center">002</td><td align="center">Peter</td><td align="center">2000</td></tr><tr><td align="center">003</td><td align="center">Tim</td><td align="center">3000</td></tr></tbody></table><p>现在执行如下的 SQL 语句：</p><p><strong>select</strong> <em>ID</em>, <em>name</em><br><strong>from</strong> <em>instructor</em><br><strong>where</strong> <em>salary</em> &gt; 1000</p><p>假设当前系统采用可重复读隔离级别，一个用户执行了如上的查询，同时另外一个用户执行了如下的插入语句（删除语句也有如下的效果）：<br><strong><center>insert</strong> <strong>into</strong> <em>instructor</em> <strong>values</strong> (004, James, 4000)</center><br>查询事务读取到多少条的数据取决于它和插入事务的先后顺序，若查询事务多次读取，就有可能发生某次读到的内容比上次多，即发生了幻读。</p><br><blockquote><p>幻读异常可能引发一致性问题。假设表中某个数据列与另外一个数据列的数据是一一对应的，系统有这样的并发调度：T1-read(col1 &gt; A), T2-insert(col1&#x3D;A+1, col2&#x3D;A+1), T1-read(col2 &gt; A)，如果允许幻读，那么事务 T1 读出的 col1 &gt; A 的数据条数与 col2 &gt; A 的条数是不一致的。</p></blockquote><br><p><font color=dark-green>如何实现可串行化级别？</font><br>采用 SSI 和 两阶段加锁都可以实现。</p><blockquote><p>脏写对一致性的影响实在太严重了，所以所有的隔离级别都不允许脏写的发生。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>事务</tag>
      
      <tag>mvcc</tag>
      
      <tag>snapshot</tag>
      
      <tag>two-phase-locking</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>事务篇二：事务的可串行化</title>
    <link href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/"/>
    <url>/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>该文章的内容来自 database system concepts 17.5-7，建议先看文章：<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E6%80%BB%E8%AE%BA/" title="事务篇一：事务总论">事务篇一：事务总论</a></p><p>为什么会有事务的串行化？因为数据库系统会允许事务的并发（concurrency），事务并发有如下两个优点：<br><strong>提升吞吐量和资源利用率</strong>。事务通常包含多个操作（步骤），一些操作需要更多的 CPU 计算，而另外一些需要更多的 I&#x2F;O. CPU 和 I&#x2F;O 设备通常可以并行（parallel），如果同时允许多个事务并发，能减少 CPU 和 I&#x2F;O 的空闲时间，那么可以提升吞吐量和资源利用率。</p><p><strong>减少等待时间</strong>。如果所有事务只能串行，那么短事务只能等待长事务结束才能执行。如果事务能够并发，则能在一定程度上共享 CPU 和磁盘资源，减少响应时间。</p><p><strong>并发很有好处，但它可能会破坏事务的隔离性，破坏数据的一致性</strong>。因此有必要研究各个并发的事务需要满足什么关系，才能保证隔离性和数据库的一致性。并发的具体实现方案，参见文章：<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" title="事务篇三：事务的并发控制">事务篇三：事务的并发控制</a>。</p><h2 id="Schedules"><a href="#Schedules" class="headerlink" title="Schedules"></a>Schedules</h2><p><strong>考虑一个例子</strong>：有 A 和 B 两个账户，其初始值分别为 1000 和 2000，事务 T1 从账户 A 转账 50 到账户 B，而事务 T2 从账户 A 转账 10% 到账户 B。这两个事务包含许多指令（instructions），当它们并发时，这些指令可能以各种顺序执行（但属于同一个事务的的各指令间相对顺序一定，而属于不同事务的指令间顺序可能变化），这样具体的一个顺序称为一个调度（schedule），如下面的调度1和调度2<br><img src="/img/schedule-serializable/schedule1.jpg" alt="schedule 1"><br><img src="/img/schedule-serializable/schedule2.jpg" alt="schedule 2"></p><p><strong>串行调度（serial schedule）</strong>：在调度中，属于同一个事务的指令出现在一起。如 schedule 1 是串行的调度，而 schedule 2 不是。对于 n 个事务，可以组成最多 n! 个不同的串行调度。</p><p>很明显，串行的调度方案一定能保证隔离性和一致性，但在并发时，不同事务的指令可能交叉，调度并不总是串行的，但如果能保证一个调度方案对数据库的修改与某个串行调度方案的修改结果一致，那么该调度方案就保证了一致性，如上图调度1执行后，A+B&#x3D;3000，调度2执行后，A+B&#x3D;3000，这样的调度等价于串行调度，被称为<strong>可串行化调度（serializable schedule）</strong>。</p><h2 id="Conflict-Serializability"><a href="#Conflict-Serializability" class="headerlink" title="Conflict Serializability"></a>Conflict Serializability</h2><p>但我们不能将并发事务的调度方案执行后，根据结果来判断这样的并发是否能保证数据库的一致性。需要采取其他办法判断。</p><p>这里先介绍冲突可串行化（conflict serializability）的概念。</p><p>冲突的定义为：假设，指令 I 和 J 属于不同的事务且对相同的数据执行操作，当 I 和 J 之中至少有一个是修改操作（write）时，I，J 冲突。</p><p>对于非冲突的指令，我们可以交换它们的顺序而不会影响调度最终的结果（如交换调度2中 T1 的 read(B) 和 T2 的 write(A)），而对于冲突的指令，交换它们则会产生影响。如果一个调度方案 S1 能够通过交换一系列不冲突指令后，变成调度方案 S2，那么 S1 和 S2 冲突等价（conflict equivalent），如果 S2 恰好是一个串行调度方案，那么 S1 就可以称为<strong>冲突可串行化</strong>（与串行调度冲突等价）。</p><p>由上可知，如果一个调度方案冲突可串行化，那么它能保证数据的一致性，现在问题是如何判断一个调度是否冲突可串行化？构造<strong>先行图（precedence graph）</strong>后可以很容易的判断。</p><p>先行图是一个有向图，其顶点表示并发的事务，假设在一个调度方案 S 中有 TA 和 TB 两个并发事务，Q 表示某个数据。当且仅当以下任何一种情况发生时，TA 到 TB 有一条边 （TA -&gt; TB）：</p><ul><li>TA 执行 write(Q) 之后 TB 会执行 read(Q)</li><li>TA 执行 read(Q) 之后 TB 会执行 write(Q)</li><li>TA 执行 write(Q) 之后 TB 会执行 write(Q)</li></ul><p>如果边 TA -&gt; TB 存在，那么在任何与 S 等价的串行调度方案中，TA 都要先于 TB 执行。</p><p><strong>假如某个调度方案的先行图含有环，那么，该调度就不是冲突可串行化的，若没有环，则该调度是冲突可串行化的</strong>。</p><p><font color=red>调度可串行化就万事大吉了吗？</font>上面讨论的内容都没有提到事务失败的情况，见下图：<br><img src="/img/schedule-serializable/schedule3.jpg" alt="schedule 3"></p><p>很明显，调度3是冲突可串行化的。如果 T6 执行完 read(B) 后提交前失败了，按照原子性定义，T6 需要回滚，需要注意，T7 读取的数据正是 T6 更新的数据，而 T6 回滚时，T7 已经提交了。</p><p>一个允许事务并发的系统，为保证原子性，若一个事务失败了，依赖于这个事务的其他事务（即这些事务读取了失败事务所做的更新）都需要失败终止和回滚。在调度3中，T7 依赖于 T6，所以 T7 也需要回滚，但已经不可能了。</p><p>像调度3这样的调度方案属于不可恢复调度（nonrecoverable schedule）。</p><p><strong>可恢复调度（recoverable schedule）</strong>是指，在一个调度中，对于任一两个事务 TA, TB，若 TA 读取了 TB 所做的更新，那么 TB 的提交操作必须出现在 TA 的提交操作之前。若 T7 的提交操作延迟到 T6 提交之后，那么调度3就是可恢复的。</p><p>为了提升性能，还需要介绍一种调度：cascadeless schedule (不知道怎么翻译，暂译作非失败连锁式调度)</p><p>考虑下面调度4的情况：<br><img src="/img/schedule-serializable/schedule4.jpg" alt="schedule 4"></p><p>调度4中，T9 依赖 T8，T10 依赖 T9，若 T8 失败终止，那么 T9 和 T10 都需要失败终止。调度4同时满足冲突可串行化和可恢复，但单个事务的失败引起太多的事务失败，这显然会降低系统的性能，这样的调度被称为失败连锁式调度（cascade schedule）。</p><p>这里给出 <strong>cascadeless schedule 的定义</strong>：在一个调度中，对于任一两个事务 TA, TB，若 TA 读取了 TB 所做的更新，那么 TB 的提交操作必须出现在 TA 的<strong>读</strong>操作之前，显然，所有的 cascadeless 调度都是可恢复的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>为了保证事务并发执行和单个按次序执行之间结果的一致性，提出了可串行化调度的定义。为了更加容易判断一个调度是否可串行化，引出了冲突可串行化的定义。可串行化调度能保证事务都成功执行后的一致性，但未能保证事务失败后的原子性，为了保证原子性，提出了可恢复调度的概念。再进一步，为了减少事务失败所引起的回滚操作，在可恢复之上再做限定，给出了 cascadeless 调度的定义。</p><p>按照隔离性的定义，<u>事务并发在一定程度上会破坏事务的隔离性</u>，因为属于不同事务的指令会可能交叉执行。ACID 的本质是什么？目的只有一个，那就是把活干了的同时要保证数据库的状态是确定的、正确的，说白了，AID 都是在为 C 服务。</p><p><u>冲突可串行化在干什么？</u>它其实是给出了一种方法（先行图），这种方法从执行结果上来看是否与串行化执行一致（满足隔离性），只要结果与串行化一致，那就和满足了隔离性是等价的。</p><p><u>可恢复调度又在干嘛？</u>它其实是让并发时，事务也能满足原子性。</p><p>这里没有涉及到 D.</p><p>在文章 <a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" title="事务篇三：事务的并发控制">事务篇三：事务的并发控制</a>，会介绍采取何种方法，能生成满足这些定义的调度方案。</p>]]></content>
    
    
    <categories>
      
      <category>事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>事务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>事务篇四：Log-Based Recovery System</title>
    <link href="/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/"/>
    <url>/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><img src="/img/log-based_recovery_system/1.png" alt="磁盘交互模型"><br>事务的原子性和一致性（数据库数据一致性）的保证，除了在事务代码的编写（逻辑）、事务的调度（并发）上面下功夫外，还需要考虑系统故障的发生。当系统从故障中恢复后，应当正确处理这些异常，以保证事务的原子性和数据的一致性。</p><p><font color=dark-green>一个例子</font></p><p>考虑银行有两个账户 A 和 B，初始账户分别为 1000 和 2000，一个事务负责从账户 A 转账 50 到账户 B。假设故障发生在事务执行过程中，那么有如下可能发生：</p><ol><li>A &#x3D; 950, B &#x3D; 2050</li><li>A &#x3D; 950, B &#x3D; 2000</li><li>A &#x3D; 1000, B &#x3D; 2050</li><li>A &#x3D; 1000, B &#x3D; 2000</li></ol><p>根据磁盘交互模型图可知，事务需要先将需要的数据从 buffer pool 读入自己的私有内存，等到操作完成再将更新后的数据写入 buffer pool，至于这些数据何时会写入磁盘与 buffer pool 页面置换算法有关。上面情况 1 表示一切正常；情况 2 表示含有 A 新值的 block 被正常写入磁盘后系统崩溃；情况 3 表示含有 B 新值的 block 被正常写入磁盘后系统崩溃；情况 4 表示两个 block 都还未写入磁盘就发生故障（当然 A 和 B 可能存在于同一个 block 中）。在情况 2 和 3 中数据库一致性都已被破坏，事务的原子性也未能保证。</p><h2 id="Log-Based-Recovery-System"><a href="#Log-Based-Recovery-System" class="headerlink" title="Log-Based Recovery System"></a>Log-Based Recovery System</h2><p>为了保证事务的原子性和数据的一致性，需要一种方法从故障中恢复被部分修改的数据，目前最广泛使用的方式是：<strong>在更新数据库之前，先将描述修改的信息记录到 stable storage 中的 log 文件里</strong>，等到系统重启后，根据这些信息来恢复数据。这些信息被称为 log records，这个方法被称作 log-based recovery.</p><h3 id="Log-Records-amp-事务流程"><a href="#Log-Records-amp-事务流程" class="headerlink" title="Log Records &amp; 事务流程"></a>Log Records &amp; 事务流程</h3><p>当事务开始时，会向日志中追加一条事务<strong>开始记录</strong> &lt;$T_i\ $start&gt;，$T_i$ 表示事务标识符；在将更新写入数据库之前，会向日志中追加一条<strong>更新记录</strong> &lt;$T_i,\ X_j,\ V_1,\ V_2$&gt;，$X_i,\ V_1,\ V_2$ 分别表示数据标识符（通常用磁盘块号和偏移量）、该数据的旧值和新值；等到事务提交时，会向日志中追加一条<strong>提交记录</strong> &lt;$T_i\ $commit&gt;，如下面例子：<br><img src="/img/log-based_recovery_system/2.png" alt="图 2"><br>一旦这些记录被写入了日志文件中，系统就可以将事务的修改应用到数据库中了，就算故障发生，也能根据日志文件重放（replay）这些操作，恢复它们，正因为如此，日志文件必须持久化（写入 stable storage，定义见<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E6%80%BB%E8%AE%BA/" title="事务篇一：事务总论">事务篇一：事务总论</a>），以保证永久不丢失。需要注意，$T_0,\ T_1$ 的日志记录可能是相互交叉的。</p><p>现在在细节上更进一步，所谓的将记录追加到日志文件中、更新写入数据库中这些操作实际上包含两个步骤：</p><ol><li>将事务私有内存空间的数据写入某个 buffer block；</li><li>将 buffer block 写到磁盘上。</li></ol><p>第二步由数据库系统执行，发生的时机是不定的，<strong>所以当第一步发生后，就算是对文件和数据库做出了修改</strong>。需要明白，buffer block 中的数据可能会在内存中存在很长一段时间才会被写入磁盘（当然可以让第二步即时生效，但磁盘 I&#x2F;O 代价昂贵，非必须，一般是推迟写磁盘），在这段时间内系统完全有可能发生故障，所以事务提交流程中有一些<font color=red>原则（write-ahead logging, WAL, rule）必须要遵守，以保证事务的原子性</font>：</p><ul><li>事务已提交是指（可以回复客户端了），提交记录 &lt;$T_i\ $commit&gt; 已经被 <em>output</em> 到 stable storage 中。</li><li>在上一步执行之前，该事务的所有其他记录（存在于 log buffer 中）都被 <em>output</em> 到 stable storage 中。</li><li>在数据被 <em>output</em> 到数据库之前，与这些数据更新相关的所有日志记录都已经被 <em>output</em> 到 stable storage 中。</li></ul><p>只要遵守了以上原则，无论是系统崩溃后恢复还是事务的正常 abort，系统都可以根据这些记录将系统恢复到一致状态，事务自然也就保证了原子性。因此日志记录在 log 文件（stable storage）中的<strong>顺序十分重要</strong>，必须和写入 log buffer（buffer block）的顺序一致。</p><h3 id="Recovery-Algorithm"><a href="#Recovery-Algorithm" class="headerlink" title="Recovery Algorithm"></a>Recovery Algorithm</h3><p>在介绍恢复算法之前，先看看事务在正常情况下的流程，第一种是事务因失败需要 abort 回滚；第二种是事务正常提交。</p><p><strong>Case 1: 事务正常提交</strong>。这种情况下事务不需要回滚，日志文件就像图 2 所示那样。</p><p><strong>Case 2：正常 abort，事务回滚</strong>。当事务 $T_i$ 执行到一半时因为某些原因失败需要 abort 回滚（rollback 之后才能算 aborted）。回滚也就是将该事务对数据库所做的修改撤销，系统做如下操作（如图 3 示）：</p><ul><li>系统向后扫描 log 文件（注意 log 文件是只追加的，向前是指追加的方向），对于每一条属于 $T_i$ 的更新记录 &lt;$T_i,\ X_j,\ V_1,\ V_2$&gt;，系统将使用旧值 $V_1$ 更新 $X_i$ （即撤销修改），同时向日志追加一条 redo-only 记录 $&lt;T_i,\ X_j,\ V_1$&gt;。</li><li>当遇到记录 &lt;$T_i\ $start&gt;，系统向日志追加一条 &lt;$T_i\ $abort&gt; 记录，对于该事务的回滚也就结束了。</li></ul><p>系统回滚所添加的这些日志记录又被称为 compensation log records. 其实可以这样理解，事务中止处于未完成状态，系统创建一个互补事务，负责撤销事务所做的更改，并补充完整日志记录。</p><p><img src="/img/log-based_recovery_system/3.png" alt="图 3"></p><br><p><font color=dark-green>恢复算法思想</font><br>系统重启后是很懵逼的，它不清楚执行过的事务的具体情况，所以它需要查看 log 文件，根据日志记录的完整性，可以将事务分为两种：</p><ul><li><p><u>事务执行完成（committed 或 aborted）</u>。当系统重启后，发现某事务在 log 文件中包含了完整的日志记录，同时具有 &lt;$T_i\ $start&gt; 和 &lt;$T_i\ $commit&gt; 或 &lt;$T_i\ $abort&gt;。根据 WAL rule，尽管日志表明该事务已经完成，无论是事务自己正常提交还是系统所做的互补操作，但它不能确定这些操作已经写入数据库中（很可能系统在将 log buffer <em>output</em> 到 stable storage 之后就失败了，还未来得及将 data buffer <em>output</em> 到数据库），<strong>因此它必须依据日志 redo 这些操作</strong>。</p></li><li><p><u>事务执行未完成</u>。当系统重启后，发现某事务在 log 文件中没有包含完整的日志记录，即缺少了 &lt;$T_i\ $commit&gt; 或 &lt;$T_i\ $abort&gt; 记录，为保证事务的原子性和数据的一致性，该事务所做的操作必须撤销，<strong>也就是必须 undo 这些操作</strong>。当然，系统有可能在 rollback 未完成时就发生了故障，也就是日志中含有不完整的 redo-only 记录，undo 这些记录实际上就是 redo 它们。</p></li></ul><p>上面简单描述了恢复算法的的思想，很朴素，<font color=red>但却存在很大的性能问题</font>，试想在系统重启之前可能存在成千上百的事务执行大量的操作，这些操作都记录在了 log 文件中，log 文件可能变得十分巨大，若系统重启后对这些操作全都执行 redo 或 undo，系统可用性就会很差，何况，有很多操作已经实实在在写入了数据库，不需要再做额外的操作。为了减少恢复时间，数据库系统通常采用一种简单的方法，称作 checkpoints. </p><br><p><font color=dark-green>Checkpoints 思想</font><br>checkpoint 是一组如下的操作：</p><ol><li>从 log buffer <em>output</em> 所有的日志记录到 stable storage 中。</li><li>从 data buffer <em>output</em> 所有的数据修改到数据库磁盘文件中。</li><li>系统向日志文件追加一条 &lt;checkpoint L&gt; 记录，并 <em>output</em> 到 stable storage 中。L 是一组事务标识符的集合，表示执行 checkpoint 时，还处于活跃状态的事务集合。</li></ol><p><font color=red>那么有了 checkpoint 后，又当如何缓解上述问题呢？</font>假设存在一个事务 $T_i$，它在执行 checkpoint 操作之前已经完成（committed 或 aborted），那么该事务的相关日志记录要么在 checkpoint 之前 <em>output</em> 到日志文件了，要么在 checkpoint 过程中 <em>output</em> 到日志文件，总之这些记录出现在 &lt;checkpoint L&gt; 之前，即 L 中不包含 $T_i$，系统重启后，也自然没有必要 redo $T_i$ 的操作了。随着日志文件越来越大，有了 checkpoint 记录，就可以丢弃以前的记录了，反正也用不上了。</p><br><p><font color=dark-green>恢复算法详细步骤</font><br>有了以上的知识背景，现在可以提出完整的 log-based recovery algorithm 步骤了，当数据库系统从崩溃中重启后，会完成如下两个部分的工作：</p><p><strong>重放阶段（Redo Phase）</strong>. 该阶段，系统从最后一条 checkpoint 记录开始<font color=red>向前扫描</font>（追加方向），重新执行具有完整日志记录（含有 commit 或 abort 记录）的事务所做的操作。另外，该阶段在扫描时，还会记录 undo-list，包含那些没有完整记录（不含有 commit 或 abort 记录）的事务标识符，这些事务要么出现在最后一条 checkpoint 记录的 L 中，要么在最后一次 checkpoint 操作后才开始。在扫描过程中，有如下步骤：</p><ol><li>使用最后一条 checkpoint 记录中的 L 集合初始化 undo-list</li><li>当扫描到一条更新记录 &lt;$T_i,\ X_j,\ V_1,\ V_2$&gt; 或者 redo-only 记录 &lt;$T_i,\ X_j,\ V_1$&gt; 时，系统重新执行记录表示的操作。</li><li>当扫描到一条 &lt;$T_i\ $start&gt; 记录时，将 $T_i$ 添加到 undo-list 中。</li><li>当扫描到一条 &lt;$T_i\ $commit&gt; 或 &lt;$T_i\ $abort&gt; 记录时，将 $T_i$ 从 undo-list 中移除。</li></ol><p><strong>撤销阶段（Undo Phase）</strong>. 上一阶段结束后，undo-list 包含了所有处于未完成状态的事务标识符。系统从日志文件结束位置开始<font color=red>向后扫描</font>，撤销这些未完成事务的所有操作，扫描中，包含如下步骤：</p><ol><li>当扫描到属于 undo-list 中事务的日志时，执行 undo 操作，和正常情况下事务失败回滚的操作一样。</li><li>当扫描到属于 undo-list 中事务的 &lt;$T_i\ $start&gt; 日志后，系统向日志文件追加一条 &lt;$T_i\ $abort&gt;，并将该事务从 undo-list 中移除。</li><li>当 undo-list 为空时，扫描停止，该阶段结束。</li></ol><p>在 redo 阶段可能存在重复的操作，比如正常情况下的失败事务的日志记录和回滚时的记录，但正是这少许的重复，使得整个流程简单了不少，只需要两遍扫描就能完成恢复。上述两个过程如图 4 所示：<br><img src="/img/log-based_recovery_system/4.jpg" alt="图 4"></p><br><p><font color=dark-green>一些细节</font><br>checkpoint 阶段应该也是原子操作，也就是说，在执行 checkpoint 时，不允许事务向 buffer 中写更新，这个要求可以通过 fuzzy checkpoint 技术放松，这里不谈。</p><p>而且，当 buffer block 向磁盘 <em>output</em> 时，也不允许事务向 buffer block 执行写操作，不然可能会违法 WAL 规则。</p><p>保证以上两个要求，可以通过特殊的锁：</p><ul><li>当事务执行 <em>write</em> 时，应当先获取数据所在 buffer block 的排它锁，当更新执行完后，立即释放锁。</li><li>当 <em>output</em> data buffer block 时，应该先获取 该 block 的排它锁，然后 <em>output</em> 与该 block 数据相关的所有 log records 到 stable storage 中，再然后 <em>output</em> 该 block 的数据到数据库磁盘文件中，最后释放该锁。</li></ul><p>可以看到，这里的锁持有的时间很短，一般将这种锁称为 latch.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>虽然写了这么多，但这些却是最简单的知识，还有很多高级的话题没写，等着以后学习深入了再来补充吧。</p>]]></content>
    
    
    <categories>
      
      <category>事务</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>事务篇一：事务总论</title>
    <link href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E6%80%BB%E8%AE%BA/"/>
    <url>/2022/06/22/%E4%BA%8B%E5%8A%A1%E6%80%BB%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文内容来自 database system concepts 第七版第十七章，主要是大概介绍事务的基本概念。更进一步的内容会引用其他文章，所以该文相当于一篇索引。</p><p><strong>单机事务部分（local transactions）：</strong></p><a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/" title="事务篇二：事务的可串行化">事务篇二：事务的可串行化</a> <br /> <a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" title="事务篇三：事务的并发控制">事务篇三：事务的并发控制</a> <br /><a href="/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/" title="事务篇四：Log-Based Recovery System">事务篇四：Log-Based Recovery System</a> <br /><br /><p><strong>分布式事务部分（global transactions）：</strong></p><a href="/2022/06/25/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%8D%8F%E8%AE%AE/" title="事务篇五：两阶段提交协议">事务篇五：两阶段提交协议</a> <br /><a href="/2022/06/18/Percolator-%E9%9A%8F%E7%AC%94/" title="事务篇六：Percolator 随笔">事务篇六：Percolator 随笔</a> <br /><br /><p><font color=red>分布式事务部分和单机事务部分的不同主要在于如何在多个节点保证事务的原子性。</font></p><h2 id="Transaction-Basic-Concept"><a href="#Transaction-Basic-Concept" class="headerlink" title="Transaction Basic Concept"></a>Transaction Basic Concept</h2><p>事务是一个逻辑单元，它包含了一组操作，这些操作可能访问或修改不同的数据。对用户来说，这组操作（事务）是一个单一的、不可分割的部分（比如，用户发起转账请求，这个请求对用户来说单一的操作，而在实际执行时分为多个步骤：安全性检查、出账、入账等），也就是说事务要么完成（其包含的所有步骤全部执行完成）或者失败（所有步骤皆失败），这个 all-or-none 特性被称为<font color=red>原子性（atomicity）</font>。</p><p>因为事务是一个不可分割的单元，所以它内含的步骤不能被数据库的其他操作分隔开。即使多个事务并发（concurrence），数据库系统也需要保证如此，即对于每两个事务 TA，TB，TA 要么开始于 TB 结束之后，要么结束于 TB 开始之前。这个特性被称为<font color=red>隔离性（isolation）</font>，后面会发现隔离性主要讲的是各个事务互相影响的程度，不同的隔离程度有不同的现象。</p><p>当事务执行完成提交后，它对数据库的修改不会丢失，即使系统从崩溃中恢复。这个特性被称为<font color=red>持久性（durability）</font>。</p><p>另外，事务还需要保证数据库的<font color=red>一致性（consistency）</font>，即，事务将系统从一个正确的状态迁移到另一个正确的状态，正确的状态是指数据库当前的状态满足预定的约束。约束可能由数据库底层提供（比如插入新数据时需要满足主键唯一约束）也可能来自上层应用（比如出账与入账要平衡），所以一致性的要求无法完全由数据库系统本身保证，它可能与上层逻辑有关。</p><p>以上四个特性被简称位 ACID，其中 AID 都为 C 服务。</p><h2 id="Storage-Structure"><a href="#Storage-Structure" class="headerlink" title="Storage Structure"></a>Storage Structure</h2><p>这里需要清楚一些存储的概念，比如易失性存储（volatile storage）、非易失性存储（non-volatile storage）、稳定性存储（stable storage）。</p><p><strong>主要是 stable storage 的概念</strong>：只要数据写入 stable storage，那么它们就“永远”不可能丢失，要实现这样的存储，需要将数据备份（replicate）到多个非易失性存储介质上，这些非易失性存储分别独立，有自己的错误处理模块。<a href="/2022/06/14/raft-%E6%9D%82%E8%AE%B0/" title="关于多从节点如何安全的备份，参考分布式一致性协议 raft">关于多从节点如何安全的备份，参考分布式一致性协议 raft</a>。</p><h2 id="Transaction-Atomicity-and-Durability"><a href="#Transaction-Atomicity-and-Durability" class="headerlink" title="Transaction Atomicity and Durability"></a>Transaction Atomicity and Durability</h2><p><strong>需要明白，原子性的挑战在哪里？</strong></p><p>事务并不总是成功执行。结束的事务（terminated txn）有两种可能：</p><ul><li>committed，事务成功完成了所有操作，所有更新都已经写入了数据库；</li><li>aborted，事务因各种原因失败，事务造成的修改都已经恢复（rolled back），数据库回到了事务开始之前的状态。</li></ul><p>对失败的事务造成的修改如何恢复，是保证原子性的一个难点。通常采用日志的方式实现（事务对数据库的每一个修改都先写入日志中），维护日志不仅可以重新事务的修改操作（保证原子性和持久性），还能在事务失败后撤销修改，以保证原子性。数据库的 recovery system 负责保证原子性和持久性，<a href="/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/" title="参见文章：数据库恢复系统">参见文章：数据库恢复系统</a>。</p><p>对于，原子性和持久化还有一个部分需要注意：外部可见更新（observable external writes），如更新显示到屏幕、发送邮件或者网上购物等场景。如果事务半途中断，这些更新难以撤回。一般的解决方法是，先将更新存储到数据库的某个地方，等到事务提交后，在将这些更新应用到外部，另外，如果系统在事务提交后，应用更新到外部之前崩溃，那么等到系统重启后依然可以应用更新到外部。</p><h2 id="Transaction-Isolation"><a href="#Transaction-Isolation" class="headerlink" title="Transaction Isolation"></a>Transaction Isolation</h2><p>SQL 标准将隔离级别（isolation level）分为四类：</p><p><strong>可串行化-serializable</strong>，即事务之间的执行顺序可串行化，其结果等价于串行执行，能够保证数据库的一致性。该隔离级别允许一定程度的并发，属于最高的隔离级别。一般，数据库为了提升性能，实现时不会完全遵循其标准。关于串行化的定义，可以参见文章：<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/" title="事务的串行化定义">事务的串行化定义</a>。</p><p><strong>可重复读-repeatable read</strong>，该级别规定了两点：1）只读已经提交了的数据；2）在事务执行期间多次读取一个数据的时间内，不允许其他任何事务更新该数据。该隔离级别保证了在<u>一个事务中，多次读取同一个数据总会得到同样的值</u>。但注意，这里只是规定了不允许更新已存在的数据，对于其他事务插入新的数据却未做规定，这就导致了，遵循该级别的事务在两次读取中，有可能第二次读取的数据中有一些不存在于第一次读取中的新数据（共同拥有的数据还是相同的），即幻读，若使用快照隔离提供稳定的视图，则不会出现幻读。</p><p><strong>读已提交-read committed</strong>，该级别比上一级别更弱，由上一级别的叙述可知，遵循该级别的事务在两次读取同一数据，这两次的数据可能不同，因为其他事务在这段时间内更新了该数据，即不可重复读。</p><p><strong>读未提交-read uncommitted</strong>，这是最低的级别了，该级别甚至允许一个事务读取另外一个事务的中间结果，即脏读。</p><p>解释下读已提交&#x2F;未提交中的提交的含义：提交是指事务的提交。假设有两个事务： TA 读取数据 S，TB 修改数据 S。读未提交允许 TA 读取被 TB 修改了的数据 S，尽管 TB 还未提交。这里存在的可能隐患是，若 TB 失败终止了，所有修改都会回滚，也就是说，TA 读取到了无效的值。</p><p>区分下幻读和不可重复读现象的区别：幻读是指，本次读出的数据中，有一部分在之前读取的结果中不存在，幻读存在于范围读取中；不可重复读是指，本次读出的数据和之前的值不相等。</p><p>总结下，各种隔离级别可能发生的<font color=red>现象</font>：</p><table><thead><tr><th align="center"></th><th align="center">脏读</th><th align="center">幻读</th><th align="center">不可重复读</th><th align="center">脏写</th></tr></thead><tbody><tr><td align="center">可串行化</td><td align="center">禁止</td><td align="center">禁止</td><td align="center">禁止</td><td align="center">禁止</td></tr><tr><td align="center">可重复读</td><td align="center">禁止</td><td align="center">？</td><td align="center">禁止</td><td align="center">禁止</td></tr><tr><td align="center">读已提交</td><td align="center">禁止</td><td align="center">允许</td><td align="center">允许</td><td align="center">禁止</td></tr><tr><td align="center">读未提交</td><td align="center">允许</td><td align="center">允许</td><td align="center">允许</td><td align="center">禁止</td></tr></tbody></table><h2 id="Implementation-of-Isolation-Level"><a href="#Implementation-of-Isolation-Level" class="headerlink" title="Implementation of Isolation Level"></a>Implementation of Isolation Level</h2><p>该部分内容广且复杂，在文章 <a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" title="事务的并发控制中有详细介绍。">事务的并发控制中有详细介绍。</a></p>]]></content>
    
    
    <categories>
      
      <category>事务</category>
      
      <category>数据库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>事务</tag>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>事务篇六：Percolator 随笔</title>
    <link href="/2022/06/18/Percolator-%E9%9A%8F%E7%AC%94/"/>
    <url>/2022/06/18/Percolator-%E9%9A%8F%E7%AC%94/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><a href="https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Peng.pdf">percolator 论文</a><br>建议先阅读事务并发 snapshot isolation 部分: <a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" title="事务篇三：事务的并发控制">事务篇三：事务的并发控制</a></p><p>percolator 这个系统是怎么产生的呢？论文中介绍了下背景，谷歌的爬虫应用会周期性的爬取新网页，同时更新网页的索引、排名等信息，这些更新操作往往并发，在庞大的数据之上做一些小的、独立的更改，而且修改跨越多个行、多个表。谷歌现有的系统中，MapReduce 也能完成相同的工作，但由于设计理念，MapReduce 需要扫描整个数据库才能完成这些更新；而且 BigTable 也只支持单行事务，不能提供上面所说的需求，所以就在 BigTable 之上构建了 percolator.</p><p>在极其庞大的数据集之上（谷歌是数十PB），每次高效地完成少量的更新被称为<strong>增量处理（incremental processing）</strong>，percolator 便是完成这样的工作。percolator 每次采用事务的方式执行这样的更新，总的来说它采用两阶段提交协议（是对传统两阶段协议的优化，见 <a href="/2022/06/25/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%8D%8F%E8%AE%AE/" title="事务篇五：两阶段提交协议">事务篇五：两阶段提交协议</a>）。</p><p><font color=red>percolator 包含很多的内容，本文主要集中在事务系统的设计，主要是快照隔离的实现</font>。</p><h2 id="事务系统的设计"><a href="#事务系统的设计" class="headerlink" title="事务系统的设计"></a>事务系统的设计</h2><p>snapshot isolation 有两个阶段：</p><ol><li><u>本地阶段</u>，读取在 [0, startTS] 时间内提交的最新的 snapshot，即开始时间戳之前提交的，并在事务的本地空间对数据进行操作。</li><li><u>提交阶段（两阶段提交）</u>，该阶段需要先验证是否有冲突产生，snapshot isolation 要求不能有 write-write conflict.</li></ol><p>要实现这两个阶段，不仅需要依据时间戳读取数据，还需要辨别出并发事务，percolator 是怎么做的呢？它通过记录一些列信息做到：</p><ul><li>data column. 实际的数据将记录在该列，格式一般为 $startTS(T_i) + data$.</li><li>lock column. 当某个数据处于提交阶段时，更新还不可见，所以记录 lock 信息在该列表示不可见，格式一般为 $startTS(T_i) + point\_to\_data$.</li><li>write column. 当写入一条 write 数据在该列时，表示某个数据已提交，其他事务可见，格式一般为 $commitTS(T_i) + point\_to\_data$.</li></ul><p>下面分别针对读和写两个操作介绍它们是如何工作的：</p><p><strong>当事务 $T_i$ 读取数据时</strong>，它先检查同一数据在 [0, startTS($T_i$)] 区间内是否存在 lock 记录，若存在，说明存在某个与 $T_i$ 并发的事务正在修改该数据，这时，$T_i$ 等待该事务完成。当锁不存在后，$T_i$ 就在 write column 中寻找 [0, startTS($T_i$)] 区间内同一数据最新的 write record，最后依据该记录读取到真正的数据。</p><p><font color=red>这里有一些想不明白，为何 $T_i$ 需要等？</font>该事务只能看到其开始之前提交的数据，很明显，上面占据写锁的事务所做的更新对 $T_i$ 是不可见的（提交在 $T_i$ 开始之后），既然如此，为何 $T_i$ 还必须等待呢？</p><br><p><strong>写操作相对复杂一些，因为它涉及提交过程</strong>。percolator 采用两阶段提交事务：<br><u>第一阶段称为 Prewrite.</u> 在协调节点提交事务 $T_i$ 前，先获得所有被修改数据的写锁（通过写入 lock 记录实现）。这其实相当于验证阶段，查找冲突是这一阶段的重要过程。冲突来自于与之并发的其他更新事务（假设为 $T_j$），存在两种冲突，如图片所示:<br><img src="/img/percolator/conflict.png" alt="冲突示意"></p><ol><li>并发的事务已完成提交。即提交时间（write record 的时间戳）在当前事务开始时间戳之后。在 percolator 中，若当前事务检测到一条待修改数据的 write 记录，其时间戳在当前事务开始时间戳之后，那么当前事务 abort.</li><li>并发的事务正在提交。即并发事务已经对数据加锁了，锁的时间戳为事务的开始时间戳。在 percolator 中，若当前事务检测到一条修待改数据的 lock 记录，不论其时间戳为多少，当前事务都 abort.</li></ol><p>对需要修改的每一个数据，当事务 $T_i$ 遇到这两种冲突时会 abort，即满足了 snapshot isolation 对避免 write-write conflict 的保证。如果没有冲突发生，事务 $T_i$ 会结合事务开始时间戳 startTS($T_i$) 分别将修改的数据（data）和锁记录（lock）写入相应的列（这是一个原子操作，通过 BigTable 的单行事务完成）。</p><p><u>待所有数据的 lock 和 data 记录写入后，将来到第二个阶段</u>. 在该阶段，事务首先获取一个提交时间戳 commitTS($T_i$)，然后对修改的每一个数据，移除其 lock 记录，同时写入 write 记录（同样是一个原子操作），这时，该更新就对外部可见了。这里要注意下，将 lock 记录改写成 write 前要先检查以下 lock 记录还在不在，原因见后面的 <a href="#percolator-%E5%AF%B9%E5%8F%AF%E7%94%A8%E6%80%A7%E7%9A%84%E8%80%83%E8%99%91">可用性考虑中的 roll forward</a>.</p><p>以一个例子阐述上述过程，Bob 和 Joe 账户初始分别有 10 美元和 2 美元，一个事务从 Bob 账户转 7 美元到 Joe 账户。下面几幅图展示了修改过程（加粗表示新写入的记录，x:data 表示 data 是在时间戳为 x 时写入的，primary 后面会提到）：</p><p><img src="/img/percolator/example_1.jpg" alt="example_a"><br><img src="/img/percolator/example_2.jpg" alt="example_b"><br><img src="/img/percolator/example_3.jpg" alt="example_c"><br><img src="/img/percolator/example_4.jpg" alt="example_d"><br><img src="/img/percolator/example_5.jpg" alt="example_e"></p><h2 id="percolator-对可用性的考虑"><a href="#percolator-对可用性的考虑" class="headerlink" title="percolator 对可用性的考虑"></a>percolator 对可用性的考虑</h2><p><font color=red>注意传统 2PC 的 blocking 问题在这里是如何解决的。</font></p><p>上面提到事务准备提交前会对数据加锁（写入 lock 记录），这些锁分布在不同的行、不同的表、甚至不同的节点。假如事务在提交过程中系统发生故障，只释放了部分的锁（将 lock 记录改写成 write 记录）或者只写入了部分的 lock 记录，那么就可能阻塞后面新的事务，如果加锁的是热点数据，那系统的可用性就会大大降低，甚至不可用。另外，如果由系统来贸然释放这些遗留的锁，则可能造成数据的不一致，因为事务可能只提交了部分更新。因此锁必须是可恢复的（保存在 stable storage），而且还必须判断这些锁能不能安全的释放。percolator 提出了如下的解决方法。</p><p>锁是通过 lock 记录写入 BigTable 中的，能提供 stable storage 的功能。同时，在获取写锁时，事务会指定一行（某个数据）的锁为主锁（primary，如 example_b），其他锁都会包含一个指针指向主锁。事务提交释放锁时，先从主锁开始释放所有锁。</p><p>若一新事务准备提交时发现自己欲修改的数据已经被加锁了，<strong>可以猜想该锁是系统崩溃遗留下来的</strong>，此时它有两种选择：</p><ul><li><u>帮助事务提交（roll forward）</u>。若持有该锁的事务在系统崩溃前已经提交了部分的更新，那么为维护数据的一致性，剩下的更新也应当被提交（其实数据已经写入 data 列了，只不过锁还在，新事务要做的不过是移除该锁，写入 write 记录，使该更新可见）。</li><li><u>回滚该事务（roll back）</u>。若持有该锁的事务在崩溃时还未提交，那么需要撤销事务所做的更新（删除写入 data 列的数据和 lock 记录）。</li></ul><p>听起来是那么回事，但有两个问题还需要解决：</p><ol><li>如何判断事务是否已提交了部分更新？</li><li>如何判断该锁真的是系统崩溃后遗留的？</li></ol><p><strong>对于第一个问题</strong>（percolator 论文中没有提到 log 文件），primary lock 发挥了重要作用。因为释放锁总是先释放 primary lock，若 primary lock 已经不存在了，说明至少部分更新已经对外可见了，因此该事务必须被完全提交，所以需要 roll forward. 若 primary lock 依旧存在，说明该事务还未提交，roll back 也不会产生什么副作用，<font color=red>所以该 primary lock 是一个同步点（synchronize point）</font>。因为这一点，当事务释放锁时，<strong>应当检查其是否还持有主锁</strong>，看看自己是否已经被 roll back 了。</p><p>如果锁不是崩溃后留下来的，贸然 roll back 则会影响系统的性能。percolator 中，是由一系列 worker 执行事务，<strong>对于第二个问题</strong>，所以当新事务遇到锁时，发现该锁属于不活跃的 worker（dead or stuck）时，才会做如上猜想，否则 abort 或者等待。如何判断 worker 不活跃呢？论文中原文如下（偷个懒，不想写了）：</p><blockquote><p>…So, a transaction will not clean up a lock unless it suspects that a lock belongs to a dead or stuck worker. Percolator uses simple mechanisms to determine the liveness of another transaction. Running workers write a token into the Chubby lockservice [8] to indicate they belong to the system; other workers can use the existence of this token as a sign that the worker is alive (the token is automatically deleted when the process exits). To handle a worker that is live, but not working, we additionally write the wall time into the lock; a lock that contains a too-old wall time will be cleaned up even if the worker’s liveness token is valid. To handle long- running commit operations, workers periodically update this wall time while committing.</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>论文中没有提到，似乎 percolator 不能避免 read-write conflict.</p><p>percolator 论文中还包含很多其他内容，比如通知服务、时间戳、相关工作等等。本文主要讨论其事务的实现。</p>]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式事务</tag>
      
      <tag>事务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>与数据库的初见</title>
    <link href="/2022/06/16/%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%88%9D%E8%A7%81/"/>
    <url>/2022/06/16/%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%88%9D%E8%A7%81/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>该篇文章是一篇索引目录，索引课程 CMU-15445 的笔记：</p><p><strong>基础内容：</strong></p><a href="/2022/07/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%88%86%E7%B1%BB/" title="数据库系统分类">数据库系统分类</a> <br /><a href="/2022/06/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%A3%81%E7%9B%98%E5%AD%98%E5%82%A8/" title="关系型数据库磁盘存储">关系型数据库磁盘存储</a> <br /><a href="#">Post not found: Lock Manager</a>  <br /><a href="/2022/07/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%86%B2%E6%B1%A0%EF%BC%88buffer-pool%EF%BC%89/" title="数据库缓冲池（buffer pool）">数据库缓冲池（buffer pool）</a>  <br /><br /><p><strong>索引部分：</strong></p><a href="/2022/07/04/%E7%B4%A2%E5%BC%95%E7%AF%87%E9%9B%B6%EF%BC%9A%E7%B4%A2%E5%BC%95%E6%80%BB%E8%AE%BA/" title="索引篇零：索引总论">索引篇零：索引总论</a>   <br /><a href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%80%EF%BC%9A%E5%93%88%E5%B8%8C%E7%B4%A2%E5%BC%95/" title="索引篇一：哈希索引">索引篇一：哈希索引</a>   <br /><a href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%BA%8C%EF%BC%9AB-plus%20Tree/" title="索引篇二：B-plus Tree">索引篇二：B-plus Tree</a> <br /><a href="/2022/06/30/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%89%EF%BC%9ALSM%20Tree/" title="索引篇三：LSM Tree">索引篇三：LSM Tree</a>    <br /><br /><p><strong>查询处理部分：</strong></p><a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E6%80%BB%E8%AE%BA/" title="查询处理篇：总论">查询处理篇：总论</a>    <br /><a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9ASorting/" title="查询处理篇：Sorting">查询处理篇：Sorting</a>    <br /><a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9AJoin%20%E6%93%8D%E4%BD%9C/" title="查询处理篇：Join 操作">查询处理篇：Join 操作</a>    <br /><a href="/2022/07/15/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9ASelection%20%E6%93%8D%E4%BD%9C/" title="查询处理篇：Selection 操作">查询处理篇：Selection 操作</a>    <br /><br /><p><strong>查询优化部分：</strong></p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>课程的主要结构如图所示：<br><img src="/img/CMU15445/1.png"></p><p>各级存储设备的访问时间如下：<br><img src="/img/CMU15445/2.png"></p>]]></content>
    
    
    <categories>
      
      <category>数据库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CMU-15445</tag>
      
      <tag>关系型数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TinyKV Snapshot 流程探秘</title>
    <link href="/2022/06/16/TinyKV-Snapshot-%E6%B5%81%E7%A8%8B/"/>
    <url>/2022/06/16/TinyKV-Snapshot-%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在写 TinyKV 时，有一个部分很难理解，那就 Snapshot 的收发过程。<br>为了防止内存中的日志条目无限扩张，Raft 会定时&#x2F;定量清理日志（如已经提交了的日志），一些节点可能由于是新加入或者网络等原因，其想要复制的日志已经被 leader 清理出内存了，此时，leader 会给该节点发送一份 Snapshot 使其快速跟上。在实现代码时，Raft 只是使用 Snapshot 的元数据来更新了一些状态，并没有涉及的日志的追加等操作，深感疑惑。而且，Snapshot 一般很大，虽然可以作为普通消息处理，但可能会阻塞正常的流程，所以对它的收发过程也很感兴趣。为了搞清楚这些问题，追踪代码调用，总算是搞清楚了。下面分为 Snapshot 的发送、接收、处理几个方面解密。</p><h2 id="Snapshot-流程总览"><a href="#Snapshot-流程总览" class="headerlink" title="Snapshot 流程总览"></a>Snapshot 流程总览</h2><p>这里先给出 snapshot 各个部分的流程示意图，下面会对各个部分详细分析<br><img src="/img/tinykv_snapshot/tinykv_arch.png" alt="TinyKV 的整体架构（代码层面）"><br><img src="/img/tinykv_snapshot/fig_for_create.png" alt="snapshot 创建流程"><br><img src="/img/tinykv_snapshot/fig_for_send.png" alt="snapshot 发送流程"><br><img src="/img/tinykv_snapshot/fig_for_recv.png" alt="snapshot 接收流程"><br><img src="/img/tinykv_snapshot/fig_for_apply.png" alt="snapshot 应用流程"></p><h2 id="Part-1：Snapshot-的创建"><a href="#Part-1：Snapshot-的创建" class="headerlink" title="Part 1：Snapshot 的创建"></a>Part 1：Snapshot 的创建</h2><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *Raft)</span></span> sendAppend(to <span class="hljs-type">uint64</span>) &#123;<br>    term, err := r.RaftLog.Term(r.Prs[to].Next - <span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// the peer left too far behind  (or newly join), </span><br>        <span class="hljs-comment">// send it a snapshot to catch-up</span><br>        r.trySendSnapshot(to)<br>        <span class="hljs-keyword">return</span><br>    &#125;<br>    <span class="hljs-comment">// something else</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *Raft)</span></span> trySendSnapshot(to <span class="hljs-type">uint64</span>) &#123;<br>    snapshot, err := r.RaftLog.storage.snapshot()<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span><br>    &#125;<br><br>    r.msgs = <span class="hljs-built_in">append</span>(r.msgs, SnapshotMessage&#123;...&#125;)<br><br>&#125;<br></code></pre></td></tr></table></figure><p>第一个函数表明了 Raft 发送 Snapshot 的时机，第二个函数表明了 Snapshot 来自 storage。</p><blockquote><p>这个 storage 在 2A部分和之后的部分是不一样的，值得分析下。在 2A 中，storage 接口由 MemoryStorage 实现，这货存在于内存中，文档中写着，放入 storage 的日志是持久化的（stabled），当时很不理解，因为它也是存在内存中的啊，做到后面才发现，这里的 MemoryStorage 主要起着测试的作用，你只需要闭只眼假装它真的持久化了就行。而在后面的部分，storage 接口由 badger.DB（engines.Raft）实现，是实打实的写入磁盘。</p></blockquote><p>那么，调用 storage.snapshot() 实际做了什么？</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(ps *PeerStorage)</span></span> Snapshot() (eraftpb.Snapshot, <span class="hljs-type">error</span>) &#123;<br>    <span class="hljs-keyword">var</span> snapshot eraftpb.Snapshot<br>    <span class="hljs-keyword">if</span> snapshot_is_generating &#123;<br>        snapshot &lt;- ps.snapState.Receiver<br>        <span class="hljs-keyword">return</span> snapshot, <span class="hljs-literal">nil</span><br>    &#125;<br><br>    <span class="hljs-comment">// something else</span><br><br>    ch := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> *eraftpb.Snapshot, <span class="hljs-number">1</span>)<br>ps.snapState = snap.SnapState&#123;<br>StateType: snap.SnapState_Generating,<br>Receiver:  ch,<br>&#125;<br><span class="hljs-comment">// schedule snapshot generate task</span><br>ps.regionSched &lt;- &amp;runner.RegionTaskGen&#123;<br>RegionId: ps.region.GetId(),<br>Notifier: ch,<br>&#125;<br><br>    <span class="hljs-keyword">return</span> snapshot, raft.ErrSnapshotTemporarilyUnavailable<br>&#125;<br><br></code></pre></td></tr></table></figure><p>代码很清楚，如果当前正在生成 snapshot，那么就等待它生成完成并返回，否则，就创建一个 RegionTaskGen 任务发送给 regionSched 通道，并返回暂时不可用错误。那么是谁在接收该任务呢？</p><p>上面说到 Raft 调用生成 snapshot 的接口，该接口的实现（PeerStorage）会创建一个 RegionTaskGen 任务发送给 regionSched 通道。该通道的消费者实际上是 regionWorker：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(w *Worker)</span></span> Start(handler TaskHandler) &#123;<br><span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br><span class="hljs-keyword">for</span> &#123;<br>Task := &lt;-w.receiver<br><span class="hljs-keyword">if</span> _, ok := Task.(TaskStop); ok &#123;<br><span class="hljs-keyword">return</span><br>&#125;<br>handler.Handle(Task)<br>&#125;<br>&#125;()<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *regionTaskHandler)</span></span> Handle(t worker.Task) &#123;<br><span class="hljs-keyword">switch</span> t.(<span class="hljs-keyword">type</span>) &#123;<br><span class="hljs-keyword">case</span> *RegionTaskGen:<br>task := t.(*RegionTaskGen)<br><span class="hljs-comment">// It is safe for now to handle generating and applying snapshot concurrently,</span><br><span class="hljs-comment">// but it may not when merge is implemented.</span><br>r.ctx.handleGen(task.RegionId, task.Notifier)<br><span class="hljs-keyword">case</span> *RegionTaskApply:<br>task := t.(*RegionTaskApply)<br>        <span class="hljs-comment">// apply received snapshot</span><br>r.ctx.handleApply(task.RegionId, task.Notifier, task.StartKey, task.EndKey, task.SnapMeta)<br><span class="hljs-keyword">case</span> *RegionTaskDestroy:<br>task := t.(*RegionTaskDestroy)<br>r.ctx.cleanUpRange(task.RegionId, task.StartKey, task.EndKey)<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>regionWorker 创建一个协程，处理接收到的各种任务。其中有两种任务是本文需要关注的：<br>1）RegionTaskGen（生成 snapshot）<br>2）RegionTaskApply（应用从其他 peer 接收来的 snapshot）</p><p>继续看 RegionTaskGen 任务是如何执行的：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(snapCtx *snapContext)</span></span> handleGen(...) &#123;<br>snap, err := doSnapshot(...)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>notifier &lt;- <span class="hljs-literal">nil</span><br>&#125; <span class="hljs-keyword">else</span> &#123;<br>        <span class="hljs-comment">// notify task done to task creator</span><br>notifier &lt;- snap<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">doSnapshot</span><span class="hljs-params">(...)</span></span> (*eraftpb.Snapshot, <span class="hljs-type">error</span>) &#123;<br>log.Debugf(<span class="hljs-string">&quot;begin to generate a snapshot. [regionId: %d]&quot;</span>, regionId)<br>    <span class="hljs-comment">// kvDB !!!</span><br>txn := engines.Kv.NewTransaction(<span class="hljs-literal">false</span>)<br>    <span class="hljs-comment">//...</span><br>err = s.Build(txn, ...)<br>    <span class="hljs-comment">//...</span><br><span class="hljs-keyword">return</span> snapshot, err<br>&#125;<br></code></pre></td></tr></table></figure><p>该 Build() 函数最终会调用 snapBuilder.build()，函数会扫描 PeerStorage.kvDB 的数据，创建一份快照：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(b *snapBuilder)</span></span> build() <span class="hljs-type">error</span> &#123;<br><span class="hljs-keyword">defer</span> b.txn.Discard()<br>startKey, endKey := b.region.StartKey, b.region.EndKey<br><br>    <span class="hljs-comment">// all data will store in b.cfFiles</span><br><span class="hljs-keyword">for</span> _, file := <span class="hljs-keyword">range</span> b.cfFiles &#123;<br>cf := file.CF<br>sstWriter := file.SstWriter<br><br>it := engine_util.NewCFIterator(cf, b.txn)<br><span class="hljs-keyword">for</span> it.Seek(startKey); it.Valid(); it.Next() &#123;<br>item := it.Item()<br>key := item.Key()<br><br>value, err := item.Value()<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br><br>cfKey := engine_util.KeyWithCF(cf, key)<br>            <span class="hljs-comment">// store data</span><br><span class="hljs-keyword">if</span> err := sstWriter.Add(cfKey, y.ValueStruct&#123;<br>Value: value,<br>&#125;); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br>&#125;<br>it.Close()<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>自此，Snapshot 的创建过程分析完成</p><h2 id="Part-2：Snapshot-的发送"><a href="#Part-2：Snapshot-的发送" class="headerlink" title="Part 2：Snapshot 的发送"></a>Part 2：Snapshot 的发送</h2><p>当 sendAppend() 函数获取到创建的 snapshot 后，会将其封装在 pb.MessageType_MsgSnapshot 消息中，等待 RaftStorage 层调用 rawNode.Ready()：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(d *peerMsgHandler)</span></span> HandleRaftReady() &#123;<br><span class="hljs-comment">// Your Code Here (2B).</span><br>rd := d.RaftGroup.Ready()<br>d.sendMessageToPeers(rd.Messages)<br>    <span class="hljs-comment">// ...</span><br>d.RaftGroup.Advance(rd)<br>&#125;<br></code></pre></td></tr></table></figure><p>sendMessageToPeers() 最终会调用 WirteData() 函数：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(t *ServerTransport)</span></span> WriteData(...) &#123;<br><span class="hljs-keyword">if</span> msg.GetMessage().GetSnapshot() != <span class="hljs-literal">nil</span> &#123;<br>t.SendSnapshotSock(addr, msg)<br><span class="hljs-keyword">return</span><br>&#125;<br><span class="hljs-keyword">if</span> err := t.raftClient.Send(storeID, addr, msg); err != <span class="hljs-literal">nil</span> &#123;<br>log.Errorf(<span class="hljs-string">&quot;send raft msg err. err: %v&quot;</span>, err)<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到，在 WriteData 中对 snapshot 消息做了一个拦截，采用另外的方式单独处理：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(t *ServerTransport)</span></span> SendSnapshotSock(...) &#123;<br>t.snapScheduler &lt;- &amp;sendSnapTask&#123;<br>addr:     addr,<br>msg:      msg,<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>这下明白了，由于 snapshot 比较大，会采用分块传输，对它的发送操作与普通的消息分开，由 sendSnapTask 异步完成。<br>继续探寻该任务是如何被执行的，该任务被 snapWorker 接收，并调用 Handle() 处理：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *snapRunner)</span></span> Handle(t worker.Task) &#123;<br><span class="hljs-keyword">switch</span> t.(<span class="hljs-keyword">type</span>) &#123;<br><span class="hljs-keyword">case</span> *sendSnapTask:<br>r.send(t.(*sendSnapTask))<br><span class="hljs-keyword">case</span> *recvSnapTask:<br>r.recv(t.(*recvSnapTask))<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *snapRunner)</span></span> send(t *sendSnapTask) &#123;<br>t.callback(r.sendSnap(t.addr, t.msg))<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *snapRunner)</span></span> sendSnap(...) <span class="hljs-type">error</span> &#123;<br>    <span class="hljs-comment">// ...</span><br>buf := <span class="hljs-built_in">make</span>([]<span class="hljs-type">byte</span>, snapChunkLen) <span class="hljs-comment">// snapChunkLen = 1024 * 1024</span><br><span class="hljs-keyword">for</span> remain := snap.TotalSize(); remain &gt; <span class="hljs-number">0</span>; remain -= <span class="hljs-type">uint64</span>(<span class="hljs-built_in">len</span>(buf)) &#123;<br>_, err := io.ReadFull(snap, buf)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> errors.Errorf(<span class="hljs-string">&quot;failed to read snapshot chunk: %v&quot;</span>, err)<br>&#125;<br>err = stream.Send(&amp;raft_serverpb.SnapshotChunk&#123;Data: buf&#125;)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br>&#125;<br>    <span class="hljs-comment">// ...</span><br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到最终 snapshot 以 snapChunkLen 为单位分块发送出去的，后面的事情就是 gRPC 的工作了，探秘自此结束。 </p><h2 id="Part-3：Snapshot-的接收"><a href="#Part-3：Snapshot-的接收" class="headerlink" title="Part 3：Snapshot 的接收"></a>Part 3：Snapshot 的接收</h2><p>当使用 gRPC 发送 snapshot 时，对应 peer 也就进入了接收流程。上面提到的 snapWorker 也会处理接收操作，这里不再赘述。当所有的 snapshot 分块都接受完成后，就会给 raftWorker 监听的管道发送消息，最后调用 rawNode.Step() 让 raft 调用 handleSnapshot() 处理。 </p><h2 id="Part-4：应用来自其他-Peer-的-Snapshot"><a href="#Part-4：应用来自其他-Peer-的-Snapshot" class="headerlink" title="Part 4：应用来自其他 Peer 的 Snapshot"></a>Part 4：应用来自其他 Peer 的 Snapshot</h2><p>handleSnapshot() 接收到 snapshot 后只是更新了一些元数据，并将 snapshot 赋值给 pendingSnapshot，等待上层调用 Ready() 获取 pendingSnapshot：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(d *peerMsgHandler)</span></span> HandleRaftReady() &#123;<br><span class="hljs-comment">// Your Code Here (2B).</span><br>rd := d.RaftGroup.Ready()<br>applySnapResult, _ := d.peerStorage.SaveReadyState(&amp;rd)<br><span class="hljs-comment">//...</span><br>d.RaftGroup.Advance(rd)<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(ps *PeerStorage)</span></span> SaveReadyState(ready *raft.Ready) &#123;<br><span class="hljs-comment">// Your Code Here (2B/2C).</span><br>applySnapResult, err := ps.ApplySnapshot(&amp;ready.Snapshot, ...)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-built_in">panic</span>(err)<br>&#125;<br>    <span class="hljs-comment">// ...</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(ps *PeerStorage)</span></span> ApplySnapshot(snapshot *eraftpb.Snapshot, ...) &#123;<br>    <span class="hljs-comment">// ...</span><br><span class="hljs-comment">// send runner.RegionTaskApply task to region worker through </span><br>    <span class="hljs-comment">// PeerStorage.regionSched and</span><br><span class="hljs-comment">// wait until region worker finishes</span><br>ch := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> <span class="hljs-type">bool</span>, <span class="hljs-number">1</span>)<br>ps.regionSched &lt;- &amp;runner.RegionTaskApply&#123;<br>RegionId: snapData.Region.GetId(),<br>Notifier: ch,<br>SnapMeta: snapshot.Metadata,<br>StartKey: snapData.Region.StartKey,<br>EndKey:   snapData.Region.EndKey,<br>&#125;<br><br><span class="hljs-comment">// waiting</span><br>&lt;-ch<br>    <span class="hljs-keyword">return</span> ...<br>&#125;<br></code></pre></td></tr></table></figure><p>上面代码很明显了，上层获取到 Ready.Snapshot 后，会创建 RegionTaskApply 任务通过 regionSched 通道发送给 RegionWorker -&gt; handle() -&gt; handleApply() 处理：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *regionTaskHandler)</span></span> Handle(t worker.Task) &#123;<br><span class="hljs-keyword">switch</span> t.(<span class="hljs-keyword">type</span>) &#123;<br><span class="hljs-keyword">case</span> *RegionTaskGen:<br>task := t.(*RegionTaskGen)<br><span class="hljs-comment">// It is safe for now to handle generating and applying snapshot concurrently,</span><br><span class="hljs-comment">// but it may not when merge is implemented.</span><br>r.ctx.handleGen(task.RegionId, task.Notifier)<br><span class="hljs-keyword">case</span> *RegionTaskApply:<br>task := t.(*RegionTaskApply)<br>r.ctx.handleApply(task.RegionId, task.Notifier, task.StartKey, task.EndKey, task.SnapMeta)<br><span class="hljs-keyword">case</span> *RegionTaskDestroy:<br>task := t.(*RegionTaskDestroy)<br>r.ctx.cleanUpRange(task.RegionId, task.StartKey, task.EndKey)<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(snapCtx *snapContext)</span></span> handleApply(...) &#123;<br>err := snapCtx.applySnap(regionId, startKey, endKey, snapMeta)<br><span class="hljs-comment">// ...</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(snapCtx *snapContext)</span></span> applySnap(...) &#123;<br>applyOptions := snap.NewApplyOptions(snapCtx.engines.Kv, &amp;metapb.Region&#123;<br>Id:       regionId,<br>StartKey: startKey,<br>EndKey:   endKey,<br>&#125;)<br><span class="hljs-keyword">if</span> err := snapshot.Apply(*applyOptions); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到，上面一步步调用，最后调用 snapshot.Apply()，注意这里传入的是 badger.kvDB。<br>snapshot.Apply() 和上面提到的 snapshot 创建过程的 snapBuilder.build() 执行的是相反的步骤，即，将 snapshot 中的内容写入到磁盘:</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(s *Snap)</span></span> Apply(opts ApplyOptions) <span class="hljs-type">error</span> &#123;<br>externalFiles := <span class="hljs-built_in">make</span>([]*os.File, <span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(s.CFFiles))<br><span class="hljs-keyword">for</span> _, cfFile := <span class="hljs-keyword">range</span> s.CFFiles &#123;<br>file, err := os.Open(cfFile.Path)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>log.Errorf(<span class="hljs-string">&quot;open ingest file %s failed: %s&quot;</span>, cfFile.Path, err)<br><span class="hljs-keyword">return</span> err<br>&#125;<br>externalFiles = <span class="hljs-built_in">append</span>(externalFiles, file)<br>&#125;<br>    <span class="hljs-comment">// write to DB</span><br>n, err := opts.DB.IngestExternalFiles(externalFiles)<br>    <span class="hljs-comment">// ...</span><br>&#125;<br></code></pre></td></tr></table></figure><p>snapshot 的应用分析自此结束。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过上面的分析，可以得到以下信息：</p><ol><li>snapshot 的创建、发送、接收和处理都与 Raft 无关，它无需关系具体数据（除了元数据）。</li><li>snapshot 的发送和接收都采取了单独的 RPC 异步处理。</li><li>生成 snapshot 需要从 kvDB 中读取数据，然后返回给 Raft，最后通过 Ready 交给上层发送。</li><li>snapshot 接收后，需要先交给 Raft 更新一些元数据，然后通过 Ready 交给上层写到 kvDB 中。</li></ol>]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>数据库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TinyKV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Raft 算法问答录</title>
    <link href="/2022/06/14/raft-%E6%9D%82%E8%AE%B0/"/>
    <url>/2022/06/14/raft-%E6%9D%82%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前段时间学习了 CMU-15445 的课程，也写完了 project，了解了数据库内核的基本知识。这段时间在做 TinyKV，刚好看了 raft，细节很多，所以来总结下。</p><p>关于 raft 网上有很多资料：<br><a href="https://raft.github.io/raft.pdf">raft 小论文</a><br><a href="http://files.catwell.info/misc/mirror/2014-ongaro-raft-phd.pdf">raft 博士论文</a><br><a href="https://tanxinyu.work/raft/">raft 博客</a><br><a href="https://www.codedump.info/post/20180922-etcd-raft/">etcd raft 实现</a></p><p>所以，这里并不是对 raft 算法本身的细节记录（可能存在部分），而是自己阅读、实现时的一些疑问和解答。</p><p><u>看完 raft 小论文后，强烈推荐阅读 raft 博士论文（虽然 258 页）</u>，因为它会结合工业实现阐述遇到的问题和解决方式，里面包含了很多细微的点，是一份能大致了解分布式系统的资料。本文绝大多数内容来自这篇博士论文。</p><h2 id="Q0：raft-算法有哪些优化？"><a href="#Q0：raft-算法有哪些优化？" class="headerlink" title="Q0：raft 算法有哪些优化？"></a>Q0：raft 算法有哪些优化？</h2><ol><li>PreVote 见 Q3.</li><li>Lease 见 Q6.</li><li>Read-Only 见 Q9.</li><li>raft 活性考虑 见 Q10.</li><li>批处理和流水线优化 见 Q11.</li><li>高级的 multiRaft 见 Q12.</li></ol><h2 id="Q1：raft-算法解决了什么问题？"><a href="#Q1：raft-算法解决了什么问题？" class="headerlink" title="Q1：raft 算法解决了什么问题？"></a>Q1：raft 算法解决了什么问题？</h2><p>raft 是一个分布式共识协议（算法），其主要作用是让集群中的节点对某件事情达成一致，如客户端发起更新请求，为了保证多个节点的数据状态一致，就需要让该更新请求在所有节点上都应用成功，否则更新请求失败。raft 算法可以看作一个黑匣子，当某个节点接收到客户端的请求后，首先将该请求交给 raft 模块，由 raft 模块负责节点间的协商，最后将结果返回给节点，节点再反馈客户端。如下图的复制状态机所示。</p><blockquote><p>分布式系统的共识算法会将数据的写入复制到多个副本，从而在网络隔离或节点失败的时候仍然提供可用性。</p></blockquote><p><img src="/img/raft/rsm.png"></p><p>图中的 consensus module 就相当于 raft 黑匣子，state machine 可以理解为 kv 键值数据库。还需要注意两点：1）日志记录和协商同步进行；2）图中是分层的，表示多个客户端和集群节点。</p><h2 id="Q2：在实际实现中，整个分布式系统的流程如何？"><a href="#Q2：在实际实现中，整个分布式系统的流程如何？" class="headerlink" title="Q2：在实际实现中，整个分布式系统的流程如何？"></a>Q2：在实际实现中，整个分布式系统的流程如何？</h2><p>这个问题实际是关于</p><ul><li>集群节点如何与客户端交互？</li><li>集群节点如何与 raft 模块交互？</li><li>raft 模块如何与其他 raft 模块交互？</li></ul><p>要解答这些问题，需要借助现有的成熟的工业系统，比如 etcd、tikv 等等，因为比较熟悉 TinyKV，所以以它为例。</p><p>TinyKV 的设计参照了 etcd，将 raft 模块设计成独立的部分，raft 需要的网络、存储服务由上层（非 raft)提供，比较具有灵活性，具体流程如下：</p><ul><li>客户端向节点发送请求（commands：put&#x2F;get&#x2F;delete…)</li><li>节点准备 WAL(write-ahead-log)</li><li>节点将 commands 封装成 entry 发送给 raft 模块，开始执行共识协议。</li><li>某个时间点，节点获取 raft 算法的输出，主要有以下部分：<ul><li>raft 需要存储的日志记录（unstable entry 和一些 raft 自身状态信息）</li><li>committed entry（已经在大多数节点间达成一致的 entry）</li><li>messages，需要发往其他 raft 模块的消息</li></ul></li><li>当上层模块获取到 raft 的输出后，按其类型做一些操作，对于 unstable entry 和状态信息，执行持久化操作；对于 committed entry，它们已处于安全状态，可以应用其中的 commands 于数据库了，并且可以就这些 commands（请求）向客户端反馈成功；对于messages，将其发送到对应的 raft 模块。完成以上操作后，通知 raft 模块。</li></ul><p><u>为什么 raft 会存储 unstable entry 和状态信息，比如 peers，是为了从崩溃中恢复</u></p><p><strong>总结一下</strong>，raft 模块被独立实现，其算法输入来自上层（这里的输入可能是客户端请求，也可能是其他 raft 模块的消息），其算法输出由上层负责处理（存储、发送等），从这里也能知道，raft 根本不关心 entry 中的具体请求，那是上层逻辑的责任，它只需要采取办法能够唯一标识一条 entry 即可（Term、Index）。</p><h2 id="Q3：当网络分区发生时，raft算法有什么表现？"><a href="#Q3：当网络分区发生时，raft算法有什么表现？" class="headerlink" title="Q3：当网络分区发生时，raft算法有什么表现？"></a>Q3：当网络分区发生时，raft算法有什么表现？</h2><p>该问题比较大，需要分类讨论：</p><ul><li>单个 follower 节点被隔离，恢复后，会发生什么？</li><li>网络分区发生时，leader 处在少数部分，恢复后会发生什么？</li><li>网络分区发生时，leader 处在多数部分，恢复后会发生什么？</li></ul><p><font color=red>对于第一个问题</font>，先看被隔离节点的表现：因为是 follower，它只能被动应答，在一段时间内没有异常发生。等到 election_timeout 后，它自增 Term，发起选举请求，由于网络问题，其他节点接收不到该请求，然后再次等到 election_timeout，再次自增 Term，发起选举请求……它重复该操作，直到从隔离中恢复。</p><p>该节点（记为 A）从隔离中恢复后，可能会先收到 leader 发来的 AppenEntriesRPC 或者 HeartBeatRPC，但 leader.Term &lt; A.Term，根据算法，这些 RPC 对 A 没有影响，但 leader 会受到影响（response 中的 Term 比 leader 的 Term 大），leader 会转变为 follower。集群中先超时的节点会率先发起选举请求，由于存在选举限制：<strong>要获取到大多数的选票，就必须具有最新的日志记录</strong>：</p><blockquote><p>Raft determines which of two logs is more up-to-date by comparing the index and term of <u>the last entries in the logs</u>. If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the logs end with the same term, then whichever log is longer is more up-to-date.</p></blockquote><p>这样的选举可能会持续多次，但无论如何节点 A 都不可能当选 leader，因为它被隔离，没有后面新追加的日志。也就是说，节点 A 的重新加入造成了系统不必要的抖动，其原因在于，节点 A 在隔离期间盲目地自增 Term。</p><p>etcd 是如何解决该问题的呢？采用 PreVote 机制，即当一个节点超时后，它并不急于自增 Term，而是先发起选举请求，如果能获取到大多数的选票，再自增 Term 重新发起选举。这样，当 follower 从隔离中恢复后，就不会因为 Term 过大干扰集群的正常流程。下面是 Raft 博士论文中的叙述：</p><blockquote><p>The Pre-Vote algorithm solves the issue of a partitioned server disrupting the cluster when it rejoins.</p></blockquote><p><u>然而，还有这样一种情况</u>，当 A 从隔离中恢复后，由于其未收到 heartbeat 超时，发起选举，虽然其 Term 没有增加（PreVote 的限制），但是它在大多数节点中仍然具有较新的日志（可能是隔离期间没有新的请求，也可能是隔离时间比较短，新的日志还没有复制到大多数的节点上），依然可以当选 leader。集群当然可以正常工作，但旧 leader 上还未来得及复制的新日志就会被覆盖，客户端也就需要超时重试，这实际上也算一个干扰，该问题的解决方式会在 Q6 中问题三提到（租约期-Lease）。</p><br><p><font color=red>对于第二个问题</font>，leader 处在少数节点分区部分，根据 raft 要求，一条 entry 能被提交，该entry 至少需要被 N&#x2F;2 + 1 个节点安全复制，因此上层交付的任何 proposal 都无法被提交，自然无法被应用到数据库和反馈客户端，客户端会出现请求超时。后面的客户端请求可能会被路由到另外一个节点，直到请求能够被正常执行。</p><p>当网络分区恢复后，该 leader 会接受来自新 leader 的 RPC 请求，转换成 follower，开始正常的日志复制。该种情况下，是否会出现第一个问题中的场景呢？是有可能的，比如四个节点，每个分区中存在两个节点，包含 leader 的分区不会触发新的选举，但另外一个分区会发起多次的选举（或预选举），<strong>这种情况下，整个系统瘫痪，无法对外服务</strong>。</p><br><p><font color=red>对于第三个问题</font>，这种情况相对较简单，系统可正常对外服务，少数分区可能存在多次选举，但分区恢复后，可以开始正常的日志复制，具体过程在前两个问题中已经提及。</p><h2 id="Q4：leader-commit-日志之前崩溃了，会发生什么？"><a href="#Q4：leader-commit-日志之前崩溃了，会发生什么？" class="headerlink" title="Q4：leader commit 日志之前崩溃了，会发生什么？"></a>Q4：leader commit 日志之前崩溃了，会发生什么？</h2><p>该问题在 raft 论文中有论述，是关于如何处理前任 leader 复制的日志。<strong>我当时的疑问是</strong>：leader 将最新的日志复制到了一部分节点后，或许是还未满足大多数原则，或许是 commit 之前就崩溃了，这些最新的日志会被怎么处理？raft 协议有一个规定：</p><blockquote><p>一个 leader 只能提交当前任期的日志，不能提交之前任何任期的日志。当 leader 提交一条当前任期的记录时，之前的所有日志记录都会被提交（被动）。</p></blockquote><p><font color=red>下面先来看一下，遵守该规则时，raft 协议的表现</font>：</p><p><img src="/img/raft/Q4_1.png"></p><p>图中方块中的数字标识 Term，上方的数字标识 Index。</p><ul><li><p>(a)中，S1 为 Term2 的 leader，在将 Index&#x3D;2 的日志复制到 S2 后崩溃。</p></li><li><p>(b)中，S1 崩溃后，S5 在 Term3 当选 leader（S5 获得 S3、S4 和 S5 的选票），并追加了一条日志。</p></li><li><p>(c)中，S5 崩溃，S1 在 Term4 当选 leader，并继续复制日志，此时它将 Index&#x3D;2 的日志成功复制到了大多数节点上，但还未提交。</p></li><li><p>(d1) 和 (d2) 描述两种情况：<br>  第一种是(d1)，以前任期复制的日志（未提交）被后面新的日志覆盖。客户端等待响应超时，会重新发起请求（我之前还在担心，这会不会造成数据丢失，太天真了）。对应的是 S1 再次崩溃，在 (c) 的局面下，S5 再次当选 leader（图中未画出新的任期，S5 可以获得 S2-4 的选票），由于复制日志以 leader 的日志为准，所以 Index&#x3D;2 的以前任期的日志会被 S5 的 Index&#x3D;2 的日志覆盖。</p><p>  第二种是(d2)，以前任期复制的日志可以被后面新的 leader 提交（属于被动提交，因为 raft 中，提交一条日志，就表示该条日志之前的所有日志都已被提交）。对应的是，S1 在崩溃之前将日志复制到了大多数节点上，此时 S5 已经不可能再当选，新的 leader 只能在 S1-3 之中。假设，S1 未崩溃，那么，S1 通过提交 Index&#x3D;3 的日志，之前的日志也就一起被动提交了；就算 S1 在提交之前崩溃了，新的 leader 通过提交当前任期的日志也能提交以前任期的所有日志。</p></li></ul><p><strong>从这里，应当认识到两点</strong>：</p><ul><li>复制的日志可能会被覆盖，客户端会重试。</li><li>raft 算法中，leader 会强制要求其他节点的日志与自己一致，对安全性的考虑应该结合选举限制一起理解。</li></ul><p><font color=red>那如果不遵守该协议，raft 表现又是怎么样的呢？</font></p><ul><li>从 (b) 开始可以以另外一种方式解读，假设 S5 在当 Term &#x3D; 3 当选后，向日志中追加了一条 Index &#x3D; 2 的记录后就崩溃了</li><li>(c) 中，S1 在 Term &#x3D; 4 成功当选（先不看 Index &#x3D; 3 的粉红色方块，假设它不存在），然后继续复制本地 Index &#x3D; 2，Term &#x3D; 2 的记录，最终复制到了大多数节点上（S1，S2，S3），然后提交该记录（注意该条记录不属于 S1 的当前任期）。</li><li>提交后 S1 就挂了，这时来到了 (d1)，S5 恢复后成功当选（日志一样长，但 Term 更大），它继续复制自己本地 Index &#x3D; 2 的日志记录，其他节点的日志当然会与该记录发生冲突，然而 raft 复制日志是以 leader 为准，<u>所以 Index &#x3D; 2 的记录会被覆盖，尽管它们已经被提交了</u>。同一个 Index 的记录可被提交了两次，一次 Term &#x3D; 2，一次 Term &#x3D; 3.</li><li>(d2) 自然描绘的是正常的情况，S1 没挂，并且还有新的请求（Term &#x3D; 4，Index &#x3D; 3）到达，它们都被复制到了大多数节点。</li></ul><p>也就是说，如果一个 leader 主动提交了不属于当前任期的日志记录，那么已经提交的记录依然可能被覆盖，raft 是禁止覆盖已提交的日志的，如果覆盖已提交的日志会造成数据丢失。</p><p><u>现在进一步分析</u>，当新 leader 当选后，以前任期的记录会在有当前任期记录提交时被动提交，那如果迟迟没有新的请求到来，以前任期的记录也就长时间不能被提交，为了能够快速提交以前任期的记录，raft 协议要求每个新 leader 当选后都要向日志中追加一条 no-op entry（如 (c) 中 Index &#x3D; 3，Term &#x3D; 4 的粉红色方块），然后复制给其他节点，最后提交它。</p><p><font color=red>leader 选举成功后会先提交一条 no-op 日志是非常重要的内容</font>，除了上述提到的问题，还涉及到很多其他方面，总结如下：</p><blockquote><ul><li>如果没有这一步，已经存在于大多数节点的以前任期的记录不能及时地被提交，那么就可能增加客户端地响应时间，同时，也面临被新 leader 覆盖地的风险。</li><li>Q9 提到的 read-only 优化需要。</li><li>防止单步成员变更过程中出现脑裂，见 Q5.</li></ul></blockquote><h2 id="Q5：Raft-成员变更过程如何理解？"><a href="#Q5：Raft-成员变更过程如何理解？" class="headerlink" title="Q5：Raft 成员变更过程如何理解？"></a>Q5：Raft 成员变更过程如何理解？</h2><p><font color=dark-green>Raft 成员变更（Cluster Membership Change）</font><br>raft 集群中的每一个节点都会记录集群节点的信息，即 peer 配置，如果节点发起选举或是 leader 复制日志都需要该配置，以便将请求发送给其他节点。<u>成员变更本质上就是 peer 配置的变化</u>：向集群中新增节点，就是向 peer 配置中新增节点信息；从集群中移除节点，就是从 peer 配置中删除节点信息。</p><p>raft 博士毕业论文中设计了两种算法来处理成员变更：</p><ul><li>方法一：一次变更只包括一个节点加入集群或从集群中移除；</li><li>方法二：一次变更包括多个节点加入集群或从集群中移除。</li></ul><p>由于对安全性的考虑，第二种方法会引入额外的复杂度，不如第一种简单，考虑如下情况（见下图）：<br><img src="/img/raft/Q5-1.png"><br>当集群中成员发生变更时，该变更不可能同时应用在所以节点上，上图集群中有 3 个节点，现新增 2 个，在变更的过程中，有些节点更新了自己的 peer 配置，感知到的是 5 个节点，如 server 3，而其他节点可能还未得到更新，记录的仍是 3 个节点，如 server 1 和 server 2。<u>如果此时发生选举，就可能会选出两个 leader</u>：</p><ul><li>server 3：获得 server 3、4、5 的选票。</li><li>server 1：获得 server 1、2 的选票。</li></ul><p>由于 server 1 的 peer 配置还未更新，认为集群中只有 3 个节点，那么它已经得到大多数的选票了，可以当选为 leader；而 server 3，根据它的 peer 配置同样也得到了大多数的选票，可以当选为 leader. 同时移除多个节点也会导致该问题。也就是说一次涉及多个节点加入或移除的变更是不安全的，不能保证唯一的 leader。</p><p>为了解决这个问题（会产生两个大多数群体），第二种方法会引入一个过渡状态，被称为 joint consensus（共同一致），有时间再讨论这种方法。<font color=red>本节只讨论第一种方法</font>。</p><p><u>为什么一次变更只包含一个节点的情况（方法一）不会引发上述问题呢？</u>考虑如下场景：<br><img src="/img/raft/Q5-2.png"></p><p><strong>如上图 case 1 所示（case 2 类似）</strong>，集群中刚开始有 A、B、C 三个节点，$T_0$ 时刻节点 D 加入了该集群。</p><p>$T_1$ 时刻，节点 C 感知到新节点的加入，更新了 peer 配置，此时，集群分为了两部分：更新了 peer 的节点（C、D）和未更新 peer 的节点（A、B），但这并不会将集群分裂成两个大多数群体，C 或 D 要想当选 leader，至少需要 3 票（包含自身选票），A 或 B 要想当选，至少需要获得 2 两票（包含自身选票），但这两种情况不可能同时发生，前者的 3 票与后者的 2 票存在重合。</p><p>$T_2$ 时刻，B 节点也感知到了新节点的加入，更新了 peer 配置，此时集群中依然存在两个部分：更新了 peer 的节点（B、C、D）和未更新 peer 的节点（A），这种情况下同样不会产生两个 leader.</p><p>$T_3$ 时刻，A 节点也感知到了新节点，至此整个集群都完成了 peer 配置的更新。</p><p><strong>再来看 case 4 所示（case 3 类似）</strong>，集群中刚开始有 A、B、C、D 四个节点，$T_0$ 时刻节点 D 被移除了该集群。</p><p>$T_1$ 时刻，节点 C 感知到了节点的移除，更新了 peer 配置，此时集群分为两部分：更新了 peer 配置的节点（C）和未更新 peer 配置的节点（A、B），同样，如果此时需要选举 leader，不会产生两个，若 C 想要当选 leader，至少需要 2 票（包含自身选票），若 A 或 B 想要当选，至少需要 3 票（包含自身选票），前者的 2 票与后者的 3 票存在重合，不可能同时满足。</p><p>$T_2$ 时刻，节点 B 感知到了节点的移除，更新了 peer 配置，虽然集群中还是存在两个部分，同样也不会产生两个 leader.</p><p>$T_3$ 时刻，节点 A 也感知到了节点的移除，至此整个集群都完成了 peer 配置的更新。</p><p><font color=red>因此，若一次变更只包含单个节点的加入或删除，那么不会造成集群的分裂，这种情况下变更是安全的。</font></p><br><p><strong>如果存在多个节点需要加入或删除怎么办？</strong></p><p>论文指出，对于这种情况，可以一次一个节点地变更。由上面的讨论可知，若一次变更只涉及一个节点，那么当其他节点感知到变更时，就可以更新并应用新的 peer 配置，并且不会产生安全性问题。下面是论文描述的单个节点的变更过程：</p><blockquote><ul><li>leader 接收到成员变更请求（一种特殊的 entry：$C_{new}$）后，将该 entry 存在日志中，并应用该配置，同时将 $C_{new}$ 复制给其他 follower 节点。</li><li>follower 节点收到 $C_{new}$ 后，同样存入日志中，并应用该新配置。</li><li>当一个节点（candidate 或 leader）需要给其他所有节点发送请求时，使用的是从日志中检索到的最新的配置。</li><li>当 $C_{new}$ 在所有节点中达成一致后被提交，此次变更就完成了，leader 反馈变更成功，可以开始下一次的变更。</li></ul></blockquote><p>上述过程其实和普通的 raft 过程没什么区别，主要的不同在与配置是即时生效的，<font color=red>但单步成员变更存在一个问题</font>，<a href="https://zhuanlan.zhihu.com/p/359206808">如下例子</a>：</p><blockquote><p><img src="/img/raft/Q5-3.png"><br>$t_0$：节点 abcd 的成员配置为 $C_0$；<br>$t_1$：节点 abcd 在 Term 0 选出 a 为 Leader，b 和 c 为 Follower；<br>$t_2$：节点 a 同步成员变更日志 $C_u$，只同步到 a 和 u，未成功提交；<br>$t_3$：节点 a 宕机；<br>$t_4$：节点 d 在 Term 1 被选为 Leader，b 和 c 为 Follower；<br>$t_5$：节点 d 同步成员变更日志 $C_v$，同步到 c、d、v，成功提交；<br>$t_6$：节点 d 同步普通日志 E，同步到 c、d、v，成功提交；<br>$t_7$：节点 d 宕机；<br>$t_8$：节点 a 在 Term 2 重新选为 Leader，u 和 b 为 Follower;<br>$t_9$：节点 a 同步本地的日志 $C_u$ 给所有人，造成已提交的 $C_v$ 和 E 丢失。</p></blockquote><blockquote><p>为什么会出现这样的问题呢？根本原因是上一任 Leader 的成员变更日志还没有同步到多数派就宕机了，新 Leader 一上任就进行成员变更，使用新的成员配置提交日志，之前上一任 Leader 重新上任之后可能形成另外一个多数派集合，产生脑裂，将已提交的日志覆盖，造成数据丢失。</p></blockquote><blockquote><p>Raft 作者在发现这个问题之后，也给出了修复方法。修复方法很简单, 跟 Raft 的日志 Commit 条件类似：新任 Leader 必须在当前 Term 提交一条日志之后，才允许同步成员变更日志。也即 Leader 在当前 Term 还未提交日志之前，不允许同步成员变更日志。</p></blockquote><blockquote><p>按照这个修复方法，最简单的实现就是 Leader 上任后先提交一条 no-op 日志，然后再同步成员变更日志。这条 no-op 日志可以保证跟上一任 Leader 未提交的成员变更日志至少有一个节点交集，这样可以发现上一任 Leader 的日志是旧的，从而阻止上一任 Leader 重新选为 Leader，进而阻止了脑裂的产生。</p></blockquote><blockquote><p>对应上面这个例子，就是 $L_1$ 当选 Leader 后必须先提交一条 no-op 日志，然后才能开始同步 $C_v$ 和 E，以便能发现 $L_2$ 的日志是旧的，从而阻止 $L_2$ 当选 Leader。</p></blockquote><blockquote><p>另一种方法是使用 Joint Consensus 成员变更，没有这样的正确性问题。</p></blockquote><p>我在做 TinyKV 项目时发现，$C_{new}$ 并不是即时生效的，而是上层模块处理 committedEntries 时若发现 $C_{new}$ 被提交了，才会更新节点的配置，也是算是搞定了这个问题。</p><h2 id="Q6：如何提升成员变更中的集群可用性？"><a href="#Q6：如何提升成员变更中的集群可用性？" class="headerlink" title="Q6：如何提升成员变更中的集群可用性？"></a>Q6：如何提升成员变更中的集群可用性？</h2><p>要想解答问题首先得了解问题，成员变更过程中集群的可用性为何会受影响？分为三个部分：</p><ol><li><u>新加入一个节点到集群中时</u>，该新节点的日志大概率非常落后于 leader（甚至为空），在追赶上（catch up）leader 之前，这个新节点不能参与任何新日志提交的决策，甚至是拖累。比如一个初始有 3 个节点的集群新加入了一个节点，在新节点追赶的过程中，初始 3 个节点中的某个节点崩溃下线了，那么该集群在一段时间内都处于不可用（无法提交任何新的日志），因为达成一致需要大多数节点（$\frac{3}{4}$）成功复制了该日志条目，而新节点还在追赶中。</li><li><u>当 leader 节点收到移除自己的请求时</u>。论文中到了该问题（因为 Q5 中方法二需要处理该问题），其实不算一个大问题，解决方法很多，后面会提到。</li><li><u>被移除的节点可能会发起选举干扰集群的正常流程</u>。当 leader 收到移除某个节点的请求后，leader 不再给被移除的节点发送任何日志（包括移除请求日志 $C_{new}$），同样也不会发送心跳包。被移除的节点感到很无辜啊，它并不知道自己已经不属于该集群了，所以等待它的必然是超时，它自然会发起选举请求，也就干扰到了集群。</li></ol><p><font color=red>对于第一个问题</font>，解决方法很简单，新节点刚开始加入时，将其当作 non-voter，即不被算作大多数，当其追赶上 leader 的日志后，由 leader 再一次的将其作为正式成员加入到集群中。</p><p>还有一个问题需要解决：<u>如何定义“追赶上”？</u>当新节点还在复制日志时，新的日志又会到达 leader，如果是完全相同，基本不可能。细想一下，新节点加入集群影响可用性主要是因为它需要较长的时间复制日志，那如果这个时间比较短，对可用性的影响自然就可以接受，论文中建议这个时间低于 election timeout，所以当新节点与 leader 的日志差距小到复制的时间开销低于 election timeout，那么就可以将其正式加入集群了。<u>具体算法是这样的</u>：</p><blockquote><p>新节点从 leader 处复制日志被划分为多个 round. 在每个 round 中复制 leader 在该次 round 开始时具有的所有日志，复制过程中新到达的日志会在下一个 round 中复制。leader 可能会等待几个 round（比如 10），若最后一次 round 所花费的时间少于 election timeout，那么 leader 就可以将该新节点正式加入集群中。<br>如果新节点出问题了，或者太慢了（无论是哪种都会超时），那么 leader 会主动中止本次成员更改，返回失败，后面节点可以继续请求加入，这时，它已经具有一部分日志了，成功的概率大大地增加。</p></blockquote><br><p><font color=red>对于第二个问题</font>，有两种方法可以解决：</p><ol><li>如 Q5 中提到的，可以在该 $C_{new}$ 提交后再应用新的配置。但需要注意一点，当 leader 接受到移除自己的请求 $C_{new}$ 后，leader 不应该参与任何日志的提交决策，即统计大多数时不应该算上自己，比如集群中只有两个节点时，当 leader 提交 $C_{new}$ 后移除自己，另外一个节点很可能根本没有复制 $C_{new}$，集群就陷入不可用状态了。</li><li>使用 Q7 中提到的 leadership transfer.</li></ol><br><p><font color=red>对于第三个问题</font>，粗看起来有点像 Q3 的问题一，采用 PreVote 优化就能解决。实际中，很有可能当一个节点被移除后超时发起选举时，在大多数节点中它依然具有较新的日志，也就是它依然能够当选 leader. 有意思的事情发生了，被移除的节点居然还能自己回归（当然移除节点请求会超时返回失败，稍后可能会重试）。如果移除请求会不停重试，该节点始终会被移除集群，然而这个过程就耗费了太多时间，而且旧 leader 的一些日志还可能会被覆盖，导致客户端请求超时失败。</p><blockquote><p>需要解释下，如果被移除的节点发起选举时，新的配置还未提交，那么被移除的节点有可能回归，该次配置也就失败了；如果新的配置已经提交了，那么这样的选举请求会干扰集群，使他们选举新的 leader，影响可用性。其实吧，可不可以在节点回复其他节点请求时先判断一下请求来源方到底在不在本集群中呢？</p></blockquote><br><p>PreVote 能够阻止部分的干扰，但显然还不够，<u>因此 raft 引入了租期（Lease）的概念</u>：如果一个 follower 能够在 election timeout 时间内收到 leader 的信息（AppendEntries 或 HeartBeat），则该 follower 处在租期内。当一个 follower 收到一个选举投票请求时，如该 follower 还处在租期内，那么 follower 会拒绝或者直接忽视该投票请求。</p><p>下面代码是 etcd 对 Lease 的实现。引入上面的方法后，尽管被移除的节点在大多数节点中具有较新的日志，只要 leader 正常工作，被移除的节点就不可能当选，自然也就不会干扰到集群。然而，<u>Lease 可能会影响 Q7 中的 leadership transfer</u>，所以代码中有一个特殊的变量 force，来判断是不是处于 leadership transfer.</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *raft)</span></span> Step(m pb.Message) <span class="hljs-type">error</span> &#123;<br><span class="hljs-comment">// Handle the message term, which may result in our stepping down to a follower.</span><br><span class="hljs-keyword">switch</span> &#123;<br><span class="hljs-keyword">case</span> m.Term == <span class="hljs-number">0</span>:<br><span class="hljs-comment">// local message</span><br><span class="hljs-keyword">case</span> m.Term &gt; r.Term:<br><span class="hljs-keyword">if</span> m.Type == pb.MsgVote || m.Type == pb.MsgPreVote &#123;<br>force := bytes.Equal(m.Context, []<span class="hljs-type">byte</span>(campaignTransfer))<br>inLease := r.checkQuorum &amp;&amp; r.lead != None &amp;&amp;<br>                       r.electionElapsed &lt; r.electionTimeout<br><span class="hljs-keyword">if</span> !force &amp;&amp; inLease &#123;<br><span class="hljs-comment">// If a server receives a RequestVote request within the minimum election timeout</span><br><span class="hljs-comment">// of hearing from a current leader, it does not update its term or grant its vote</span><br>r.logger.Infof(<span class="hljs-string">&quot;%x [logterm: %d, index: %d, vote: %x] ignored %s from %x [logterm: %d,</span><br><span class="hljs-string">                index: %d] at term %d: lease is not expired (remaining ticks: %d)&quot;</span>,<br>r.id, r.raftLog.lastTerm(), r.raftLog.lastIndex(), r.Vote, m.Type, m.From, m.LogTerm,<br>                    m.Index, r.Term, r.electionTimeout-r.electionElapsed)<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br>            <span class="hljs-comment">// ...</span><br>&#125;<br>        <span class="hljs-comment">// ...</span><br>    &#125;<br>    <span class="hljs-comment">// ...</span><br>&#125;<br></code></pre></td></tr></table></figure><p><strong>其实这里我有个疑问</strong>：实现 Lease 后，岂不是会增加正常选举的时长？假设某时刻 leader 维修下线了，一段时间后，某个 follower 率先超时向其他所有发送选举请求，由于其他节点都还未超时（还在租期内），该 follower 一定不会当选，且只有当集群中超过半数的节点都超时后才可能会有 leader 产生。</p><h2 id="Q7：Leadership-Transfer-过程是怎样的？"><a href="#Q7：Leadership-Transfer-过程是怎样的？" class="headerlink" title="Q7：Leadership Transfer 过程是怎样的？"></a>Q7：Leadership Transfer 过程是怎样的？</h2><p>raft 通过该过程，允许一个节点将其 leader 的身份移交给另外一个节点，<u>移交 leadership 在以下两个场景中很有用</u>：</p><ul><li>当前 leader 节点需要下线。leader 节点可能需要下线维护或集群移除了该 leader 节点。无论是哪一种，当 leader 节点下线后，集群必须等待一段时间后才能选举出新的 leader，对外提供服务，采用 leadership transfer 就可能避免等待这样一段时间。</li><li>节点不适合继续承担 leader 角色（或有更好的选择）。一直处于高负载的节点不适合做 leader，远离 datacenter 的节点也不太适合做 leader…可以通过 leadership transfer 将 leadership 身份移交给合适的节点。</li></ul><p>结合 raft 协议的基本流程可知，要想将 leader 身份转移给另外一个节点，<u>需要保证两点</u>：1) 这个节点有足够新的日志（至少在大多数节点中较新），2) 该节点还得率先发起选举。raft 通过以下步骤保证这两点：</p><ol><li>当前 leader 停止接受新的客户端请求。</li><li>当前 leader 通过常规的日志复制，将其日志全部复制到候选节点，使其具有最新的日志记录。</li><li>上述步骤完成后，当前 leader 发送 TimeoutNow 请求给候选节点，候选节点收到后发起选举（不用等待 election timeout）。<u>这里与 Q6 中的 Lease 机制有冲突</u>，需要在投票请求中添加一个字段，暗示这样的选举是合法的。</li></ol><p>当前 leader 收到候选节点的投票请求时，当前 leader 就会变成 follower（步骤 1 和 2 能保证这一点），此时他就可以下线了，transfer 过程就完成了。这个过程当然可能出问题，下面是论文中关于异常的叙述：</p><blockquote><p>It is also possible for the target server to fail; in this case, the cluster must resume client operations. If leadership transfer does not complete after about an election timeout, the prior leader aborts the transfer and resumes accepting client requests. If the prior leader was mistaken and the target server is actually operational, then at worst this mistake will result in an extra election, after which client operations will be restored.</p></blockquote><p><strong>我当然又有问题了</strong>，步骤 1 很明显会导致集群一段时间内不可用，transfer 带来的好处能不能抵消这个缺点？？？但是论文中说他们还没有测试过…</p><h2 id="Q8：Raft-之外的一致性问题？"><a href="#Q8：Raft-之外的一致性问题？" class="headerlink" title="Q8：Raft 之外的一致性问题？"></a>Q8：Raft 之外的一致性问题？</h2><p>关于分布式系统一致性内容，可以参考文章<a href="/2022/08/02/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B/" title="分布式一致性模型">分布式一致性模型</a>。</p><p>如 Q2 中所说，leader 节点收到来自客户端的请求（command）后会将它们封装为 entry，然后 propose 给 raft 模块开始共识协议，当 entry 达成一致被提交后，上层模块就会将其中的 command 应用到数据库（复制状态机）中，至此，raft 协议就应该结束了，分布式节点（应该是大多数节点）的状态都达到了一致。</p><p>当上层模块将 command 应用后，应该就该请求反馈客户端，<u>如果节点在回复客户端前崩溃了</u>，那么客户端很可能会超时重试，意味着相同的请求会再次被提交和应用，这种现象被称为 at-least-once semantics，会导致不正确的结果和不一致的状态。如下图。<br><img src="/img/raft/Q8-1.png"></p><p>为了避免上述问题，就必须实现 linearizable semantics（线性语义），也就是每个请求只执行一次，无论重复多少次（同一条指令）都只会返回第一次执行的结果。</p><p><u>如何是实现线性语义呢？论文中采用如下方法</u>：</p><ul><li>每个客户端启动时会向集群注册自己，集群给每个客户端一个独一无二的 id（标识符），此后客户端会给自己的每一个请求分配一个唯一的 serial_number，并将该序号和自己的标识符封装在请求中。同时，集群中的每个节点为每一个（应用过其请求的）客户端都维护一个 session &#x3D; [id, serial_number, response]（需持久化）.</li><li>当集群的中的节点（leader）接收到一个请求时，首先会根据请求中的 id 和 serial_number 查找是否存在相应的 session，如果存在，直接回复，如果不存在，就开始正常的 raft 协议（id 和 serial_number 也会封装在 entry 中，这样应用这个请求的每一个节点就都能维护一个对应的 session 了）。</li><li>如果一个客户端一次只发送一个请求，那么只需要维护最近的一个 session 即可，当新的 serial_number 来到时，旧的就可以删除了；如果一次发送多个请求，那么需要维护一组 session，客户端的请求中可以包含最小的没有接收到回复的 serial_number，那么节点就可以删除小于这个序号的所有 session 了。</li></ul><blockquote><p>Given this filtering of duplicate requests, Raft provides linearizability.</p></blockquote><br><p><u>上面的方法看起来不错，但还需要考虑一个问题</u>：<br>因为内存有限，session 不可能永远的存储下去，除了根据客户端的序号清理过期的外，还需要主动清理一些不太活跃的 session. 其实可以将 session 保存在 stable storage 中，采用 B+ 树索引文件组织。</p><p><u>如果要主动清理，那么又会涉及两个问题</u>：</p><ol><li>如何在所有节点中对清理某一 session 达成一致（要么所有节点——至少大多数节点——都清理，要么都不清理），不然，有些节点清理了，会可能应用重复的 command，而另外一些没有清理，则不会应用重复的 command，整个集群就处于不一致状态。</li><li>当清理了不应该清理的 session 后应该怎么办？当节点接收到某个请求后，如果不存在该客户端的任何 session 记录，那么节点无法判断请求是否是重复的。</li></ol><p><u>对于第一个问题</u>，一种方法是超时清理，若某个 session 对应的客户端在一段时间内（寿命）不活跃，那么节点清理该客户端的 session. 如何在集群节点间对超时达成协议呢？每个 entry 可以再加入一个时间戳，表示该请求的受理时间，这样 session 的起始时间就一致了。客户端可以在不活跃的时候发送保活请求（同样需要执行共识协议），应用该请求会更新 session 的寿命。</p><p><u>对于第二个问题</u>，前面提到客户端第一次启动时需要向集群注册（RegisterRPC），这个注册实际上也是一个请求，同样需要执行共识协议，为它创建第一个 session. 当节点遇到没有任何 session 记录的客户端请求（非注册请求）时就直接返回失败，abort 客户端，然客户端自己处理异常（比如发送回滚请求等）。</p><h2 id="Q9：只读请求（read-only-request）的优化？"><a href="#Q9：只读请求（read-only-request）的优化？" class="headerlink" title="Q9：只读请求（read-only request）的优化？"></a>Q9：只读请求（read-only request）的优化？</h2><p>前面提到，对于客户端发来的所有请求，都会走 raft 流程，包括 append log、replicate log、commit、apply，最后再回复客户端，会有较大的延迟，因为要日志落盘、网络传输等。<u>只读请求不修改复制状态机，对数据的一致性不会产生影响</u>，按理来说可以不走 raft 流程，而实际上在大多数的系统中读请求数量会远高于写请求，那么有没有方法优化 read-only 请求呢？</p><p>当然有了，介绍之前先讨论下如果只读请求不走 raft 流程会产生什么问题。</p><p><u>先考虑只有 leader 能接受请求</u>。如果 leader 一收到只读请求就直接执行并回复，那它可能返回过期的数据，为什么？有以下两个原因：</p><ul><li>leader 身份过期。leader 处于网络分区中，其他分区早已经选出新 leader 并接受了新的请求。那么当前的 leader 的数据可能已经过期了。</li><li>读请求涉及的数据可能已经 commit 但还未 apply。</li></ul><p>能不能读取过期的数据完全取决于系统的类型，允许读取过期数据实际上破坏了<a href="/2022/08/02/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B/" title="分布式一致性模型">分布式一致性模型</a>中的线性一致性，具体可参阅文章。这里讨论如何避免读取过期的数据。</p><p>原因找到了，leader 采用下面的步骤解决上述问题：</p><ol><li>raft 算法可以保证，leader 在选举成功时一定具有所有 committed 的日志，但也可能拥有存在于大多数节点而还未 commit 的日志，leader 为了确定在自己任期的 commit index，它会在选举成功后 append 一条 noop-entry，如 Q4，这样他就能得知最新的 commit index 了。</li><li>leader 另外采用一个变量 readIndex 来记录当前的 commit index.</li><li>为了确保自己的身份没有过期，leader 会广播一轮心跳（即便心跳还未超时），如果收到了大多数节点的回复，说明自身还未过期，也就是 readIndex 是当前最大的 commit index.</li><li>leader 等待它的复制状态机 apply 日志，等待 apply 到 readIndex 时，就能保证复制状态机中的数据是最新的</li><li>这时 leader 执行这些只读请求然后回复。这样就一定能避免读到过期的日志，也就保证了线性一致性。</li></ol><p>可以看到这 5 个步骤解决了上述的两个问题。当只读请求到达时，leader 总是会重复执行 3 - 5 步骤，尤其是步骤 3，开销还是很大，所以 leader 可以通过累计一定数量的只读请求，让它们平摊开销。</p><p><u>如果 follower 也能执行只读请求</u>就能进一步提升系统的读吞吐量，也能减轻 leader 的负载。follower 不一定具有最新的数据，它只能通过询问 leader 最新的 readIndex 是多少，但该 leader 自己的数据有可能过期，所以可以采用如下的方法：当 follower 收到只读请求后，向 leader 发送消息请求最新的 readIndex，leader 收到请求后执行上述的 1-3 步骤，返回给 follower 最新的 readIndex，最后再由 follower 执行步骤 4-5.</p><p><u>raft 博士论文还提到了一种优化</u>，但他们不推荐，这种优化可以避免 leader 发送心跳包来确认自己的身份。这个优化基于如下的思考：只要集群中未发起选举，那么当前 leader 一定拥有最新的数据，其他节点要想发起选举必须要等到 election_timeout 超时，如果此时距 leader 上次收到大多数节点回复的时间在一个阈值内，那么就没有必要再次发送心跳来确认身份，因为在这期间不可能有节点超时发起选举，这个阈值是多少呢？<br>$$<br>start + \frac{election\ timeout}{clock\ drift\ bound}<br>$$</p><p><u>这个方法和 Q6 中的 lease 本质是一样的</u>，对于 follower，只要自己的 election_timeout 还未超时，那么它就认为 leader 还处于它自己的租期中；对于 leader，只要没有任何 follower 超时选举，它就依然处于租期中。考虑到各个机器的时钟有差异，所以有一个参数 clock drift bound. 这个方法最难的地方在于时钟的猜想不一定能保证。</p><h2 id="Q10：极端情况下的活性问题"><a href="#Q10：极端情况下的活性问题" class="headerlink" title="Q10：极端情况下的活性问题"></a>Q10：极端情况下的活性问题</h2><p><u>考虑这样一种情况</u>：一个 raft 集群有 4 个节点，其中一个节点与其他 3 个的网络连接不太稳定，不妨假设该节点不能收到其他节点的消息但能给其他节点发送消息。由于收不到 leader 的心跳包，该节点自增 Term 发起选举，导致正常工作的集群 leader 下线重新选举，这种情况会一直反复导致集群无法正常工作。</p><p>Q3 中提到的 PreVote 机制可以避免这种情况，然而<u>问题并没有解决</u>，再考虑下面的情况：<br><img src="/img/raft/Q10_1.png"></p><blockquote><p>图中描述的情况是：leader 节点（D）与节点 B、E 之间存在可靠连接，与节点 A、C 之间存在不稳定的连接，节点 A、B、C 之间存在可靠的连接。初始时 leader 能收到大多数（包括自己）节点的回复，整个集群正常工作，而此时节点 E 崩溃下线。</p></blockquote><p>很明显，A、C 两个节点不能按时收到 D 的心跳包，会超时发起选举，假设 A 率先超时向 B、C 发起 PreVote 请求，但由于节点 B 能收到心跳包，所以不会同意 PreVote 请求（因为 PreVote 包含的 Term 和节点 B 一样大，但节点 B 的日志更新，或者它们的日志一样新，但 B 在 lease 中），因此节点 A、C 不可能获取到多数选票当选 leader。<u>该集群的问题是</u>不能选出新的 leader，而当 leader（节点 D）的 AppendEntries 又只能达到两个节点（包括自己）不满足多数原则，因此整个集群无法取得进展，依然不满足活性。</p><p>PreVote 机制本来是为了增强 raft 集群可用性（活性）而设计的，而在这里却<u>起了反作用</u>：5 个节点的集群应该可以容忍 2 两个节点的失败，但增加 PreVote 后反而无法容忍仅仅一个节点的失败。如果没有 PreVote，节点 A&#x2F;C 是有可能当选新 leader 的，整个集群也能正常服务。</p><p>因此 raft 需要一种机制让 leader 主动下台：如果 leader 没能收到大多数节点的回复，那么就主动下台，等待集群节点超时重新选举，<u>etcd 把这一优化叫做 checkQuorum</u>. PreVote 确保了一旦 leader 选举成功，整个系统将是稳定的，再结合 checkQuorum，就可以解决 raft 算法的活性问题了。</p><h2 id="Q11：Raft-批处理和流水线是什么"><a href="#Q11：Raft-批处理和流水线是什么" class="headerlink" title="Q11：Raft 批处理和流水线是什么"></a>Q11：Raft 批处理和流水线是什么</h2><p><img src="/img/raft/Q11_1.jpg" alt="来自深入理解分布式系统"><br>如上图，leader 在收到请求后一般是先落盘然后再并行向 follower 复制日志，其实 leader 的落盘可以和发送复制请求并行，在 TinyKV 项目中，日志的落盘是上层模块负责的，看样子是实现了这个优化的。</p><p>再者，leader 可以累计一部分日志后在将它们复制给 follower，也就是在一次 RPC 中尽量发送更多的日志，同时日志落盘时也可以采用 batch 方式一次写入多条日志，TinyKV 中也是这样做的。</p><p>如果只是以批处理的方式发送消息，那么领导者还是需要等待跟随者返回才能继续后面的流程，<u>对此可以使用流水线 (pipeline)</u> 来进行加速。前文提到，领导者会维护一个变量 nextIndex 来表示下一个给跟随者发送的日志位置，在大部分情况下，只要领导者与跟随者建立了连接，我们都会认为网络是稳定互通的，网络分区只是小概率事件。所以，当领导者给跟随者发送了一批日志消息之后，它可以直接更新 nextIndex，并且立刻发送后续的日志，不需要等待跟随者的返回。如果网络出现了错误，或者跟随者返回了一些错误，那么领导者可以重新调整 nextIndex 的值,然后重新发送日志。</p><p>Appendentries RPC 对于日志一致性的检查保证了流水线的安全性。如果 RPC 失败或超时了，那么领导者就要将 nextIndex 递减并重试一次日志复制。如果 AppendEntriesRPC 一致性检查还是失败，那么领导者可能进一步递减 nextIndex 并重试发送前一个记录，或者等待前一个记录被确认。最初的线程架构阻碍了流水线的实现，因为它只支持串行地向每个跟随者发送一个 RPC 请求。想要<u>支持流水线，领导者必须以多线程的方式与一个跟随者建立多个连接</u>。</p><p>如果领导者不使用多线程，那么效果会是怎样的呢？其实这样的流水线和批处理没有多大区别，在 TCP 网络层面，消息已经是串行的了，TCP 本身就有滑动窗口来做批处理优化，同时单条连接保证了消息很少会乱序。<u>使用多线程连接是安全的</u>。即使因为在多个连接中请求不能保证请求有序，但在大部分情况下还是先发送的请求先到达。即使后发送的请求先到达了，由于存在AppendEntriesRPC 的一致性检查，后发送的请求自然会失败，失败后重试即可。</p><p><u>raft 算法系统的整体性能在很大程度上取决于如何安排批处理和流水线</u>。如果在高负载的情况下，一个批处理中积累的请求数量不够，那么整体处理效率就会很低，导致低吞吐量和高延迟。另一方面，如果在一个批处理中积累了太多的请求，那么延迟将不必要地变高，因为早期的请求要等待后来的请求到达才会发送批处理请求。</p><h2 id="Q12：MultiRaft-是个啥"><a href="#Q12：MultiRaft-是个啥" class="headerlink" title="Q12：MultiRaft 是个啥"></a>Q12：MultiRaft 是个啥</h2><a href="/2022/07/22/PingCAP%20TinyKV%20%E6%80%BB%E7%BB%93/" title="PingCAP TinyKV 总结">PingCAP TinyKV 总结</a><h2 id="Q13：Raft-的日志压缩"><a href="#Q13：Raft-的日志压缩" class="headerlink" title="Q13：Raft 的日志压缩"></a>Q13：Raft 的日志压缩</h2><p>一般日志采用 LSM 存储引擎存储，可以参考文章 <a href="/2022/06/30/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%89%EF%BC%9ALSM%20Tree/" title="索引篇三：LSM Tree">索引篇三：LSM Tree</a>.</p>]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
    </categories>
    
    
    <tags>
      
      <tag>raft</tag>
      
      <tag>分布式一致性协议</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2022年(23届)秋招记录</title>
    <link href="/2022/06/07/2022%E5%B9%B4-23%E5%B1%8A-%E7%A7%8B%E6%8B%9B%E8%AE%B0%E5%BD%95/"/>
    <url>/2022/06/07/2022%E5%B9%B4-23%E5%B1%8A-%E7%A7%8B%E6%8B%9B%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="d36b903f8904efcb9c5e332ef892e179d2b37212847ce2b60ff4875a7cbdcd03">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">找完工作后再开放</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
