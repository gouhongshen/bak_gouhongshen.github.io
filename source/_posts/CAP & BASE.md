---
title: CAP & BASE
date: 2022-07-17 11:23:02
tags:
sticky: 1
---

数据一般有多个备份，存储在多个分布式节点上。像 raft 这类共识算法所做的努力是维护多个数据备份之间的一致性。raft 算法的正常执行需要集群中大多数节点能够相互通信，如果不能满足这个条件，为了满足一致性，集群将对更新请求不可用。

而在大规模分布式系统中，节点故障和网络分区几乎无法避免。如果和某个客户端相连的分区（可能多个）中节点数都不足大多数，那该客户端将无法执行更新请求（读请求有可能，看具体实现）；如果集群的所有分区的节点数都不足大多数（如三个节点三个分区），那么整个集群也处于更新不可用状态。

## CAP 理论
上面描述的情况中冲突在于，分区发生时，可用性和一致性往往不能兼顾（取决于分区的严重性），<u>那么有没有方法可能做到这一点呢？CAP 理论告诉我们：不可能</u>。CAP 指出，任何分布式数据库最多只能同时保证以下三个特性中的两个：
* 一致性（Consistency），指线性一致性。
* 可用性（Availability）
* 分区容忍性（Partition-Tolerance）

一致性和可用性都接触过了，这里解释下<u>分区容忍性的含义</u>，要想访问到数据就必须与存储该数据的机器联通，如果数据只存储一份，当该机器不与外界联通时（发生网络分区或节点故障），外界将彻底不能访问该数据（不能容忍分区的发生），解决办法很简单，那就是在分布式节点中存储数据的多个备份，这样只要有一台机器能与外界连通，数据就能被访问（提高了分区容忍性），有了备份自然就需要复制协议（一致性协议）。

<font color=red>很明显，分区容忍性是一定需要满足的，而在大型分布式系统中，节点故障和网络分区又一定会发生，那么就只能在一致性和可用性之间抉择。</font>

## BASE 理论
CAP 指出了问题，但并没有给出问题的解决方式，<u>BASE 理论做到了这一点</u>，它包含了以下三个特性：
* 基本可用性（Basically Available）
* 软状态（Soft State）
* 最终一致性（Eventually Consistent）

CAP 中的一致性是强一致性，如银行系统，但并不是所有分布式系统都必须时时刻刻满足强一致性，比如，两个用户同时编辑同一份文本，如果网络断开或节点发生故障（分区），他们的编辑就会被保存在客户端的本地（可以理解为一个节点），待分区从分区恢复后，双方上传自己的更新时就可能会发生冲突，这个冲突并不是很严重，可以稍后解决。<u>对于这样的系统 BASE 理论给出了解决方法</u>：
> 当分区发生后，系统照样处理更新请求，提供基本的可用性，但这样一来，各个节点的状态就可能不一致，也就是系统处于软状态，待分区被解决后，再处理冲突，使系统达到最终一致性。
> 
> MongoDB 等数据库系统可以配置处理写请求时的最少参与节点数目（数据备份数），如果配置为集群的大多数，那么就不会存在不一致，如果少于就会有不一致的风险。

<br>

<u>所以 BASE 的重点落在了最终一致性上面</u>，要做到这一点，首先要能检测出不一致（冲突），然后解决它们，下面介绍一种方法。

## Merkle Tree
梅克尔树（Merkle Tree）又称哈希树（hash tree），是一种用来高效检测数据集之间的不一致的数据结构。哈希树在区块链中用得很多，比如验证区块中交易记录的完整性，不过这里讨论的是它在数据复制中的作用：假设数据有多个副本，这些副本之间的数据可能存在不一致，在最终一致性系统里，就需要方法来找出不一致的数据，当然可以通过一一比较两个副本的所有数据做到，不过开销就很难接受。下面介绍哈希树，看它是如何高效地完成这个工作的。

以一个例子阐述哈希树的工作原理。
![](/img/cap_base/1.jpg)
如图所示，有一个数据副本，它包含 8 个键值对数据。哈希树选择一个哈希函数 $h_1$ 将每个键值对的 key 映射成一个 $n$ 位的整数。





> 这篇文章写完后，再写数据的复制，里面要提到哈希树的应用，自然也会提到一致性哈希