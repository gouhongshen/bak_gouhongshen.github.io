---
title: 数据分片
date: 2022-07-25 10:04:26
tags:
sticky: 1
---

数据分片又叫数据分区（data partitioning），指的是将一个数据集分成多个部分分开存储在不同的磁盘或者机器，这些分片在实践中表现为一个个小数据库，为了与节点/网络故障导致的分区相区别，之后本文统称为分片。

对于<u>关系型数据库</u>，数据以表的形式存储，因此数据分片又分为两种：
* 水平分片（horizontal partitioning），又称为 sharding，以元组为单位，若干个元组构成一个分片。
* 垂直分片（vertical partitioning），又称为列式存储，以列为单位，若干个列（整列）构成一个分片。

对于非关系型数据库，自然就没有这些区分了。本文主要讨论水平分片，内容同样适用于非关系型数据。

<u>数据分片是为了啥</u>？主要有以下的好处：
> 1. 主要是为了提高水平扩展（horizontal scaling）能力。水平扩展是指添加更多的机器，以分散负载，允许更多的流量和更快的处理（不同分片的请求可以并行，减少响应时间）。与水平拓展对应的是垂直扩展（vertical scaling），垂直扩展也称为向上扩展（scaling up），是指升级现有服务器的硬件，通常是添加更多内存或 CPU 到单台服务器中。
> 2. 分片还可以通过减少宕机的影响，使应用程序更稳定可靠。对于分片数据库，宕机可能只会影响单个分片。即使这可能使某些用户无法使用应用程序或网站部分功能，但仍会低于整个数据库崩溃带来的影响。

### 数据分片方法
访问数据一般有以下三种方式：
* 扫描整个数据集（所有分片的集合）
* 定位某一条数据，被称为单点查询，给定一个属性值，查询一条具体的数据。
* 范围查询，给定一个属性值的区间，查询所有相应属性值落在该区间的所有数据。

在介绍数据分片方法时，也会讨论它们各自在访问数据时的优缺点。假设有 $n$ 个节点，$N_1, N_2, ..., N_n$ 用于存储分片，有以下三种方式完成数据分片：

<font color=red>Round-Robin</font>. 该方法以任一顺序扫描每一条数据，将第 $i$ 条数据发送到 $N_{((i-1)\\ mod\\ n) + 1}$ 节点。 
> 该方法能够均匀分配数据，使得每个节点都具有相同数量的数据。该方法非常适合按顺序扫描整个数据集，相比于只使用一个节点，扫描整个数据集只需要花费 $\frac{1}{n}$ 的时间，即加速比为 $n$，然而对于单点查询和范围查询就不太又好了，需要查询每一个节点来获取想要的数据。

<font color=red>Hash Partitioning</font>. 该方法先确定分片需要的属性，然后将属性值映射到集合 $\\{1,2,...,n\\}$，然后将数据存储到对应的节点上。
> 该方案可以说最适合单点查询了，如果查询请求中包含分配属性的值，那么映射后找到对应的节点，然后将请求路由过去就好了，其他不相干的节点就可以同时处理其他的请求。如果哈希函数选取得很好，能够将数据均匀的分配到 $n$ 个节点，那么扫描整个数据集的加速比也能接近 $n$. <u>该方式对范围查询不友好</u>，需要扫描所有的节点获取想要的数据。

<font color=red>Range Partitioning</font>. 该方法先确定属性，然后将属性值划分为多个区间，处于不同区间的属性值存储在不同的节点上。
> 先确定属性（设为 $A$），和一个分片向量（partitioning vector）$[v_1,v_2,...,v_{n-1}]$，然后按照这样的方式分片：考虑一条数据 $t,\\ t[A] = x$，如果 $x < v_1$，则将 $t$ 存储在 $N_1$；如果 $x >= v_{n-1}$，则 $t$ 存储在 $N_n$；如果 $v_i <= t < v_{i+1}$，则 $t$ 存储在 $N_{i+1}$，如下图：
> ![](/img/data_partitioning/1.jpg)
> 该方式对单点查询和<u>范围查询都很友好</u>。


### 处理数据倾斜（skew）
除了 Round-Robin 方法，任何一种数据分片方法都不能阻止数据倾斜的存在，<u>数据倾斜是指</u>：一些分片中具有非常多的数据，而另外一些分片中的数据量却很少，被称为 data distribution skew，一般是由如下原因产生的：
* 属性值的分布本身就不均匀（Attribute-Value Skew）。数据分片需要确定属性，而该属性的取值可能就存在倾斜，比如大学成员年龄就大多分布在[20, 30].
* 因为方法选择不当导致数据倾斜（Partitioning Skew）。对于范围分片来说，如果分片向量选择不当，就可能导致数据倾斜；又如哈希函数选择不当时，也会有数据倾斜。

> 数据分片的初衷是提升并行度和减少单个节点的负载，然而就算是数据在多个分片中存在少许的倾斜，对加速比的影响都很大（**后续会写一篇关于并行系统加速比的文章**）。考虑这样一个例子，如果数据集有 1000 条数据，将其分成 10 个分片，如果存在数据倾斜，那么有的分片会多于 100 条数据，有的会少于 100 条。假如某一个分片有 200 条数据，如果按照最长处理时间，那么加速比就只有 5，而不是理想的 10.
> 另外除了以上的数据分布倾斜外，还可能有处理倾斜（execution skew），即请求特别偏爱某些数据，导致某些节点的负载很高。<u>对于处理倾斜</u>，可以通过为每个分片复制多个备份来减轻负载，如果操作大多局限于某些列，那么可以采用垂直分片，减少不必要的列数据传输。

<br>

下面将介绍一些处理倾斜的方法。

<u>第一种方法是 Balanced Range-Partitioning Vectors</u>. 假如数据集的分布已知，那么就可以根据其分布特点设计一个不会产生数据倾斜的分片向量，一般有两种方式：1) 可以先对数据集按照某个属性进行排序，然后再均分数据。2) 方法一显然代价较高，那么可以通过记录属性值的频率或采用构造直方图的方式避免排序。

上述方法一个很明显的缺点就是它不太灵活，他需要提前知道数据的分布，而且如果随着数据集的增长数据倾斜依然会发生（重新执行分片操作代价昂贵）。

<br>

<u>第二种方法是 Virtual Node Partitioning</u>. 该方法在真实节点之上创建数倍的虚拟节点（virtual node），若干个虚拟节点被映射到同一个真实节点，数据分片时，分片直接对应的就是虚拟节点，除此之外系统需要维护一个映射数据结构：[attribute -> virtual node, virtual node -> real node]，如下图：
![](/img/data_partitioning/2.jpg)
> 采用的是 range partitioning，tablet 就是 virtual node.

<u>为什么要多一层虚拟节点呢</u>？这其实增加了灵活性，我们可以动态地记录一个真实节点的数据量和负载，如果某个节点的数据量太多或负载太高，我们就可以将映射到该真实节点的某些虚拟节点迁移到其他负载较小的真实节点上或新节点上，而不会影响到其他的节点和数据，代价较低，最后再修改映射表

virtual node 的点子不错，<u>还可以再进一步</u>：随着数据集的增长，虚拟节点的数据量或负载会变得很大（高），无论往哪里迁移都是一个很大的负担，迁移的代价也很高，这其实是一个粒度问题。为此就有了下面的方法三。

<br>

<u>第三种方法是 Dynamic Repartitioning</u>. 该方法同样使用 virtual node，不同之处在于，对于数据量太大或负载太高的虚拟节点，该方法选择从中分裂出一个新的虚拟节点，类似 B+ 树的插入和可拓展哈希的分裂操作。新分裂出的虚拟节点就可以迁移到其他的真实节点上，以更小的粒度均衡负载。

dynamic repartitioning 在分布式数据库和文件存储中用得非常多，如 GFS 和 BigTable，在这些系统中，它们用 table 指代一个数据集，然后一个 table 被分片为若干个 tablets，tablet 对应的其实就是 virtual node. 系统会维护一个 partition table（如上图）记录数据、tablets、真实节点直接的映射关系。partition table 一般会存储多个备份在客户端或 router（未必是路由器）中，下面是一些描述，其实大多数内容在上面已经说过了：
> Read requests must specify a value for the partitioning attribute, which is used to identify the tablet which could contain a record with that key value; a request that does not specify a value for the partitioning attribute would have to be sent to all tablets. A read request is processed by using the partitioning key value v to identify the tablet whose range of keys contains v, and then sending the request to the real node where the tablet resides. <u>The request can be handled efficiently at that node by</u> maintaining, for each tablet, an index on the partitioning key attribute.

> Write, insert, and delete requests are processed similarly, by routing the requests to the correct tablet and real node, using the mechanism described above for reads.

> The above scheme allows tablets to be split if they become too big; the key range corresponding to the tablet is split into two, with a <u>newly created tablet getting half the key range</u>. Records whose key range is mapped to the new tablet are then moved from the original tablet to the new tablet. The partition table is updated to reflect the split, so requests are then correctly directed to the appropriate tablet.

> If a real node gets overloaded, either due to a large number of requests or due to too much data at the node, some of the tablets from the node can be moved to a different real node that has a lower load. <u>Tablets can also be moved similarly</u> in case one of the real nodes has a large amount of data, while another real node has less data. Finally, if a new real node joins a system, some tables can be moved from existing nodes to the new node. Whenever a tablet is moved to a different real node, the partition table is updated; subsequent requests will then be sent to the correct real node.

看吧，十分灵活。

<br>

<u>第四种方法是使用 Consistent Hashing</u>. 这是一个较大的话题，直接附上原文的描述，详情参阅文章{% post_link "一致性哈希" %}：
> An alternative fully distributed approach is supported by a hash based partitioning scheme called <u>consistent hashing</u>. In the consistent hashing approach, keys are hashed to a large space, such as 32 bit integers. Further, node (or <u>virtual node</u>) identifiers are also hashed to the same space. A key $k_i$ could be logically mapped to the node $n_j$ whose hash value $h(n_j)$ is the highest value among all nodes satisfying $h(n_j) < h(k_i)$. But to ensure that every key is assigned to a node, hash values are treated as lying on a cycle similar to the face of a clock, where the maximum hash value $Max_{hash}$ is immediately followed by 0. Then, key $k_i$ is then logically mapped to the node $n_j$ whose hash value $h(n_j)$ is the closest among all nodes, when we move anti-clockwise in the circle from $h(k_i)$.

> <u>Distributed hash tables</u> based on this idea have been developed where there is no need for either a master node or a router; instead each participating node keeps track of a few other peer nodes, and routing is implemented in a cooperative manner. New nodes can join the system, and integrate themselves by following specified protocols in a completely peer-to-peer manner, without the need for a master. 





